---
title: "Go 网络 IO 模型之 EPOLL"
date: 2023-07-15T15:43:21+08:00
draft: False
tags: ["Learn"]
---

全文目录

![](img/image-20230722160040281.png)

## 1.IO 多路复用
### 1.1 IO 多路复用解释

首先拆解多路复用一词：

* 多路：存在多个待服务的对象
* 复用：只由一个执行单元提供服务

串联上述要点，多路复用指的是，由一个执行单元，同时对多个对象提供服务，形成一对多的服务关系

打个比方：多名顾客在餐厅内用餐，考虑到经营成本，很难做到为每名顾客单独提供一名招待员作一对一服务，因此餐厅经理安排每名服务生固定负责几个餐桌，服务生在几个桌次间来回辗转提供服务，这个过程本质上就是一种多路复用

![](img/image-20230722160302425.png)

在 linux 操作系统中，对 IO 多路复用的概念有着更加明确的定义：

- 多路：存在多个需要处理 io event 的 fd（linux 中，一切皆文件，所有事物均可抽象为一个文件句柄 file descriptor，简称 fd）
- 复用：复用一个 loop thread 同时为多个 fd 提供处理服务（线程 thread 是内核视角下的最小调度单位；多路复用通常为循环模型 loop model，因此称为 loop thread）

IO 多路复用中，loop thread 是提供服务的乙方；待处理 io event 的 fd 们是甲方。本着顾客是上帝的原则，乙方有义务为甲方提供更优质的服务，这里的服务质量就体现在一句话：”随叫随到，别让老板等久了”

在餐厅顾客没有需求的时候，服务生趁着闲工夫摸个鱼打个盹也尚无不可。但是一旦顾客招呼时，服务生需要第一时间赶到对需求作出响应

此外，由于服务生和顾客之间的服务关系是一对多，所以还要考虑到有多名顾客同时招呼时，服务生如何作兼容处理，让每名顾客都不至于产生被冷落的感觉。这是一门学问，也同样是计算机领域 IO 多路复用场景下需要解决的问题

### 1.2 多路复用简单实现
#### 1.2.1 阻塞 IO
通过一段伪代码，来尝试让 IO 多路复用这个概念看起来更加具体一些：

```Go
    // 多个待服务的 fd 
    fds = [fd1,fd2,fd3,...]
    // 遍历 fd 列表，末尾和首部相连，形成循环
    i = 0
    for {
       // 获取本轮待处理的 fd
       fd = fds[i]        
       // 从 fd 中读数据
       data = read(fd)  
       // 处理数据 
       handle(data)             
       // 推进遍历
       i++
       if i == len(fds){
         i = 0
       }
    }
```

上述搭了个架子，核心分为几步：

- 定义了待处理的 fds 列表（多路）
- 循环遍历 fds 列表，每轮负责读一个 fd（复用）

这是个乞丐版的 IO 多路复用模型看起来似乎有那么点意思了. 然而其本质上是一种阻塞 IO 模型（Blocking IO，简称 BIO）. 事实上，上述实现存在一个致命的问题，那就是句柄 fd 默认的 io 操作是阻塞型的，因此倘若在读 fd1 的时候，io event 没到达，那么 loop thread 就会陷入阻塞，后续 fd2、fd3 哪怕有 io event 到达，也无法得到执行

上述问题翻译成更形象的场景，大概就是：

* A桌顾客对服务生说，你先搁这候着，我看会儿菜单，一会点菜

* 服务生于是站定A桌，打定主意在A桌点完菜之后再离开

* 在此期间，服务生辖区内的B桌、C桌招呼有事，服务生也充耳不闻，只等A桌事情完结才肯挪动步子

这样的服务显然不够到位，倘若人人如此，餐厅必然面临倒闭

#### 1.2.2 非阻塞 IO
基于 BIO 存在的问题，我们进行一轮改进，核心是将 read 操作由同步阻塞操作改为带有尝试性的非阻塞操作。在读一个 fd 的时候，倘若 io event 已就绪就正常读取，否则就即时返回并抛出一个特定类型的错误，让 loop thread 能够正常执行下去，为其他 fd 提供服务

```Go
 // 多个待服务的 fd 
    fds = [fd1,fd2,fd3,...]
    // 遍历 fd 列表，末尾和首部相连，形成循环
    i = 0
    for {
       // 获取本轮待处理的 fd
       fd = fds[i]        
       // 尝试从 fd 中读数据，失败时不阻塞，而是抛出错误
       data,err = tryRead(fd)  
       // 读取数据成功，处理数据
       if err == nil{
          handle(data) 
       } 
       // 小睡一秒后再推进流程
       sleep(1 second)
       // 推进遍历
       i++
       if i == len(fds){
         i = 0
       }
    }
```

上述伪代码核心步骤如下：

- 定义了待处理的 fds 列表
- 遍历 fds 列表，每轮尝试从一个 fd 中读数据
- 倘若 io event 已就绪，则正常处理结果
- 倘若 io event 未就绪，只抛出错误，同样不阻塞流程
- 小睡一会儿，然后继续推进流程

这里确实解决阻塞 IO 中的问题，其本质上是一种非阻塞 IO 模型（Nonblocking IO，简称 NIO），但这里仍然存在问题，就是每轮处理之间的休眠时间。倘若在休眠期间，fd 中有 io event 到达，就无法被正常处理，这同样是一种不好的体验

这一问题翻译成餐厅的场景，指的就是服务生每次主动问询或者为一名客人提供服务后，就要大喘气休息几分钟，期间对客人不管不顾，这样的服务态度客人同样不会买账

那大家可能会问了，倘若把此处的休眠操作去除了如何？

答案是同样有问题. 倘若不限制轮询的执行频率，那么不轮 fd 中是否有 io event，程序都会一直高强度运行，这会导致 CPU 空转，造成很大程度的资源浪费

用餐厅的场景来聊，指的是餐厅招了个视听都不好的服务生，他感应不到客人的召唤，需要时时刻刻奔走在各个餐桌之间主动去询问客人们是否需要服务。这种情况下，哪怕客人们性子好不嫌烦，服务生自己也被这种高强度的反复横跳动作给累坏了

那大家可能又问了. 餐厅就不能招个正常的服务生吗，让他在听到客人的招呼时就去提供服务，否则就在一边老实歇着

没错，这就是正解，设计程序的码农们也是这么想的. 然而实际情况很悲催，在用户态视角下的程序正是哪一个耳目昏聩的服务生，对于 io event 的到达并没有能力做到准确地把握

于是，这就需要引入**操作系统内核**的帮助，通过几个内核对外暴露的接口，来进行 IO 多路复用的优雅实现，**做到真正意义上的“随叫随到”**

### 1.3 IO 多路复用的优雅实现
linux 内核提供了三种经典的多路复用技术：

![](img/image-20230722160504838.png)

从上图中可以看到，各个技术之间通过单向箭头连接，因此是一个持续演化改进的过程，select 最通用，但是相对粗糙；而 epoll 则最精致，在性能上也有着最优越的表现

poll 在 select 的基础之上做了改进，但治标不治本，优化得不够彻底. 我们核心还是来对比看看 select 和 epoll 之间的共性和差异：

（1）select

- 一次可以处理多个 fd，体现多路，但 fd 数量有限，最多 1024 个
- loop thread 通过 select 将一组 fd 提交到内核做监听
- 当 fd 中无 io event 就绪时，loop thread 会陷入阻塞
- 每当这组 fd 中有 io event 到达时，内核会唤醒 loop thread
- loop thread 无法精准感知到哪些 fd 就绪，需要遍历一轮 fd 列表，时间复杂度 O(N)
- 托付给内核的 fd 列表只具有一轮交互的时效，新的轮次中，loop thread 需要重新将监听的 fd 列表再传递给内核一次

（2）epoll

- 每次处理的 fd 数量无上限
- loop thread 通过 epoll_create 操作创建一个 epoll 池子
- loop thread 通过 epoll_ctl 每次将一个待监听的 fd 添加到 epoll 池中
- 每当 fd 列表中有 fd 就绪事件到达时，会唤醒 loop thread，同时内核会将处于就绪态的 fd 直接告知 loop thread，无需额外遍历

综上所述，select 和 epoll 等多路复用操作利用了内核的能力，能在待监听 fd 中有 io event 到达时，将 loop thread 唤醒，**避免无意义的主动轮询操作**

其中，epoll 相比于 select 的核心性能优势在于：

- loop thread 被唤醒时，能明确知道哪些 fd 需要处理，减少了一次额外遍历的操作，时间复杂度由 O(N) 优化到 O(1)
- epoll 通过将创建池子和添加 fd 两个操作解耦，**实现了池中 fd 数据的复用，减少了用户态与内核态间的数据拷贝成本**


## 2.EventPoll 原理

### 2.1 核心指令

epoll 又称 EventPoll，使用很简单，包含三个指令：

- epoll_create
- epoll_ctl
- epoll_wait



未完待续...