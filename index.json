[{"content":"","date":"22 September 2023","permalink":"/diary/","section":"Diaries","summary":"","title":"Diaries"},{"content":"","date":"22 September 2023","permalink":"/tags/plan/","section":"Tags","summary":"","title":"Plan"},{"content":"","date":"22 September 2023","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":"22 September 2023","permalink":"/tags/todo/","section":"Tags","summary":"","title":"TODO"},{"content":" 1.复习计划： # 计算机网络（9.18） 操作系统（9.19） MySQL（9.20） Redis（9.21） 2.每日计划： # 小徐的编程日记 Linux 就该这么学 Rust 相关文档 3.整理计划： # 整理笔记 数据采集 Grafana 完善 ","date":"22 September 2023","permalink":"/diary/todo/","section":"Diaries","summary":"1.","title":"TODO"},{"content":"Less can be more\n","date":"22 September 2023","permalink":"/","section":"Welcome to NewtSun","summary":"Less can be more","title":"Welcome to NewtSun"},{"content":"","date":"22 September 2023","permalink":"/golang/learn/limit/","section":"Golang","summary":"","title":"Go 限流"},{"content":"关于 Golang 的一些学习分享\u0026hellip;\n","date":"22 September 2023","permalink":"/golang/","section":"Golang","summary":"关于 Golang 的一些学习分享\u0026hellip;","title":"Golang"},{"content":"","date":"22 September 2023","permalink":"/tags/learn/","section":"Tags","summary":"","title":"Learn"},{"content":"","date":"22 September 2023","permalink":"/basic/","section":"Basics","summary":"","title":"Basics"},{"content":"","date":"22 September 2023","permalink":"/tags/mysql/","section":"Tags","summary":"","title":"MySQL"},{"content":"","date":"22 September 2023","permalink":"/tags/review/","section":"Tags","summary":"","title":"review"},{"content":"总结一些常见的计网面试问题\n1.HTTP # 1.1 HTTP 常见的状态码 # 1xx 类状态码属于提示信息，是协议处理中的一种中间状态，实际用到的比较少。\n2xx 类状态码表示服务器成功处理了客户端的请求，也是我们最愿意看到的状态。\n「200 OK」是最常见的成功状态码，表示一切正常。如果是非 HEAD 请求，服务器返回的响应头都会有 body 数据。 「204 No Content」也是常见的成功状态码，与 200 OK 基本相同，但响应头没有 body 数据。 「206 Partial Content」是应用于 HTTP 分块下载或断点续传，表示响应返回的 body 数据并不是资源的全部，而是其中的一部分，也是服务器处理成功的状态。 3xx 类状态码表示客户端请求的资源发生了变动，需要客户端用新的 URL 重新发送请求获取资源，也就是重定向。\n「301 Moved Permanently」表示永久重定向，说明请求的资源已经不存在了，需改用新的 URL 再次访问。 「302 Found」表示临时重定向，说明请求的资源还在，但暂时需要用另一个 URL 来访问。 301 和 302 都会在响应头里使用字段 Location，指明后续要跳转的 URL，浏览器会自动重定向新的 URL。\n「304 Not Modified」不具有跳转的含义，表示资源未修改，重定向已存在的缓冲文件，也称缓存重定向，也就是告诉客户端可以继续使用缓存资源，用于缓存控制。 4xx 类状态码表示客户端发送的报文有误，服务器无法处理，也就是错误码的含义。\n「400 Bad Request」表示客户端请求的报文有错误，但只是个笼统的错误。 「403 Forbidden」表示服务器禁止访问资源，并不是客户端的请求出错。 「404 Not Found」表示请求的资源在服务器上不存在或未找到，所以无法提供给客户端。 5xx 类状态码表示客户端请求报文正确，但是服务器处理时内部发生了错误，属于服务器端的错误码。\n「500 Internal Server Error」与 400 类型，是个笼统通用的错误码，服务器发生了什么错误，我们并不知道。 「501 Not Implemented」表示客户端请求的功能还不支持，类似“即将开业，敬请期待”的意思。 「502 Bad Gateway」通常是服务器作为网关或代理时返回的错误码，表示服务器自身工作正常，访问后端服务器发生了错误。 「503 Service Unavailable」表示服务器当前很忙，暂时无法响应客户端，类似“网络服务正忙，请稍后重试”的意思。 1.2 HTTP 与 HTTPS 的区别 # HTTP 是超文本传输协议，信息是明文传输，存在安全风险的问题。HTTPS 则解决 HTTP 不安全的缺陷，在 TCP 和 HTTP 网络层之间加入了 SSL/TLS 安全协议，使得报文能够加密传输。 HTTP 连接建立相对简单， TCP 三次握手之后便可进行 HTTP 的报文传输。而 HTTPS 在 TCP 三次握手之后，还需进行 SSL/TLS 的握手过程，才可进入加密报文传输。 两者的默认端口不一样，HTTP 默认端口号是 80，HTTPS 默认端口号是 443。 HTTPS 协议需要向 CA（证书权威机构）申请数字证书，来保证服务器的身份是可信的。 1.3 HTTPS 解决了 HTTP 的哪些问题 # HTTP 由于是明文传输，所以安全上存在以下三个风险：\n窃听风险，比如通信链路上可以获取通信内容，用户号容易没。 篡改风险，比如强制植入垃圾广告，视觉污染，用户眼容易瞎。 冒充风险，比如冒充淘宝网站，用户钱容易没。 HTTPS 在 HTTP 与 TCP 层之间加入了 SSL/TLS 协议，可以很好的解决了上述的风险：\n信息加密：交互信息无法被窃取，但你的号会因为「自身忘记」账号而没。 校验机制：无法篡改通信内容，篡改了就不能正常显示，但百度「竞价排名」依然可以搜索垃圾广告。 身份证书：证明淘宝是真的淘宝网，但你的钱还是会因为「剁手」而没。 可见，只要自身不做「恶」，SSL/TLS 协议是能保证通信是安全的。\nHTTPS 是如何解决上面的三个风险的？\n混合加密的方式实现信息的机密性，解决了窃听的风险。 摘要算法的方式来实现完整性，它能够为数据生成独一无二的「指纹」，指纹用于校验数据的完整性，解决了篡改的风险。 将服务器公钥放入到数字证书中，解决了冒充的风险。 1.4 HTTPS 如何建立连接 # SSL/TLS 协议基本流程：\n客户端向服务器索要并验证服务器的公钥。 双方协商生产「会话秘钥」。 双方采用「会话秘钥」进行加密通信。 前两步也就是 SSL/TLS 的建立过程，也就是 TLS 握手阶段。\nTLS 的「握手阶段」涉及四次通信，使用不同的密钥交换算法，TLS 握手流程也会不一样的，现在常用的密钥交换算法有两种：RSA 算法 (opens new window)和 ECDHE 算法 (opens new window)。\n基于 RSA 算法的 TLS 握手过程比较容易理解，所以这里先用这个给大家展示 TLS 握手过程，如下图：\nTLS 协议建立的详细流程：\n1.ClientHello\n首先，由客户端向服务器发起加密通信请求，也就是 ClientHello 请求。\n在这一步，客户端主要向服务器发送以下信息：\n（1）客户端支持的 TLS 协议版本，如 TLS 1.2 版本。\n（2）客户端生产的随机数（Client Random），后面用于生成「会话秘钥」条件之一。\n（3）客户端支持的密码套件列表，如 RSA 加密算法。\n2.SeverHello\n服务器收到客户端请求后，向客户端发出响应，也就是 SeverHello。服务器回应的内容有如下内容：\n（1）确认 TLS 协议版本，如果浏览器不支持，则关闭加密通信。\n（2）服务器生产的随机数（Server Random），也是后面用于生产「会话秘钥」条件之一。\n（3）确认的密码套件列表，如 RSA 加密算法。\n（4）服务器的数字证书。\n3.客户端回应\n客户端收到服务器的回应之后，首先通过浏览器或者操作系统中的 CA 公钥，确认服务器的数字证书的真实性。\n如果证书没有问题，客户端会从数字证书中取出服务器的公钥，然后使用它加密报文，向服务器发送如下信息：\n（1）一个随机数（pre-master key）。该随机数会被服务器公钥加密。\n（2）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。\n（3）客户端握手结束通知，表示客户端的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供服务端校验。\n上面第一项的随机数是整个握手阶段的第三个随机数，会发给服务端，所以这个随机数客户端和服务端都是一样的。\n服务器和客户端有了这三个随机数（Client Random、Server Random、pre-master key），接着就用双方协商的加密算法，各自生成本次通信的「会话秘钥」。\n4.服务器的最后回应\n服务器收到客户端的第三个随机数（pre-master key）之后，通过协商的加密算法，计算出本次通信的「会话秘钥」。\n然后，向客户端发送最后的信息：\n（1）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。\n（2）服务器握手结束通知，表示服务器的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供客户端校验。\n至此，整个 TLS 的握手阶段全部结束。接下来，客户端与服务器进入加密通信，就完全是使用普通的 HTTP 协议，只不过用「会话秘钥」加密内容。\n1.5 客户端校验数字证书的流程 # CA 签发证书的过程，如上图左边部分：\n首先 CA 会把持有者的公钥、用途、颁发者、有效时间等信息打成一个包，然后对这些信息进行 Hash 计算，得到一个 Hash 值； 然后 CA 会使用自己的私钥将该 Hash 值加密，生成 Certificate Signature，也就是 CA 对证书做了签名； 最后将 Certificate Signature 添加在文件证书上，形成数字证书； 客户端校验服务端的数字证书的过程，如上图右边部分：\n首先客户端会使用同样的 Hash 算法获取该证书的 Hash 值 H1； 通常浏览器和操作系统中集成了 CA 的公钥信息，浏览器收到证书后可以使用 CA 的公钥解密 Certificate Signature 内容，得到一个 Hash 值 H2 ； 最后比较 H1 和 H2，如果值相同，则为可信赖的证书，否则则认为证书不可信。 1.6 HTTPS 的应用数据是如何保证完整性 # TLS 在实现上分为握手协议和记录协议两层：\nTLS 握手协议就是我们前面说的 TLS 四次握手的过程，负责协商加密算法和生成对称密钥，后续用此密钥来保护应用程序数据（即 HTTP 数据）； TLS 记录协议负责保护应用程序数据并验证其完整性和来源，所以对 HTTP 数据加密是使用记录协议； TLS 记录协议主要负责消息（HTTP 数据）的压缩，加密及数据的认证，过程如下图：\n具体过程如下：\n首先，消息被分割成多个较短的片段,然后分别对每个片段进行压缩。 接下来，经过压缩的片段会被加上消息认证码（MAC 值，这个是通过哈希算法生成的），这是为了保证完整性，并进行数据的认证。通过附加消息认证码的 MAC 值，可以识别出篡改。与此同时，为了防止重放攻击，在计算消息认证码时，还加上了片段的编码。 再接下来，经过压缩的片段再加上消息认证码会一起通过对称密码进行加密。 最后，上述经过加密的数据再加上由数据类型、版本号、压缩后的长度组成的报头就是最终的报文数据。 记录协议完成后，最终的报文数据将传递到传输控制协议 (TCP) 层进行传输。\n1.7 HTTP/1.1 相比 HTTP/1.0 提高了什么性能 # HTTP/1.1 相比 HTTP/1.0 性能上的改进：\n使用长连接的方式改善了 HTTP/1.0 短连接造成的性能开销。 支持管道（pipeline）网络传输，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去，可以减少整体的响应时间。 但 HTTP/1.1 还是有性能瓶颈：\n请求 / 响应头部（Header）未经压缩就发送，首部信息越多延迟越大。只能压缩 Body 的部分； 发送冗长的首部。每次互相发送相同的首部造成的浪费较多； 服务器是按请求的顺序响应的，如果服务器响应慢，会招致客户端一直请求不到数据，也就是队头阻塞； 没有请求优先级控制； 请求只能从客户端开始，服务器只能被动响应。 1.8 HTTP/2 做了什么优化 # HTTP/2 协议是基于 HTTPS 的，所以 HTTP/2 的安全性也是有保障的。\n那 HTTP/2 相比 HTTP/1.1 性能上的改进：\n头部压缩 二进制格式 并发传输 服务器主动推送资源 1.头部压缩\nHTTP/2 会压缩头（Header）如果你同时发出多个请求，他们的头是一样的或是相似的，那么，协议会帮你消除重复的部分。\n这就是所谓的 HPACK 算法：在客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就提高速度了。\n2.二进制格式\nHTTP/2 不再像 HTTP/1.1 里的纯文本形式的报文，而是全面采用了二进制格式，头信息和数据体都是二进制，并且统称为帧（frame）：头信息帧（Headers Frame）和数据帧（Data Frame）。\n这样虽然对人不友好，但是对计算机非常友好，因为计算机只懂二进制，那么收到报文后，无需再将明文的报文转成二进制，而是直接解析二进制报文，这增加了数据传输的效率。\n比如状态码 200 ，在 HTTP/1.1 是用 \u0026lsquo;2\u0026rsquo;\u0026lsquo;0\u0026rsquo;\u0026lsquo;0\u0026rsquo; 三个字符来表示（二进制：00110010 00110000 00110000），共用了 3 个字节，如下图：\n在 HTTP/2 对于状态码 200 的二进制编码是 10001000，只用了 1 字节就能表示，相比于 HTTP/1.1 节省了 2 个字节，如下图：\nHeader: :status: 200 OK 的编码内容为：1000 1000\n最前面的 1 标识该 Header 是静态表中已经存在的 KV。\n在静态表里，“:status: 200 ok” 静态表编码是 8，二进制即是 1000。\n因此，整体加起来就是 1000 1000。\n3.并发传输\n我们都知道 HTTP/1.1 的实现是基于请求-响应模型的。同一个连接中，HTTP 完成一个事务（请求与响应），才能处理下一个事务，也就是说在发出请求等待响应的过程中，是没办法做其他事情的，如果响应迟迟不来，那么后续的请求是无法发送的，也造成了队头阻塞的问题。\n而 HTTP/2 就很牛逼了，引出了 Stream 概念，多个 Stream 复用在一条 TCP 连接。\n从上图可以看到，1 个 TCP 连接包含多个 Stream，Stream 里可以包含 1 个或多个 Message，Message 对应 HTTP/1 中的请求或响应，由 HTTP 头部和包体构成。Message 里包含一条或者多个 Frame，Frame 是 HTTP/2 最小单位，以二进制压缩格式存放 HTTP/1 中的内容（头部和包体）。\n针对不同的 HTTP 请求用独一无二的 Stream ID 来区分，接收端可以通过 Stream ID 有序组装成 HTTP 消息，不同 Stream 的帧是可以乱序发送的，因此可以并发不同的 Stream ，也就是 HTTP/2 可以并行交错地发送请求和响应。\n比如下图，服务端并行交错地发送了两个响应： Stream 1 和 Stream 3，这两个 Stream 都是跑在一个 TCP 连接上，客户端收到后，会根据相同的 Stream ID 有序组装成 HTTP 消息。\n4.服务器推送\nHTTP/2 还在一定程度上改善了传统的「请求 - 应答」工作模式，服务端不再是被动地响应，可以主动向客户端发送消息。\n客户端和服务器双方都可以建立 Stream， Stream ID 也是有区别的，客户端建立的 Stream 必须是奇数号，而服务器建立的 Stream 必须是偶数号。\n比如下图，Stream 1 是客户端向服务端请求的资源，属于客户端建立的 Stream，所以该 Stream 的 ID 是奇数（数字 1）；Stream 2 和 4 都是服务端主动向客户端推送的资源，属于服务端建立的 Stream，所以这两个 Stream 的 ID 是偶数（数字 2 和 4）。\n再比如，客户端通过 HTTP/1.1 请求从服务器那获取到了 HTML 文件，而 HTML 可能还需要依赖 CSS 来渲染页面，这时客户端还要再发起获取 CSS 文件的请求，需要两次消息往返，如下图左边部分：\n如上图右边部分，在 HTTP/2 中，客户端在访问 HTML 时，服务器可以直接主动推送 CSS 文件，减少了消息传递的次数。\n1.9 HTTP/2 有什么缺陷 # HTTP/2 通过 Stream 的并发能力，解决了 HTTP/1 队头阻塞的问题，看似很完美了，但是 HTTP/2 还是存在“队头阻塞”的问题，只不过问题不是在 HTTP 这一层面，而是在 TCP 这一层。\nHTTP/2 是基于 TCP 协议来传输数据的，TCP 是字节流协议，TCP 层必须保证收到的字节数据是完整且连续的，这样内核才会将缓冲区里的数据返回给 HTTP 应用，那么当「前 1 个字节数据」没有到达时，后收到的字节数据只能存放在内核缓冲区里，只有等到这 1 个字节数据到达时，HTTP/2 应用层才能从内核中拿到数据，这就是 HTTP/2 队头阻塞问题。\n图中发送方发送了很多个 packet，每个 packet 都有自己的序号，你可以认为是 TCP 的序列号，其中 packet 3 在网络中丢失了，即使 packet 4-6 被接收方收到后，由于内核中的 TCP 数据不是连续的，于是接收方的应用层就无法从内核中读取到，只有等到 packet 3 重传后，接收方的应用层才可以从内核中读取到数据，这就是 HTTP/2 的队头阻塞问题，是在 TCP 层面发生的。\n所以，一旦发生了丢包现象，就会触发 TCP 的重传机制，这样在一个 TCP 连接中的所有的 HTTP 请求都必须等待这个丢了的包被重传回来。\n1.10 HTTP/3 做了什么优化 # 前面我们知道了 HTTP/1.1 和 HTTP/2 都有队头阻塞的问题：\nHTTP/1.1 中的管道（ pipeline）虽然解决了请求的队头阻塞，但是没有解决响应的队头阻塞，因为服务端需要按顺序响应收到的请求，如果服务端处理某个请求消耗的时间比较长，那么只能等响应完这个请求后， 才能处理下一个请求，这属于 HTTP 层队头阻塞。 HTTP/2 虽然通过多个请求复用一个 TCP 连接解决了 HTTP 的队头阻塞 ，但是一旦发生丢包，就会阻塞住所有的 HTTP 请求，这属于 TCP 层队头阻塞。 HTTP/2 队头阻塞的问题是因为 TCP，所以 HTTP/3 把 HTTP 下层的 TCP 协议改成了 UDP。\nUDP 发送是不管顺序，也不管丢包的，所以不会出现像 HTTP/2 队头阻塞的问题。大家都知道 UDP 是不可靠传输的，但基于 UDP 的 QUIC 协议 可以实现类似 TCP 的可靠性传输。\nQUIC 有以下 3 个特点。\n无队头阻塞 更快的连接建立 连接迁移 1.无队头阻塞\nQUIC 协议也有类似 HTTP/2 Stream 与多路复用的概念，也是可以在同一条连接上并发传输多个 Stream，Stream 可以认为就是一条 HTTP 请求。\nQUIC 有自己的一套机制可以保证传输的可靠性的。当某个流发生丢包时，只会阻塞这个流，其他流不会受到影响，因此不存在队头阻塞问题。这与 HTTP/2 不同，HTTP/2 只要某个流中的数据包丢失了，其他流也会因此受影响。\n所以，QUIC 连接上的多个 Stream 之间并没有依赖，都是独立的，某个流发生丢包了，只会影响该流，其他流不受影响。\n2.更快的连接建立\n对于 HTTP/1 和 HTTP/2 协议，TCP 和 TLS 是分层的，分别属于内核实现的传输层、openssl 库实现的表示层，因此它们难以合并在一起，需要分批次来握手，先 TCP 握手，再 TLS 握手。\nHTTP/3 在传输数据前虽然需要 QUIC 协议握手，但是这个握手过程只需要 1 RTT，握手的目的是为确认双方的「连接 ID」，连接迁移就是基于连接 ID 实现的。\n但是 HTTP/3 的 QUIC 协议并不是与 TLS 分层，而是 QUIC 内部包含了 TLS，它在自己的帧会携带 TLS 里的“记录”，再加上 QUIC 使用的是 TLS/1.3，因此仅需 1 个 RTT 就可以「同时」完成建立连接与密钥协商，如下图：\n甚至，在第二次连接的时候，应用数据包可以和 QUIC 握手信息（连接信息 + TLS 信息）一起发送，达到 0-RTT 的效果。\n如下图右边部分，HTTP/3 当会话恢复时，有效负载数据与第一个数据包一起发送，可以做到 0-RTT（下图的右下角）：\n3.连接迁移\n基于 TCP 传输协议的 HTTP 协议，由于是通过四元组（源 IP、源端口、目的 IP、目的端口）确定一条 TCP 连接。\n那么当移动设备的网络从 4G 切换到 WIFI 时，意味着 IP 地址变化了，那么就必须要断开连接，然后重新建立连接。而建立连接的过程包含 TCP 三次握手和 TLS 四次握手的时延，以及 TCP 慢启动的减速过程，给用户的感觉就是网络突然卡顿了一下，因此连接的迁移成本是很高的。\n而 QUIC 协议没有用四元组的方式来“绑定”连接，而是通过连接 ID 来标记通信的两个端点，客户端和服务器可以各自选择一组 ID 来标记自己，因此即使移动设备的网络变化后，导致 IP 地址变化了，只要仍保有上下文信息（比如连接 ID、TLS 密钥等），就可以“无缝”地复用原连接，消除重连的成本，没有丝毫卡顿感，达到了连接迁移的功能。\n所以， QUIC 是一个在 UDP 之上的伪 TCP + TLS + HTTP/2 的多路复用的协议。\nQUIC 是新协议，对于很多网络设备，根本不知道什么是 QUIC，只会当做 UDP，这样会出现新的问题，因为有的网络设备是会丢掉 UDP 包的，而 QUIC 是基于 UDP 实现的，那么如果网络设备无法识别这个是 QUIC 包，那么就会当作 UDP包，然后被丢弃。\nHTTP/3 现在普及的进度非常的缓慢，不知道未来 UDP 是否能够逆袭 TCP。\n1.11 https 和 http 相比，只是传输的内容多了对称加密吗 # 建立连接时候：https 比 http多了 TLS 的握手过程； 传输内容的时候：https 会把数据进行加密，通常是对称加密数据； 1.12 为啥 SSL 的握手是 4 次 # SSL/TLS 1.2 需要 4 握手，需要 2 个 RTT 的时延，我文中的图是把每个交互分开画了，实际上把他们合在一起发送，就是 4 次握手：\n另外， SSL/TLS 1.3 优化了过程，只需要 1 个 RTT 往返时延，也就是只需要 3 次握手：\n1.13 优化 HTTP/1.1 协议的思路 # 尽量避免发送 HTTP 请求； 在需要发送 HTTP 请求时，考虑如何减少请求次数； 减少服务器的 HTTP 响应的数据大小； 1.13.1 如何避免发送 HTTP 请求 # 对于一些具有重复性的 HTTP 请求，比如每次请求得到的数据都一样的，我们可以把这对「请求-响应」的数据都缓存在本地，那么下次就直接读取本地的数据，不必在通过网络获取服务器的响应了，这样的话 HTTP/1.1 的性能肯定肉眼可见的提升。\n所以，避免发送 HTTP 请求的方法就是通过缓存技术，HTTP 设计者早在之前就考虑到了这点，因此 HTTP 协议的头部有不少是针对缓存的字段。\n那缓存是如何做到的呢？\n客户端会把第一次请求以及响应的数据保存在本地磁盘上，其中将请求的 URL 作为 key，而响应作为 value，两者形成映射关系。\n这样当后续发起相同的请求时，就可以先在本地磁盘上通过 key 查到对应的 value，也就是响应，如果找到了，就直接从本地读取该响应。毋庸置疑，读取本地磁盘的速度肯定比网络请求快得多，如下图：\n万一缓存的响应不是最新的，而客户端并不知情，那么该怎么办呢？\n放心，这个问题 HTTP 设计者早已考虑到。\n所以，服务器在发送 HTTP 响应时，会估算一个过期的时间，并把这个信息放到响应头部中，这样客户端在查看响应头部的信息时，一旦发现缓存的响应是过期的，则就会重新发送网络请求。\n如果客户端从第一次请求得到的响应头部中发现该响应过期了，客户端重新发送请求，假设服务器上的资源并没有变更，还是老样子，那么你觉得还要在服务器的响应带上这个资源吗？\n很显然不带的话，可以提高 HTTP 协议的性能，那具体如何做到呢？\n只需要客户端在重新发送请求时，在请求的 Etag 头部带上第一次请求的响应头部中的摘要，这个摘要是唯一标识响应的资源，当服务器收到请求后，会将本地资源的摘要与请求中的摘要做个比较。\n如果不同，那么说明客户端的缓存已经没有价值，服务器在响应中带上最新的资源。\n如果相同，说明客户端的缓存还是可以继续使用的，那么服务器仅返回不含有包体的 304 Not Modified 响应，告诉客户端仍然有效，这样就可以减少响应资源在网络中传输的延时，如下图：\n1.13.2 如何减少 HTTP 请求次数 # 减少 HTTP 请求次数自然也就提升了 HTTP 性能，可以从这 3 个方面入手：\n减少重定向请求次数； 合并请求； 延迟发送请求； 1.13.3 如何减少 HTTP 响应数据大小 # 对于 HTTP 的请求和响应，通常 HTTP 的响应的数据大小会比较大，也就是服务器返回的资源会比较大。\n于是，我们可以考虑对响应的资源进行压缩，这样就可以减少响应的数据大小，从而提高网络传输的效率。\n压缩的方式一般分为 2 种，分别是：\n无损压缩； 有损压缩； 1.14 TLS 握手过程 # HTTP 由于是明文传输，所谓的明文，就是说客户端与服务端通信的信息都是肉眼可见的，随意使用一个抓包工具都可以截获通信的内容。\nHTTPS 在 HTTP 与 TCP 层之间加入了 TLS 协议，来解决上述的风险。\n信息加密： HTTP 交互信息是被加密的，第三方就无法被窃取； 校验机制：校验信息传输过程中是否有被第三方篡改过，如果被篡改过，则会有警告提示； 身份证书：证明淘宝是真的淘宝网； 在进行 HTTP 通信前，需要先进行 TLS 握手。TLS 的握手过程，如下图：\n其中每一个「框」都是一个记录（record），记录是 TLS 收发数据的基本单位，类似于 TCP 里的 segment。多个记录可以组合成一个 TCP 包发送，所以通常经过「四个消息」就可以完成 TLS 握手，也就是需要 2个 RTT 的时延，然后就可以在安全的通信环境里发送 HTTP 报文，实现 HTTPS 协议。\n所以可以发现，HTTPS 是应用层协议，需要先完成 TCP 连接建立，然后走 TLS 握手过程后，才能建立通信安全的连接。\n事实上，不同的密钥交换算法，TLS 的握手过程可能会有一些区别。\n这里先简单介绍下密钥交换算法，因为考虑到性能的问题，所以双方在加密应用信息时使用的是对称加密密钥，而对称加密密钥是不能被泄漏的，为了保证对称加密密钥的安全性，所以使用非对称加密的方式来保护对称加密密钥的协商，这个工作就是密钥交换算法负责的。\n接下来，以最简单的 RSA 密钥交换算法，来看看它的 TLS 握手过程。\n1.15 RSA 握手过程 # 传统的 TLS 握手基本都是使用 RSA 算法来实现密钥交换的，在将 TLS 证书部署服务端时，证书文件其实就是服务端的公钥，会在 TLS 握手阶段传递给客户端，而服务端的私钥则一直留在服务端，一定要确保私钥不能被窃取。\n在 RSA 密钥协商算法中，客户端会生成随机密钥，并使用服务端的公钥加密后再传给服务端。根据非对称加密算法，公钥加密的消息仅能通过私钥解密，这样服务端解密后，双方就得到了相同的密钥，再用它加密应用消息。\n我用 Wireshark 工具抓了用 RSA 密钥交换的 TLS 握手过程，你可以从下面看到，一共经历了四次握手：\n对应 Wireshark 的抓包，从下图很清晰地看到该过程：\n1.15.1 TLS 第一次握手 # 客户端首先会发一个「Client Hello」消息，字面意思我们也能理解到，这是跟服务器「打招呼」。\n消息里面有客户端使用的 TLS 版本号、支持的密码套件列表，以及生成的随机数（Client Random），这个随机数会被服务端保留，它是生成对称加密密钥的材料之一。\n1.15.2 TLS 第二次握手 # 当服务端收到客户端的「Client Hello」消息后，会确认 TLS 版本号是否支持，和从密码套件列表中选择一个密码套件，以及生成随机数（Server Random）。\n接着，返回「Server Hello」消息，消息里面有服务器确认的 TLS 版本号，也给出了随机数（Server Random），然后从客户端的密码套件列表选择了一个合适的密码套件。\n可以看到，服务端选择的密码套件是 “Cipher Suite: TLS_RSA_WITH_AES_128_GCM_SHA256”。\n这个密码套件看起来真让人头晕，好一大串，但是其实它是有固定格式和规范的。基本的形式是「密钥交换算法 + 签名算法 + 对称加密算法 + 摘要算法」， 一般 WITH 单词前面有两个单词，第一个单词是约定密钥交换的算法，第二个单词是约定证书的验证算法。比如刚才的密码套件的意思就是：\n由于 WITH 单词只有一个 RSA，则说明握手时密钥交换算法和签名算法都是使用 RSA； 握手后的通信使用 AES 对称算法，密钥长度 128 位，分组模式是 GCM； 摘要算法 SHA256 用于消息认证和产生随机数； 就前面这两个客户端和服务端相互「打招呼」的过程，客户端和服务端就已确认了 TLS 版本和使用的密码套件，而且你可能发现客户端和服务端都会各自生成一个随机数，并且还会把随机数传递给对方。\n那这个随机数有啥用呢？其实这两个随机数是后续作为生成「会话密钥」的条件，所谓的会话密钥就是数据传输时，所使用的对称加密密钥。\n然后，服务端为了证明自己的身份，会发送「Server Certificate」给客户端，这个消息里含有数字证书。\n随后，服务端发了「Server Hello Done」消息，目的是告诉客户端，我已经把该给你的东西都给你了，本次打招呼完毕。\n客户端拿到了服务端的数字证书后，对数字证书进行校验。\n1.15.3 TLS 第三次握手 # 客户端验证完证书后，认为可信则继续往下走。\n接着，客户端就会生成一个新的随机数 (pre-master)，用服务器的 RSA 公钥加密该随机数，通过「Client Key Exchange」消息传给服务端。\n服务端收到后，用 RSA 私钥解密，得到客户端发来的随机数 (pre-master)。\n至此，客户端和服务端双方都共享了三个随机数，分别是 Client Random、Server Random、pre-master。\n于是，双方根据已经得到的三个随机数，生成会话密钥（Master Secret），它是对称密钥，用于对后续的 HTTP 请求/响应的数据加解密。\n生成完「会话密钥」后，然后客户端发一个「Change Cipher Spec」，告诉服务端开始使用加密方式发送消息。\n可以发现，「Change Cipher Spec」之前传输的 TLS 握手数据都是明文，之后都是对称密钥加密的密文。\n1.15.4 TLS 第四次握手 # 服务器也是同样的操作，发「Change Cipher Spec」和「Encrypted Handshake Message」消息，如果双方都验证加密和解密没问题，那么握手正式完成。\n最后，就用「会话密钥」加解密 HTTP 请求和响应了。\n1.16 RSA 算法的缺陷 # 使用 RSA 密钥协商算法的最大问题是不支持前向保密。\n因为客户端传递随机数（用于生成对称加密密钥的条件之一）给服务端时使用的是公钥加密的，服务端收到后，会用私钥解密得到随机数。所以一旦服务端的私钥泄漏了，过去被第三方截获的所有 TLS 通讯密文都会被破解。\n为了解决这个问题，后面就出现了 ECDHE 密钥协商算法，我们现在大多数网站使用的正是 ECDHE 密钥协商算法。\n1.17 HTTPS 如何优化 # 由裸数据传输的 HTTP 协议转成加密数据传输的 HTTPS 协议，给应用数据套了个「保护伞」，提高安全性的同时也带来了性能消耗。\n因为 HTTPS 相比 HTTP 协议多一个 TLS 协议握手过程，目的是为了通过非对称加密握手协商或者交换出对称加密密钥，这个过程最长可以花费掉 2 RTT，接着后续传输的应用数据都得使用对称加密密钥来加密/解密。\n产生性能消耗的两个环节：\n第一个环节， TLS 协议握手过程； 第二个环节，握手后的对称加密报文传输。 对于第二环节，现在主流的对称加密算法 AES、ChaCha20 性能都是不错的，而且一些 CPU 厂商还针对它们做了硬件级别的优化，因此这个环节的性能消耗可以说非常地小。\n而第一个环节，TLS 协议握手过程不仅增加了网络延时（最长可以花费掉 2 RTT），而且握手过程中的一些步骤也会产生性能损耗，比如：\n对于 ECDHE 密钥协商算法，握手过程中会客户端和服务端都需要临时生成椭圆曲线公私钥； 客户端验证证书时，会访问 CA 获取 CRL 或者 OCSP，目的是验证服务器的证书是否有被吊销； 双方计算 Pre-Master，也就是对称加密密钥； 1.17.1 硬件优化 # HTTPS 协议是计算密集型，而不是 I/O 密集型，所以不能把钱花在网卡、硬盘等地方，应该花在 CPU 上。\n一个好的 CPU，可以提高计算性能，因为 HTTPS 连接过程中就有大量需要计算密钥的过程，所以这样可以加速 TLS 握手过程。\n另外，如果可以，应该选择可以支持 AES-NI 特性的 CPU，因为这种款式的 CPU 能在指令级别优化了 AES 算法，这样便加速了数据的加解密传输过程。\n如果我们的 CPU 支持 AES-NI 特性，那么对于对称加密的算法应该选择 AES 算法。否则可以选择 ChaCha20 对称加密算法，因为 ChaCha20 算法的运算指令相比 AES 算法会对 CPU 更友好一点。\n1.17.2 软件优化 # 软件升级就是将正在使用的软件升级到最新版本，因为最新版本不仅提供了最新的特性，也优化了以前软件的问题或性能。比如：\n将 Linux 内核从 2.x 升级到 4.x； 将 OpenSSL 从 1.0.1 升级到 1.1.1； 1.17.3 协议优化 # 协议的优化就是对「密钥交换过程」进行优化。\n密钥交换算法优化\nTLS 1.2 版本如果使用的是 RSA 密钥交换算法，那么需要 4 次握手，也就是要花费 2 RTT，才可以进行应用数据的传输，而且 RSA 密钥交换算法不具备前向安全性。\n总之使用 RSA 密钥交换算法的 TLS 握手过程，不仅慢，而且安全性也不高。\n因此如果可以，尽量选用 ECDHE 密钥交换算法替换 RSA 算法，因为该算法由于支持「False Start」，它是“抢跑”的意思，客户端可以在 TLS 协议的第 3 次握手后，第 4 次握手前，发送加密的应用数据，以此将 TLS 握手的消息往返由 2 RTT 减少到 1 RTT，而且安全性也高，具备前向安全性。\nECDHE 算法是基于椭圆曲线实现的，不同的椭圆曲线性能也不同，应该尽量选择 x25519 曲线，该曲线是目前最快的椭圆曲线。\n对于对称加密算法方面，如果对安全性不是特别高的要求，可以选用 AES_128_GCM，它比 AES_256_GCM 快一些，因为密钥的长度短一些。\n比如在 Nginx 上，可以使用 ssl_ciphers 指令配置想使用的非对称加密算法和对称加密算法，也就是密钥套件，而且把性能最快最安全的算法放在最前面。\nTLS 升级\n如果可以，直接把 TLS 1.2 升级成 TLS 1.3，TLS 1.3 大幅度简化了握手的步骤，完成 TLS 握手只要 1 RTT，而且安全性更高。\n在 TLS 1.2 的握手中，一般是需要 4 次握手，先要通过 Client Hello （第 1 次握手）和 Server Hello（第 2 次握手） 消息协商出后续使用的加密算法，再互相交换公钥（第 3 和 第 4 次握手），然后计算出最终的会话密钥，下图的左边部分就是 TLS 1.2 的握手过程：\n上图的右边部分就是 TLS 1.3 的握手过程，可以发现 TLS 1.3 把 Hello 和公钥交换这两个消息合并成了一个消息，于是这样就减少到只需 1 RTT 就能完成 TLS 握手。\n怎么合并的呢？具体的做法是，客户端在 Client Hello 消息里带上了支持的椭圆曲线，以及这些椭圆曲线对应的公钥。\n服务端收到后，选定一个椭圆曲线等参数，然后返回消息时，带上服务端这边的公钥。经过这 1 个 RTT，双方手上已经有生成会话密钥的材料了，于是客户端计算出会话密钥，就可以进行应用数据的加密传输了。\n而且，TLS1.3 对密码套件进行“减肥”了， 对于密钥交换算法，废除了不支持前向安全性的 RSA 和 DH 算法，只支持 ECDHE 算法。\n1.17.4 证书优化 # 为了验证的服务器的身份，服务器会在 TLS 握手过程中，把自己的证书发给客户端，以此证明自己身份是可信的。\n对于证书的优化，可以有两个方向：\n一个是证书传输：要让证书更便于传输，那必然是减少证书的大小，这样可以节约带宽，也能减少客户端的运算量。所以，对于服务器的证书应该选择椭圆曲线（ECDSA）证书，而不是 RSA 证书，因为在相同安全强度下， ECC 密钥长度比 RSA 短的多。 一个是证书验证：客户端在验证证书时，是个复杂的过程，会走证书链逐级验证，验证的过程不仅需要「用 CA 公钥解密证书」以及「用签名算法验证证书的完整性」，而且为了知道证书是否被 CA 吊销，客户端有时还会再去访问 CA， 下载 CRL 或者 OCSP 数据，以此确认证书的有效性。 1.17.5 会话复用 # TLS 握手的目的就是为了协商出会话密钥，也就是对称加密密钥，那我们如果我们把首次 TLS 握手协商的对称加密密钥缓存起来，待下次需要建立 HTTPS 连接时，直接「复用」这个密钥，不就减少 TLS 握手的性能损耗了吗？\n这种方式就是会话复用（TLS session resumption），会话复用分两种：\n第一种叫 Session ID； 第二种叫 Session Ticket； 还有一种 Pre-shared Key； Session ID\nSession ID 的工作原理是，客户端和服务器首次 TLS 握手连接后，双方会在内存缓存会话密钥，并用唯一的 Session ID 来标识，Session ID 和会话密钥相当于 key-value 的关系。\n当客户端再次连接时，hello 消息里会带上 Session ID，服务器收到后就会从内存找，如果找到就直接用该会话密钥恢复会话状态，跳过其余的过程，只用一个消息往返就可以建立安全通信。当然为了安全性，内存中的会话密钥会定期失效。\nSession Ticket\n为了解决 Session ID 的问题，就出现了 Session Ticket，服务器不再缓存每个客户端的会话密钥，而是把缓存的工作交给了客户端，类似于 HTTP 的 Cookie。\n客户端与服务器首次建立连接时，服务器会加密「会话密钥」作为 Ticket 发给客户端，交给客户端缓存该 Ticket。\n客户端再次连接服务器时，客户端会发送 Ticket，服务器解密后就可以获取上一次的会话密钥，然后验证有效期，如果没问题，就可以恢复会话了，开始加密通信。\n对于集群服务器的话，要确保每台服务器加密 「会话密钥」的密钥是一致的，这样客户端携带 Ticket 访问任意一台服务器时，都能恢复会话。\nSession ID 和 Session Ticket 都不具备前向安全性，因为一旦加密「会话密钥」的密钥被破解或者服务器泄漏「会话密钥」，前面劫持的通信密文都会被破解。\nPre-shared Key\n前面的 Session ID 和 Session Ticket 方式都需要在 1 RTT 才能恢复会话。\n而 TLS1.3 更为牛逼，对于重连 TLS1.3 只需要 0 RTT，原理和 Ticket 类似，只不过在重连时，客户端会把 Ticket 和 HTTP 请求一同发送给服务端，这种方式叫 Pre-shared Key。\n1.18 HTTP/1.1 是双工的吗 # HTTP/1.1 不是双工的，它是单工通信协议。在HTTP/1.1中，客户端发送请求，然后服务器响应请求。这是一种请求-响应模式，客户端必须等待服务器的响应才能发送下一个请求。这意味着在HTTP/1.1中，同一时间内只能进行一个方向的通信，要么是客户端向服务器发送请求，要么是服务器向客户端发送响应。\n这种单工通信模式在某些情况下可能导致性能问题，尤其是在需要大量资源加载或需要实时双向通信的应用程序中。HTTP/1.1中的这种限制导致了一些额外的开销和延迟，因此HTTP/2引入了双工通信以改善性能。双工通信允许客户端和服务器同时发送请求和响应，提高了通信的效率和响应性。\n1.19 HTTP/2 是双工的吗 # 是的，HTTP/2 支持双工（双向通信）。HTTP/2 是 HTTP/1.1 的升级版本，引入了多路复用的特性，允许客户端和服务器之间同时发送多个请求和响应，而不必等待一个请求的响应完成才能发送下一个请求。这种双工通信方式可以显著提高性能和效率，尤其在需要大量资源加载的现代 web 应用中。\n在 HTTP/2 中，请求和响应都可以在单个 TCP 连接上同时进行，每个请求和响应都有一个唯一的标识符（Stream ID），通过这些标识符可以区分不同的请求和响应。这种并发性使得客户端和服务器可以更高效地交换数据，减少了延迟，提高了性能。\nHTTP/2 的双工通信是其中一个重要的特性，有助于解决了 HTTP/1.1 中的一些性能瓶颈问题，使得现代 web 应用更具响应性。\n1.20 既然 HTTP/2 是双工的，那是否可以替代 WebSocket # HTTP/2 支持双工通信，这使得它在某些情况下可以用来替代 WebSocket。WebSocket 是一种专门用于实时双向通信的协议，而 HTTP/2 则是用于更一般的 Web 请求和响应的协议。虽然 HTTP/2 支持双工通信，但 WebSocket 在以下情况下可能更适合：\n实时性要求高的应用程序：WebSocket 是为实时性要求较高的应用程序设计的，例如在线游戏、聊天应用、协作工具等。WebSocket 可以提供低延迟的双向通信，通常比 HTTP/2 更适合这些应用。\n推送通知：WebSocket 可以轻松实现服务器到客户端的推送通知，而 HTTP/2 虽然支持服务器推送，但通常更适合请求-响应模式。\n特定的协议需求：某些应用程序可能需要特定的协议，WebSocket 允许你定义自己的消息格式和协议规则，而 HTTP/2 需要遵循 HTTP 协议的规范。\n跨域通信：WebSocket 允许跨域通信，而 HTTP/2 的跨域请求通常受到浏览器的限制。\n简化通信模型：WebSocket 通常更简单，因为它专注于双向通信，而 HTTP/2 同时支持请求-响应和双向通信，可能需要更复杂的处理。\n总之，虽然 HTTP/2 支持双工通信，但 WebSocket 仍然是处理实时双向通信的更常见和更专业的解决方案。选择使用哪种协议应该根据具体的应用需求来决定。有时， WebSocket 可能是更好的选择，而在其他情况下，HTTP/2 的双工通信可能足够满足需求。\n1.21 为什么 WebSocket 比 HTTP/2 更适合实时双向通信 # WebSocket 相对于 HTTP/2 更适合实时双向通信的主要原因在于 WebSocket 的设计目标和协议特性：\n低延迟通信：WebSocket 被设计用于实时通信场景，它的协议和通信模型经过优化，可以提供低延迟的双向通信。WebSocket 连接的建立相对较快，消息的传输也更加高效，适合需要快速响应的应用，如在线游戏、聊天应用等。\n双向通信模型：WebSocket 天生支持双向通信，客户端和服务器可以在同一个连接上同时发送和接收消息，而不需要等待请求和响应。这种双向通信模型非常适合需要实时互动的应用，例如实时聊天、协作工具等。\n推送通知：WebSocket 非常适合服务器向客户端推送通知。服务器可以随时向客户端发送消息，而客户端不需要不断地轮询服务器以获取新消息。这对于需要实时更新的应用程序非常有用，如社交媒体更新、股票市场报价等。\n自定义协议：WebSocket 允许应用程序定义自己的消息格式和协议规则，这使得它非常灵活，可以根据具体的应用需求来设计通信协议。\n跨域通信：WebSocket 允许跨域通信，这意味着客户端和服务器可以位于不同的域上，而无需涉及跨域请求的复杂性。\n持久连接：WebSocket 连接可以持久保持打开状态，而 HTTP/2 连接通常会在每个请求-响应周期之间打开和关闭。这种持久性可以减少连接建立和维护的开销，降低了通信延迟。\n虽然 HTTP/2 也支持双工通信，但它的主要设计目标是提高 HTTP 请求和响应的效率，而不是专注于实时双向通信。因此，WebSocket 在需要实时双向通信的场景下通常更为合适。但需要注意的是，具体的应用需求和架构决定了选择 WebSocket 还是 HTTP/2，有些应用可能需要两者结合使用，以满足不同的通信需求。\n1.22 HTTP/1.0、HTTP/1.1、HTTP/2 和 WebSocket # HTTP/1.0、HTTP/1.1、HTTP/2 和 WebSocket 都是网络通信协议，它们在性能、特性和使用场景方面有一些重要的区别。下面详细介绍它们的区别和使用场景：\nHTTP/1.0： 性能：HTTP/1.0 是最早的 HTTP 版本，采用了简单的请求-响应模型，每个请求都需要建立一个新的连接。这导致了较高的延迟和性能较低。 特点：HTTP/1.0 不支持复用连接，每个请求都需要一个新的 TCP 连接。头部信息使用文本格式。 使用场景：HTTP/1.0 主要用于早期的 Web 应用，不太适用于需要高性能和实时性的现代应用。 HTTP/1.1： 性能：HTTP/1.1 引入了持久连接，可以在同一个连接上传输多个请求和响应。这减少了连接建立的开销，提高了性能。 特点：HTTP/1.1 支持复用连接、头部压缩、分块传输等特性，但仍然使用文本格式的头部信息。 使用场景：HTTP/1.1 用于大部分 Web 应用，适用于一般的网页浏览和传统的 HTTP 请求-响应场景。 HTTP/2： 性能：HTTP/2 引入了二进制帧（Binary Frames）的传输，采用了头部压缩和多路复用等技术，显著提高了性能和效率。 特点：HTTP/2 支持复用连接，可以在同一个连接上并行传输多个请求和响应。头部信息二进制编码，减小了数据包大小。 使用场景：HTTP/2 适用于需要更高性能和更低延迟的 Web 应用，特别是移动应用和大规模的网站。 WebSocket： 性能：WebSocket 是一种全双工通信协议，支持实时双向通信，适用于实时聊天、游戏、实时通知等应用，具有低延迟性能。 特点：WebSocket 在同一个 TCP 连接上实现双向通信，通过帧（Frames）传输数据。它支持服务器推送、自定义消息格式等特性。 使用场景：WebSocket 适用于需要实时双向通信的应用，如在线游戏、聊天应用、股票交易、新闻推送等。 综上所述，不同的协议适用于不同的场景。HTTP/1.0、HTTP/1.1 适合传统的请求-响应场景，HTTP/2 提供更高性能和效率，适合现代 Web 应用。WebSocket 则专门用于实时双向通信，适用于需要低延迟和实时性的应用。选择协议应根据应用的需求和性能要求来决定。\n1.23 如何理解在同一个连接上传输多个请求和响应 # 在同一个连接上传输多个请求和响应意味着客户端和服务器之间的通信不需要为每个请求/响应建立一个新的网络连接。相反，它们可以在已经建立的连接上传输多个请求和响应。\n具体来说，在传统的 HTTP/1.1 中，每个请求都需要建立一个新的 TCP 连接，完成后关闭连接。这导致了以下问题：\n连接建立开销：每次建立连接都需要一些时间，包括 DNS 解析、三次握手等步骤，这会增加延迟。 资源浪费：频繁建立和关闭连接会浪费服务器和客户端的资源。 头部冗余：每个请求的头部信息（包括 Cookie、User-Agent 等）在每次请求中都要重复发送，浪费了带宽。 为解决这些问题，HTTP/2 引入了多路复用（Multiplexing）的特性。这意味着在同一个 TCP 连接上，客户端和服务器可以同时传输多个请求和响应，而不需要等待前一个请求的响应。每个请求和响应都被分解成多个帧（Frames），这些帧可以并行传输，从而减少了连接建立的开销和头部冗余。\n多路复用提供了以下优势：\n更低的延迟：由于不需要等待前一个请求的响应，所以可以更快地传输数据，减少了延迟。 更高的性能：可以在同一个连接上同时处理多个请求和响应，提高了性能。 更高的效率：头部信息的压缩和复用减少了数据传输的开销，节省了带宽。 总之，多路复用是 HTTP/2 的一个重要特性，它使现代 Web 应用能够更高效地利用网络连接，提供更快的加载速度和更低的延迟。\n一个具体的接口的例子：\n当使用 HTTP/2 协议时，多路复用的工作原理并不直接体现在接口级别，而是在连接级别。这意味着不需要为每个接口请求单独的连接，而是可以共享同一个连接进行多个请求。\n以下是一个示例，展示了如何在使用 HTTP/2 时多路复用的工作方式：\n假设有一个网站，其中包含三个不同的资源：/resource1、/resource2 和 /resource3。在 HTTP/1.1 中，每个资源的请求都需要单独的连接：\n客户端发出请求 /resource1。 客户端等待服务器响应。 客户端发出请求 /resource2。 客户端等待服务器响应。 客户端发出请求 /resource3。 客户端等待服务器响应。 这会导致多个连接的建立和关闭，增加了延迟和资源开销。\n而在 HTTP/2 中，多路复用允许以下情况发生：\n客户端发出请求 /resource1，但不需要等待服务器响应。 客户端发出请求 /resource2，同时继续等待 /resource1 的响应。 客户端发出请求 /resource3，同时继续等待 /resource1 和 /resource2 的响应。 这些请求可以在同一个连接上并行传输，服务器也可以并行响应，而不需要等待前一个请求的响应。这就是多路复用的工作方式。\n具体的接口代码取决于您使用的编程语言和框架，但 HTTP/2 协议本身支持多路复用。要使用 HTTP/2，通常需要使用支持该协议的 HTTP 客户端库或服务器软件。\n2.TCP # ","date":"22 September 2023","permalink":"/basic/network/question/","section":"Basics","summary":"总结一些常见的计网面试问题","title":"计网面试问题"},{"content":"MySQL 事务复习\n1.事务的特性 # 事务是由 MySQL 的引擎来实现的，常见的 InnoDB 引擎它是支持事务的。\n不过并不是所有的引擎都能支持事务，比如 MySQL 原生的 MyISAM 引擎就不支持事务，也正是这样，所以大多数 MySQL 的引擎都是用 InnoDB。\n事务看起来感觉简单，但是要实现事务必须要遵守 4 个特性，分别如下：\n原子性（Atomicity）：一个事务中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节，而且事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样，就好比买一件商品，购买成功时，则给商家付了钱，商品到手；购买失败时，则商品在商家手中，消费者的钱也没花出去。 一致性（Consistency）：是指事务操作前和操作后，数据满足完整性约束，数据库保持一致性状态。比如，用户 A 和用户 B 在银行分别有 800 元和 600 元，总共 1400 元，用户 A 给用户 B 转账 200 元，分为两个步骤，从 A 的账户扣除 200 元和对 B 的账户增加 200 元。一致性就是要求上述步骤操作后，最后的结果是用户 A 还有 600 元，用户 B 有 800 元，总共 1400 元，而不会出现用户 A 扣除了 200 元，但用户 B 未增加的情况（该情况，用户 A 和 B 均为 600 元，总共 1200 元）。 隔离性（Isolation）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致，因为多个事务同时使用相同的数据时，不会相互干扰，每个事务都有一个完整的数据空间，对其他并发事务是隔离的。也就是说，消费者购买商品这个事务，是不影响其他消费者购买的。 持久性（Durability）：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 InnoDB 引擎通过什么技术来保证事务的这四个特性的呢？\n持久性是通过 redo log （重做日志）来保证的； 原子性是通过 undo log（回滚日志） 来保证的； 隔离性是通过 MVCC（多版本并发控制） 或锁机制来保证的； 一致性则是通过持久性+原子性+隔离性来保证； 2.事务并发引发的问题 # MySQL 服务端是允许多个客户端连接的，这意味着 MySQL 会出现同时处理多个事务的情况。\n那么在同时处理多个事务的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题。\n2.1 脏读 # 如果一个事务「读到」了另一个「未提交事务修改过的数据」，就意味着发生了「脏读」现象。\n举个栗子。\n假设有 A 和 B 这两个事务同时在处理，事务 A 先开始从数据库中读取小林的余额数据，然后再执行更新操作，如果此时事务 A 还没有提交事务，而此时正好事务 B 也从数据库中读取小林的余额数据，那么事务 B 读取到的余额数据是刚才事务 A 更新后的数据，即使没有提交事务。\n因为事务 A 是还没提交事务的，也就是它随时可能发生回滚操作，如果在上面这种情况事务 A 发生了回滚，那么事务 B 刚才得到的数据就是过期的数据，这种现象就被称为脏读。\n2.2 不可重复读 # 在一个事务内多次读取同一个数据，如果出现前后两次读到的数据不一样的情况，就意味着发生了「不可重复读」现象。\n举个栗子。\n假设有 A 和 B 这两个事务同时在处理，事务 A 先开始从数据库中读取小林的余额数据，然后继续执行代码逻辑处理，在这过程中如果事务 B 更新了这条数据，并提交了事务，那么当事务 A 再次读取该数据时，就会发现前后两次读到的数据是不一致的，这种现象就被称为不可重复读。\n2.3 幻读 # 在一个事务内多次查询某个符合查询条件的「记录数量」，如果出现前后两次查询到的记录数量不一样的情况，就意味着发生了「幻读」现象。\n举个栗子。\n假设有 A 和 B 这两个事务同时在处理，事务 A 先开始从数据库查询账户余额大于 100 万的记录，发现共有 5 条，然后事务 B 也按相同的搜索条件也是查询出了 5 条记录。\n接下来，事务 A 插入了一条余额超过 100 万的账号，并提交了事务，此时数据库超过 100 万余额的账号个数就变为 6。\n然后事务 B 再次查询账户余额大于 100 万的记录，此时查询到的记录数量有 6 条，发现和前一次读到的记录数量不一样了，就感觉发生了幻觉一样，这种现象就被称为幻读。\n3.事务的隔离级别 # 当多个事务并发执行时可能会遇到「脏读、不可重复读、幻读」的现象，这些现象会对事务的一致性产生不同程序的影响。\n脏读：读到其他事务未提交的数据； 不可重复读：前后读取的数据不一致； 幻读：前后读取的记录数量不一致。 这三个现象的严重性排序如下：\nSQL 标准提出了四种隔离级别来规避这些现象，隔离级别越高，性能效率就越低，这四个隔离级别如下：\n读未提交（read uncommitted），指一个事务还没提交时，它做的变更就能被其他事务看到； 读提交（read committed），指一个事务提交之后，它做的变更才能被其他事务看到； 可重复读（repeatable read），指一个事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，MySQL InnoDB 引擎的默认隔离级别； 串行化（serializable ）；会对记录加上读写锁，在多个事务对这条记录进行读写操作时，如果发生了读写冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行； 按隔离水平高低排序如下：\n针对不同的隔离级别，并发事务时可能发生的现象也会不同。\n也就是说：\n在「读未提交」隔离级别下，可能发生脏读、不可重复读和幻读现象； 在「读提交」隔离级别下，可能发生不可重复读和幻读现象，但是不可能发生脏读现象； 在「可重复读」隔离级别下，可能发生幻读现象，但是不可能脏读和不可重复读现象； 在「串行化」隔离级别下，脏读、不可重复读和幻读现象都不可能会发生。 所以，要解决脏读现象，就要升级到「读提交」以上的隔离级别；要解决不可重复读现象，就要升级到「可重复读」的隔离级别，要解决幻读现象不建议将隔离级别升级到「串行化」。\n不同的数据库厂商对 SQL 标准中规定的 4 种隔离级别的支持不一样，有的数据库只实现了其中几种隔离级别，我们讨论的 MySQL 虽然支持 4 种隔离级别，但是与SQL 标准中规定的各级隔离级别允许发生的现象却有些出入。\nMySQL 在「可重复读」隔离级别下，可以很大程度上避免幻读现象的发生（注意是很大程度避免，并不是彻底避免），所以 MySQL 并不会使用「串行化」隔离级别来避免幻读现象的发生，因为使用「串行化」隔离级别会影响性能。\nMySQL InnoDB 引擎的默认隔离级别虽然是「可重复读」，但是它很大程度上避免幻读现象（并不是完全解决了），解决的方案有两种：\n针对快照读（普通 select 语句），是通过 MVCC 方式解决了幻读，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。 针对当前读（select \u0026hellip; for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读，因为当执行 select \u0026hellip; for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。 接下来，举个具体的例子来说明这四种隔离级别，有一张账户余额表，里面有一条账户余额为 100 万的记录。然后有两个并发的事务，事务 A 只负责查询余额，事务 B 则会将我的余额改成 200 万，下面是按照时间顺序执行两个事务的行为：\n在不同隔离级别下，事务 A 执行过程中查询到的余额可能会不同：\n在「读未提交」隔离级别下，事务 B 修改余额后，虽然没有提交事务，但是此时的余额已经可以被事务 A 看见了，于是事务 A 中余额 V1 查询的值是 200 万，余额 V2、V3 自然也是 200 万了； 在「读提交」隔离级别下，事务 B 修改余额后，因为没有提交事务，所以事务 A 中余额 V1 的值还是 100 万，等事务 B 提交完后，最新的余额数据才能被事务 A 看见，因此额 V2、V3 都是 200 万； 在「可重复读」隔离级别下，事务 A 只能看见启动事务时的数据，所以余额 V1、余额 V2 的值都是 100 万，当事务 A 提交事务后，就能看见最新的余额数据了，所以余额 V3 的值是 200 万； 在「串行化」隔离级别下，事务 B 在执行将余额 100 万修改为 200 万时，由于此前事务 A 执行了读操作，这样就发生了读写冲突，于是就会被锁住，直到事务 A 提交后，事务 B 才可以继续执行，所以从 A 的角度看，余额 V1、V2 的值是 100 万，余额 V3 的值是 200万。 这四种隔离级别具体是如何实现的呢？\n对于「读未提交」隔离级别的事务来说，因为可以读到未提交事务修改的数据，所以直接读取最新的数据就好了； 对于「串行化」隔离级别的事务来说，通过加读写锁的方式来避免并行访问； 对于「读提交」和「可重复读」隔离级别的事务来说，它们是通过 Read View 来实现的，它们的区别在于创建 Read View 的时机不同，大家可以把 Read View 理解成一个数据快照，就像相机拍照那样，定格某一时刻的风景。「读提交」隔离级别是在「每个语句执行前」都会重新生成一个 Read View，而「可重复读」隔离级别是「启动事务时」生成一个 Read View，然后整个事务期间都在用这个 Read View。 注意，执行「开始事务」命令，并不意味着启动了事务。在 MySQL 有两种开启事务的命令，分别是：\n第一种：begin/start transaction 命令； 第二种：start transaction with consistent snapshot 命令； 这两种开启事务的命令，事务的启动时机是不同的：\n执行了 begin/start transaction 命令后，并不代表事务启动了。只有在执行这个命令后，执行了增删查改操作的 SQL 语句，才是事务真正启动的时机； 执行了 start transaction with consistent snapshot 命令，就会马上启动事务。 4.Read View 在 MVCC 里如何工作 # Read View 有四个重要的字段：\nm_ids ：指的是在创建 Read View 时，当前数据库中「活跃事务」的事务 id 列表，注意是一个列表，“活跃事务”指的就是，启动了但还没提交的事务。 min_trx_id ：指的是在创建 Read View 时，当前数据库中「活跃事务」中事务 id 最小的事务，也就是 m_ids 的最小值。 max_trx_id ：这个并不是 m_ids 的最大值，而是创建 Read View 时当前数据库中应该给下一个事务的 id 值，也就是全局事务中最大的事务 id 值 + 1； creator_trx_id ：指的是创建该 Read View 的事务的事务 id。 知道了 Read View 的字段，我们还需要了解聚簇索引记录中的两个隐藏列。\n假设在账户余额表插入一条小林余额为 100 万的记录，然后我把这两个隐藏列也画出来，该记录的整个示意图如下：\n对于使用 InnoDB 存储引擎的数据库表，它的聚簇索引记录中都包含下面两个隐藏列：\ntrx_id，当一个事务对某条聚簇索引记录进行改动时，就会把该事务的事务 id 记录在 trx_id 隐藏列里； roll_pointer，每次对某条聚簇索引记录进行改动时，都会把旧版本的记录写入到 undo 日志中，然后这个隐藏列是个指针，指向每一个旧版本记录，于是就可以通过它找到修改前的记录。 在创建 Read View 后，我们可以将记录中的 trx_id 划分这三种情况：\n一个事务去访问记录的时候，除了自己的更新记录总是可见之外，还有这几种情况：\n如果记录的 trx_id 值小于 Read View 中的 min_trx_id 值，表示这个版本的记录是在创建 Read View 前已经提交的事务生成的，所以该版本的记录对当前事务可见。 如果记录的 trx_id 值大于等于 Read View 中的 max_trx_id 值，表示这个版本的记录是在创建 Read View 后才启动的事务生成的，所以该版本的记录对当前事务不可见。 如果记录的 trx_id 值在 Read View 的 min_trx_id 和 max_trx_id 之间，需要判断 trx_id 是否在 m_ids 列表中： 如果记录的 trx_id 在 m_ids 列表中，表示生成该版本记录的活跃事务依然活跃着（还没提交事务），所以该版本的记录对当前事务不可见。 如果记录的 trx_id 不在 m_ids列表中，表示生成该版本记录的活跃事务已经被提交，所以该版本的记录对当前事务可见。 这种通过「版本链」来控制并发事务访问同一个记录时的行为就叫 MVCC（多版本并发控制）。\n5.可重复读如何工作 # 可重复读隔离级别是启动事务时生成一个 Read View，然后整个事务期间都在用这个 Read View。\n假设事务 A （事务 id 为51）启动后，紧接着事务 B （事务 id 为52）也启动了，那这两个事务创建的 Read View 如下：\n事务 A 和 事务 B 的 Read View 具体内容如下：\n在事务 A 的 Read View 中，它的事务 id 是 51，由于它是第一个启动的事务，所以此时活跃事务的事务 id 列表就只有 51，活跃事务的事务 id 列表中最小的事务 id 是事务 A 本身，下一个事务 id 则是 52。 在事务 B 的 Read View 中，它的事务 id 是 52，由于事务 A 是活跃的，所以此时活跃事务的事务 id 列表是 51 和 52，活跃的事务 id 中最小的事务 id 是事务 A，下一个事务 id 应该是 53。 接着，在可重复读隔离级别下，事务 A 和事务 B 按顺序执行了以下操作：\n事务 B 读取小林的账户余额记录，读到余额是 100 万； 事务 A 将小林的账户余额记录修改成 200 万，并没有提交事务； 事务 B 读取小林的账户余额记录，读到余额还是 100 万； 事务 A 提交事务； 事务 B 读取小林的账户余额记录，读到余额依然还是 100 万； 接下来，跟大家具体分析下。\n事务 B 第一次读小林的账户余额记录，在找到记录后，它会先看这条记录的 trx_id，此时发现 trx_id 为 50，比事务 B 的 Read View 中的 min_trx_id 值（51）还小，这意味着修改这条记录的事务早就在事务 B 启动前提交过了，所以该版本的记录对事务 B 可见的，也就是事务 B 可以获取到这条记录。\n接着，事务 A 通过 update 语句将这条记录修改了（还未提交事务），将小林的余额改成 200 万，这时 MySQL 会记录相应的 undo log，并以链表的方式串联起来，形成版本链，如下图：\n你可以在上图的「记录的字段」看到，由于事务 A 修改了该记录，以前的记录就变成旧版本记录了，于是最新记录和旧版本记录通过链表的方式串起来，而且最新记录的 trx_id 是事务 A 的事务 id（trx_id = 51）。\n然后事务 B 第二次去读取该记录，发现这条记录的 trx_id 值为 51，在事务 B 的 Read View 的 min_trx_id 和 max_trx_id 之间，则需要判断 trx_id 值是否在 m_ids 范围内，判断的结果是在的，那么说明这条记录是被还未提交的事务修改的，这时事务 B 并不会读取这个版本的记录。而是沿着 undo log 链条往下找旧版本的记录，直到找到 trx_id 「小于」事务 B 的 Read View 中的 min_trx_id 值的第一条记录，所以事务 B 能读取到的是 trx_id 为 50 的记录，也就是小林余额是 100 万的这条记录。\n最后，当事物 A 提交事务后，由于隔离级别时「可重复读」，所以事务 B 再次读取记录时，还是基于启动事务时创建的 Read View 来判断当前版本的记录是否可见。所以，即使事物 A 将小林余额修改为 200 万并提交了事务， 事务 B 第三次读取记录时，读到的记录都是小林余额是 100 万的这条记录。\n就是通过这样的方式实现了，「可重复读」隔离级别下在事务期间读到的记录都是事务启动前的记录。\n6.读提交如何工作 # 读提交隔离级别是在每次读取数据时，都会生成一个新的 Read View。\n也意味着，事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这期间另外一个事务修改了该记录，并提交了事务。\n那读提交隔离级别是怎么工作呢？我们还是以前面的例子来聊聊。\n假设事务 A （事务 id 为51）启动后，紧接着事务 B （事务 id 为52）也启动了，接着按顺序执行了以下操作：\n事务 B 读取数据（创建 Read View），小林的账户余额为 100 万； 事务 A 修改数据（还没提交事务），将小林的账户余额从 100 万修改成了 200 万； 事务 B 读取数据（创建 Read View），小林的账户余额为 100 万； 事务 A 提交事务； 事务 B 读取数据（创建 Read View），小林的账户余额为 200 万； 那具体怎么做到的呢？我们重点看事务 B 每次读取数据时创建的 Read View。前两次 事务 B 读取数据时创建的 Read View 如下图：\n分析下为什么事务 B 第二次读数据时，读不到事务 A （还未提交事务）修改的数据？\n事务 B 在找到小林这条记录时，会看这条记录的 trx_id 是 51，在事务 B 的 Read View 的 min_trx_id 和 max_trx_id 之间，接下来需要判断 trx_id 值是否在 m_ids 范围内，判断的结果是在的，那么说明这条记录是被还未提交的事务修改的，这时事务 B 并不会读取这个版本的记录。而是，沿着 undo log 链条往下找旧版本的记录，直到找到 trx_id 「小于」事务 B 的 Read View 中的 min_trx_id 值的第一条记录，所以事务 B 能读取到的是 trx_id 为 50 的记录，也就是小林余额是 100 万的这条记录。\n我们来分析下为什么事务 A 提交后，事务 B 就可以读到事务 A 修改的数据？\n在事务 A 提交后，由于隔离级别是「读提交」，所以事务 B 在每次读数据的时候，会重新创建 Read View，此时事务 B 第三次读取数据时创建的 Read View 如下：\n事务 B 在找到小林这条记录时，会发现这条记录的 trx_id 是 51，比事务 B 的 Read View 中的 min_trx_id 值（52）还小，这意味着修改这条记录的事务早就在创建 Read View 前提交过了，所以该版本的记录对事务 B 是可见的。\n正是因为在读提交隔离级别下，事务每次读数据时都重新创建 Read View，那么在事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这期间另外一个事务修改了该记录，并提交了事务。\n7.总结 # 事务是在 MySQL 引擎层实现的，我们常见的 InnoDB 引擎是支持事务的，事务的四大特性是原子性、一致性、隔离性、持久性，我们这次主要讲的是隔离性。\n当多个事务并发执行的时候，会引发脏读、不可重复读、幻读这些问题，那为了避免这些问题，SQL 提出了四种隔离级别，分别是读未提交、读已提交、可重复读、串行化，从左往右隔离级别顺序递增，隔离级别越高，意味着性能越差，InnoDB 引擎的默认隔离级别是可重复读。\n要解决脏读现象，就要将隔离级别升级到读已提交以上的隔离级别，要解决不可重复读现象，就要将隔离级别升级到可重复读以上的隔离级别。\n而对于幻读现象，不建议将隔离级别升级为串行化，因为这会导致数据库并发时性能很差。MySQL InnoDB 引擎的默认隔离级别虽然是「可重复读」，但是它很大程度上避免幻读现象（并不是完全解决了），解决的方案有两种：\n针对快照读（普通 select 语句），是通过 MVCC 方式解决了幻读，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。 针对当前读（select \u0026hellip; for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读，因为当执行 select \u0026hellip; for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。 对于「读提交」和「可重复读」隔离级别的事务来说，它们是通过 Read View 来实现的，它们的区别在于创建 Read View 的时机不同：\n「读提交」隔离级别是在每个 select 都会生成一个新的 Read View，也意味着，事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这期间另外一个事务修改了该记录，并提交了事务。 「可重复读」隔离级别是启动事务时生成一个 Read View，然后整个事务期间都在用这个 Read View，这样就保证了在事务期间读到的数据都是事务启动前的记录。 这两个隔离级别实现是通过「事务的 Read View 里的字段」和「记录中的两个隐藏列」的比对，来控制并发事务访问同一个记录时的行为，这就叫 MVCC（多版本并发控制）。\n在可重复读隔离级别中，普通的 select 语句就是基于 MVCC 实现的快照读，也就是不会加锁的。而 select .. for update 语句就不是快照读了，而是当前读了，也就是每次读都是拿到最新版本的数据，但是它会对读到的记录加上 next-key lock 锁。\n","date":"22 September 2023","permalink":"/basic/mysql/review/03/","section":"Basics","summary":"MySQL 事务复习","title":"MySQL 事务"},{"content":"MySQL 索引\n1.字段解释 # EXPLAIN SELECT * FROM user_info WHERE NAME LIKE \u0026#34;xxx%\u0026#34; EXPLAIN 是 MySQL 中用于分析查询执行计划的关键字。通过执行 EXPLAIN 后跟随一个查询语句，您可以获取有关 MySQL 如何执行查询的信息。执行计划提供了关于查询优化器的决策，以及哪些索引和表将被访问的详细信息。\n以下是 EXPLAIN 输出的常见字段和它们的含义：\nid: 查询的唯一标识符，通常是一个数字，表示查询的执行顺序。如果查询包含子查询，每个子查询都会有一个唯一的标识符。 select_type: 表示查询的类型。可能的值包括： SIMPLE: 简单的 SELECT 查询。 PRIMARY: 最外层查询。 SUBQUERY: 子查询。 DERIVED: 在 FROM 子句中派生的表（派生表）。 UNION: 使用 UNION 进行联合的查询。 UNION RESULT: UNION 查询的结果。 table: 表示访问的表的名称。 type: 表示访问表的方式和策略。可能的值包括： ALL: 全表扫描。 index: 使用索引进行扫描。 range: 使用索引范围扫描。 ref: 使用非唯一索引进行查找。 eq_ref: 使用唯一索引进行查找。 const: 使用常数值查找（通常在主键或唯一索引上）。 system: 用于特定系统表的查询。 NULL: 表示没有使用表。 possible_keys: 显示可能用于此查询的索引列表。 key: 表示实际选择的索引。 key_len: 表示索引中使用的字节数。 ref: 表示用于查找的列。 rows: 表示预计要扫描的行数。这是一个估算值，表示查询的执行计划预计需要处理的行数。 Extra: 提供了关于查询执行的其他信息，例如使用的临时表、文件排序等。可能的值包括： Using where: 使用了 WHERE 子句进行过滤。 Using index: 通过索引进行查找，无需访问表数据。 Using temporary: 使用了临时表进行排序。 Using filesort: 使用了文件排序。 Using join buffer: 使用了连接缓冲区。 通过分析 EXPLAIN 输出，您可以更好地理解查询的执行计划，识别性能瓶颈，优化查询和索引，以提高数据库查询性能。此外，了解查询执行计划还有助于调试查询问题和优化查询性能。\n","date":"22 September 2023","permalink":"/basic/mysql/review/02/","section":"Basics","summary":"MySQL 索引","title":"MySQL 索引"},{"content":"MySQL 相关知识复习\n1.MySQL 执行流程 # MySQL 的架构共分为两层：Server 层和存储引擎层：\nServer 层负责建立连接、分析和执行 SQL。MySQL 大多数的核心功能模块都在这实现，主要包括连接器，查询缓存、解析器、预处理器、优化器、执行器等。另外，所有的内置函数（如日期、时间、数学和加密函数等）和所有跨存储引擎的功能（如存储过程、触发器、视图等。）都在 Server 层实现。 存储引擎层负责数据的存储和提取。支持 InnoDB、MyISAM、Memory 等多个存储引擎，不同的存储引擎共用一个 Server 层。现在最常用的存储引擎是 InnoDB，从 MySQL 5.5 版本开始， InnoDB 成为了 MySQL 的默认存储引擎。我们常说的索引数据结构，就是由存储引擎层实现的，不同的存储引擎支持的索引类型也不相同，比如 InnoDB 支持索引类型是 B+树 ，且是默认使用，也就是说在数据表中创建的主键索引和二级索引默认使用的是 B+ 树索引。 好了，现在我们对 Server 层和存储引擎层有了一个简单认识，接下来，就详细说一条 SQL 查询语句的执行流程，依次看看每一个功能模块的作用。\n1.1 连接器 # 如果你在 Linux 操作系统里要使用 MySQL，那你第一步肯定是要先连接 MySQL 服务，然后才能执行 SQL 语句，普遍我们都是使用下面这条命令进行连接：\n# -h 指定 MySQL 服务得 IP 地址，如果是连接本地的 MySQL服务，可以不用这个参数； # -u 指定用户名，管理员角色名为 root； # -p 指定密码，如果命令行中不填写密码（为了密码安全，建议不要在命令行写密码），就需要在交互对话里面输入密码 mysql -h$ip -u$user -p 连接的过程需要先经过 TCP 三次握手，因为 MySQL 是基于 TCP 协议进行传输的，如果 MySQL 服务并没有启动，则会报错。\n如果用户密码都没有问题，连接器就会获取该用户的权限，然后保存起来，后续该用户在此连接里的任何操作，都会基于连接开始时读到的权限进行权限逻辑的判断。\n所以，如果一个用户已经建立了连接，即使管理员中途修改了该用户的权限，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。\n如何查看 MySQL 服务被多少个客户端连接了？\n如果你想知道当前 MySQL 服务被多少个客户端连接了，你可以执行 show processlist 命令进行查看。\n比如上图的显示结果，共有两个用户名为 root 的用户连接了 MySQL 服务，其中 id 为 6 的用户的 Command 列的状态为 Sleep ，这意味着该用户连接完 MySQL 服务就没有再执行过任何命令，也就是说这是一个空闲的连接，并且空闲的时长是 736 秒（ Time 列）。\n空闲连接会一直占用着吗？\n当然不是了，MySQL 定义了空闲连接的最大空闲时长，由 wait_timeout 参数控制的，默认值是 8 小时（28880秒），如果空闲连接超过了这个时间，连接器就会自动将它断开。\nmysql\u0026gt; show variables like \u0026#39;wait_timeout\u0026#39;; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | wait_timeout | 28800 | +---------------+-------+ 1 row in set (0.00 sec) 当然，我们自己也可以手动断开空闲的连接，使用的是 kill connection + id 的命令。\nmysql\u0026gt; kill connection +6; Query OK, 0 rows affected (0.00 sec) 一个处于空闲状态的连接被服务端主动断开后，这个客户端并不会马上知道，等到客户端在发起下一个请求的时候，才会收到这样的报错“ERROR 2013 (HY000): Lost connection to MySQL server during query”。\nMySQL 的连接数有限制吗？\nMySQL 服务支持的最大连接数由 max_connections 参数控制，比如我的 MySQL 服务默认是 151 个,超过这个值，系统就会拒绝接下来的连接请求，并报错提示“Too many connections”。\nmysql\u0026gt; show variables like \u0026#39;max_connections\u0026#39;; +-----------------+-------+ | Variable_name | Value | +-----------------+-------+ | max_connections | 151 | +-----------------+-------+ 1 row in set (0.00 sec) MySQL 的连接也跟 HTTP 一样，有短连接和长连接的概念，它们的区别如下：\n// 短连接 连接 mysql 服务（TCP 三次握手） 执行sql 断开 mysql 服务（TCP 四次挥手） // 长连接 连接 mysql 服务（TCP 三次握手） 执行sql 执行sql 执行sql .... 断开 mysql 服务（TCP 四次挥手） 可以看到，使用长连接的好处就是可以减少建立连接和断开连接的过程，所以一般是推荐使用长连接。\n但是，使用长连接后可能会占用内存增多，因为 MySQL 在执行查询过程中临时使用内存管理连接对象，这些连接对象资源只有在连接断开时才会释放。如果长连接累计很多，将导致 MySQL 服务占用内存太大，有可能会被系统强制杀掉，这样会发生 MySQL 服务异常重启的现象。\n怎么解决长连接占用内存的问题？\n有两种解决方式。\n第一种，定期断开长连接。既然断开连接后就会释放连接占用的内存资源，那么我们可以定期断开长连接。\n第二种，客户端主动重置连接。MySQL 5.7 版本实现了 mysql_reset_connection() 函数的接口，注意这是接口函数不是命令，那么当客户端执行了一个很大的操作后，在代码里调用 mysql_reset_connection 函数来重置连接，达到释放内存的效果。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。\n至此，连接器的工作做完了，简单总结一下：\n与客户端进行 TCP 三次握手建立连接； 校验客户端的用户名和密码，如果用户名或密码不对，则会报错； 如果用户名和密码都对了，会读取该用户的权限，然后后面的权限逻辑判断都基于此时读取到的权限； 1.2 查询缓存 # 连接器得工作完成后，客户端就可以向 MySQL 服务发送 SQL 语句了，MySQL 服务收到 SQL 语句后，就会解析出 SQL 语句的第一个字段，看看是什么类型的语句。\n如果 SQL 是查询语句（select 语句），MySQL 就会先去查询缓存（ Query Cache ）里查找缓存数据，看看之前有没有执行过这一条命令，这个查询缓存是以 key-value 形式保存在内存中的，key 为 SQL 查询语句，value 为 SQL 语句查询的结果。\n如果查询的语句命中查询缓存，那么就会直接返回 value 给客户端。如果查询的语句没有命中查询缓存中，那么就要往下继续执行，等执行完后，查询的结果就会被存入查询缓存中。\n这么看，查询缓存还挺有用，但是其实查询缓存挺鸡肋的。\n对于更新比较频繁的表，查询缓存的命中率很低的，因为只要一个表有更新操作，那么这个表的查询缓存就会被清空。如果刚缓存了一个查询结果很大的数据，还没被使用的时候，刚好这个表有更新操作，查询缓冲就被清空了，相当于缓存了个寂寞。\n所以，MySQL 8.0 版本直接将查询缓存删掉了，也就是说 MySQL 8.0 开始，执行一条 SQL 查询语句，不会再走到查询缓存这个阶段了。\n对于 MySQL 8.0 之前的版本，如果想关闭查询缓存，我们可以通过将参数 query_cache_type 设置成 DEMAND。\n1.3 解析 SQL # 在正式执行 SQL 查询语句之前， MySQL 会先对 SQL 语句做解析，这个工作交由「解析器」来完成。\n解析器\n解析器会做如下两件事情。\n第一件事情，词法分析。MySQL 会根据你输入的字符串识别出关键字出来，例如，SQL语句 select username from userinfo，在分析之后，会得到4个Token，其中有2个Keyword，分别为select和from：\n关键字 非关键字 关键字 非关键字 select username from userinfo 第二件事情，语法分析。根据词法分析的结果，语法解析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法，如果没问题就会构建出 SQL 语法树，这样方便后面模块获取 SQL 类型、表名、字段名、 where 条件等等。\n1.4 执行 SQL # 经过解析器后，接着就要进入执行 SQL 查询语句的流程了，每条SELECT 查询语句流程主要可以分为下面这三个阶段：\nprepare 阶段，也就是预处理阶段； optimize 阶段，也就是优化阶段； execute 阶段，也就是执行阶段； 1.4.1 预处理器 # 检查 SQL 查询语句中的表或者字段是否存在； 将 select * 中的 * 符号，扩展为表上的所有列； 我下面这条查询语句，test 这张表是不存在的，这时 MySQL 就会在执行 SQL 查询语句的 prepare 阶段中报错。\nmysql\u0026gt; select * from test; ERROR 1146 (42S02): Table \u0026#39;mysql.test\u0026#39; doesn\u0026#39;t exist 1.4.2 优化器 # 经过预处理阶段后，还需要为 SQL 查询语句先制定一个执行计划，这个工作交由「优化器」来完成的。\n优化器主要负责将 SQL 查询语句的执行方案确定下来，比如在表里面有多个索引的时候，优化器会基于查询成本的考虑，来决定选择使用哪个索引。\n当然，我们本次的查询语句（select * from product where id = 1）很简单，就是选择使用主键索引。\n要想知道优化器选择了哪个索引，我们可以在查询语句最前面加个 explain 命令，这样就会输出这条 SQL 语句的执行计划，然后执行计划中的 key 就表示执行过程中使用了哪个索引，比如下图的 key 为 PRIMARY 就是使用了主键索引。\n如果查询语句的执行计划里的 key 为 null 说明没有使用索引，那就会全表扫描（type = ALL），这种查询扫描的方式是效率最低档次的，如下图：\n这张 product 表只有一个索引就是主键，现在我在表中将 name 设置为普通索引（二级索引）。\n这时 product 表就有主键索引（id）和普通索引（name）。假设执行了这条查询语句：\nselect id from product where id \u0026gt; 1 and name like \u0026#39;i%\u0026#39;; 这条查询语句的结果既可以使用主键索引，也可以使用普通索引，但是执行的效率会不同。这时，就需要优化器来决定使用哪个索引了。\n很显然这条查询语句是覆盖索引，直接在二级索引就能查找到结果（因为二级索引的 B+ 树的叶子节点的数据存储的是主键值），就没必要在主键索引查找了，因为查询主键索引的 B+ 树的成本会比查询二级索引的 B+ 的成本大，优化器基于查询成本的考虑，会选择查询代价小的普通索引。\n在下图中执行计划，我们可以看到，执行过程中使用了普通索引（name），Exta 为 Using index，这就是表明使用了覆盖索引优化。\n1.4.3 执行器 # 经历完优化器后，就确定了执行方案，接下来 MySQL 就真正开始执行语句了，这个工作是由「执行器」完成的。在执行的过程中，执行器就会和存储引擎交互了，交互是以记录为单位的。\n接下来，用三种方式执行过程，跟大家说一下执行器和存储引擎的交互过程（PS ：为了写好这一部分，特地去看 MySQL 源码，也是第一次看哈哈）。\n主键索引查询 全表扫描 索引下推 主键索引查询\n以本文开头查询语句为例，看看执行器是怎么工作的。\nselect * from product where id = 1; 这条查询语句的查询条件用到了主键索引，而且是等值查询，同时主键 id 是唯一，不会有 id 相同的记录，所以优化器决定选用访问类型为 const 进行查询，也就是使用主键索引查询一条记录，那么执行器与存储引擎的执行流程是这样的：\n执行器第一次查询，会调用 read_first_record 函数指针指向的函数，因为优化器选择的访问类型为 const，这个函数指针被指向为 InnoDB 引擎索引查询的接口，把条件 id = 1 交给存储引擎，让存储引擎定位符合条件的第一条记录。 存储引擎通过主键索引的 B+ 树结构定位到 id = 1的第一条记录，如果记录是不存在的，就会向执行器上报记录找不到的错误，然后查询结束。如果记录是存在的，就会将记录返回给执行器； 执行器从存储引擎读到记录后，接着判断记录是否符合查询条件，如果符合则发送给客户端，如果不符合则跳过该记录。 执行器查询的过程是一个 while 循环，所以还会再查一次，但是这次因为不是第一次查询了，所以会调用 read_record 函数指针指向的函数，因为优化器选择的访问类型为 const，这个函数指针被指向为一个永远返回 - 1 的函数，所以当调用该函数的时候，执行器就退出循环，也就是结束查询了。 至此，这个语句就执行完成了。\n全表扫描\n举个全表扫描的例子：\nselect * from product where name = \u0026#39;iphone\u0026#39;; 这条查询语句的查询条件没有用到索引，所以优化器决定选用访问类型为 ALL 进行查询，也就是全表扫描的方式查询，那么这时执行器与存储引擎的执行流程是这样的：\n执行器第一次查询，会调用 read_first_record 函数指针指向的函数，因为优化器选择的访问类型为 all，这个函数指针被指向为 InnoDB 引擎全扫描的接口，让存储引擎读取表中的第一条记录； 执行器会判断读到的这条记录的 name 是不是 iphone，如果不是则跳过；如果是则将记录发给客户的（是的没错，Server 层每从存储引擎读到一条记录就会发送给客户端，之所以客户端显示的时候是直接显示所有记录的，是因为客户端是等查询语句查询完成后，才会显示出所有的记录）。 执行器查询的过程是一个 while 循环，所以还会再查一次，会调用 read_record 函数指针指向的函数，因为优化器选择的访问类型为 all，read_record 函数指针指向的还是 InnoDB 引擎全扫描的接口，所以接着向存储引擎层要求继续读刚才那条记录的下一条记录，存储引擎把下一条记录取出后就将其返回给执行器（Server层），执行器继续判断条件，不符合查询条件即跳过该记录，否则发送到客户端； 一直重复上述过程，直到存储引擎把表中的所有记录读完，然后向执行器（Server层） 返回了读取完毕的信息； 执行器收到存储引擎报告的查询完毕的信息，退出循环，停止查询。 至此，这个语句就执行完成了。\n索引下推\n在这部分非常适合讲索引下推（MySQL 5.6 推出的查询优化策略），这样大家能清楚的知道，「下推」这个动作，下推到了哪里。\n索引下推能够减少二级索引在查询时的回表操作，提高查询的效率，因为它将 Server 层部分负责的事情，交给存储引擎层去处理了。\n举一个具体的例子，方便大家理解，这里一张用户表如下，我对 age 和 reward 字段建立了联合索引（age，reward）：\n现在有下面这条查询语句：\nselect * from t_user where age \u0026gt; 20 and reward = 100000; 联合索引当遇到范围查询 (\u0026gt;、\u0026lt;) 就会停止匹配，也就是 age 字段能用到联合索引，但是 reward 字段则无法利用到索引。具体原因这里可以看这篇： 索引常见面试题(opens new window)\n那么，不使用索引下推（MySQL 5.6 之前的版本）时，执行器与存储引擎的执行流程是这样的：\nServer 层首先调用存储引擎的接口定位到满足查询条件的第一条二级索引记录，也就是定位到 age \u0026gt; 20 的第一条记录； 存储引擎根据二级索引的 B+ 树快速定位到这条记录后，获取主键值，然后进行回表操作，将完整的记录返回给 Server 层； Server 层在判断该记录的 reward 是否等于 100000，如果成立则将其发送给客户端；否则跳过该记录； 接着，继续向存储引擎索要下一条记录，存储引擎在二级索引定位到记录后，获取主键值，然后回表操作，将完整的记录返回给 Server 层； 如此往复，直到存储引擎把表中的所有记录读完。 可以看到，没有索引下推的时候，每查询到一条二级索引记录，都要进行回表操作，然后将记录返回给 Server，接着 Server 再判断该记录的 reward 是否等于 100000。\n而使用索引下推后，判断记录的 reward 是否等于 100000 的工作交给了存储引擎层，过程如下 ：\nServer 层首先调用存储引擎的接口定位到满足查询条件的第一条二级索引记录，也就是定位到 age \u0026gt; 20 的第一条记录； 存储引擎定位到二级索引后，先不执行回表操作，而是先判断一下该索引中包含的列（reward列）的条件（reward 是否等于 100000）是否成立。如果条件不成立，则直接跳过该二级索引。如果成立，则执行回表操作，将完成记录返回给 Server 层。 Server 层在判断其他的查询条件（本次查询没有其他条件）是否成立，如果成立则将其发送给客户端；否则跳过该记录，然后向存储引擎索要下一条记录。 如此往复，直到存储引擎把表中的所有记录读完。 可以看到，使用了索引下推后，虽然 reward 列无法使用到联合索引，但是因为它包含在联合索引（age，reward）里，所以直接在存储引擎过滤出满足 reward = 100000 的记录后，才去执行回表操作获取整个记录。相比于没有使用索引下推，节省了很多回表操作。\n当你发现执行计划里的 Extr 部分显示了 “Using index condition”，说明使用了索引下推。\n2.MySQL 记录存储 # 2.1 MySQL 的数据存放 # 大家都知道 MySQL 的数据都是保存在磁盘的，那具体是保存在哪个文件呢？\nMySQL 存储的行为是由存储引擎实现的，MySQL 支持多种存储引擎，不同的存储引擎保存的文件自然也不同。\nInnoDB 是我们常用的存储引擎，也是 MySQL 默认的存储引擎。所以，本文主要以 InnoDB 存储引擎展开讨论。\n先来看看 MySQL 数据库的文件存放在哪个目录？\nmysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;datadir\u0026#39;; +---------------+-----------------+ | Variable_name | Value | +---------------+-----------------+ | datadir | /var/lib/mysql/ | +---------------+-----------------+ 1 row in set (0.00 sec) 我们每创建一个 database（数据库） 都会在 /var/lib/mysql/ 目录里面创建一个以 database 为名的目录，然后保存表结构和表数据的文件都会存放在这个目录里。\n比如，这里有一个名为 my_test 的 database，该 database 里有一张名为 t_order 数据库表。\n然后，我们进入 /var/lib/mysql/my_test 目录，看看里面有什么文件？\n[root@ ~]#ls /var/lib/mysql/my_test db.opt t_order.frm t_order.ibd 可以看到，共有三个文件，这三个文件分别代表着：\ndb.opt，用来存储当前数据库的默认字符集和字符校验规则。 t_order.frm ，t_order 的表结构会保存在这个文件。在 MySQL 中建立一张表都会生成一个.frm 文件，该文件是用来保存每个表的元数据信息的，主要包含表结构定义。 t_order.ibd，t_order 的表数据会保存在这个文件。表数据既可以存在共享表空间文件（文件名：ibdata1）里，也可以存放在独占表空间文件（文件名：表名字.ibd）。这个行为是由参数 innodb_file_per_table 控制的，若设置了参数 innodb_file_per_table 为 1，则会将存储的数据、索引等信息单独存储在一个独占表空间，从 MySQL 5.6.6 版本开始，它的默认值就是 1 了，因此从这个版本之后， MySQL 中每一张表的数据都存放在一个独立的 .ibd 文件。 好了，现在我们知道了一张数据库表的数据是保存在「 表名字.ibd 」的文件里的，这个文件也称为独占表空间文件。\n2.2 表空间文件的结构 # 表空间由段（segment）、区（extent）、页（page）、行（row）组成，InnoDB存储引擎的逻辑存储结构大致如下图：\n下面我们从下往上一个个看看。\n2.2.1 行（row） # 数据库表中的记录都是按行（row）进行存放的，每行记录根据不同的行格式，有不同的存储结构。\n后面我们详细介绍 InnoDB 存储引擎的行格式，也是本文重点介绍的内容。\n2.2.2 页（page） # 记录是按照行来存储的，但是数据库的读取并不以「行」为单位，否则一次读取（也就是一次 I/O 操作）只能处理一行数据，效率会非常低。\n因此，InnoDB 的数据是按「页」为单位来读写的，也就是说，当需要读一条记录的时候，并不是将这个行记录从磁盘读出来，而是以页为单位，将其整体读入内存。\n默认每个页的大小为 16KB，也就是最多能保证 16KB 的连续存储空间。\n页是 InnoDB 存储引擎磁盘管理的最小单元，意味着数据库每次读写都是以 16KB 为单位的，一次最少从磁盘中读取 16K 的内容到内存中，一次最少把内存中的 16K 内容刷新到磁盘中。\n页的类型有很多，常见的有数据页、undo 日志页、溢出页等等。数据表中的行记录是用「数据页」来管理的。\n总之知道表中的记录存储在「数据页」里面就行。\n2.2.3 区（extent） # 我们知道 InnoDB 存储引擎是用 B+ 树来组织数据的。\nB+ 树中每一层都是通过双向链表连接起来的，如果是以页为单位来分配存储空间，那么链表中相邻的两个页之间的物理位置并不是连续的，可能离得非常远，那么磁盘查询时就会有大量的随机I/O，随机 I/O 是非常慢的。\n解决这个问题也很简单，就是让链表中相邻的页的物理位置也相邻，这样就可以使用顺序 I/O 了，那么在范围查询（扫描叶子节点）的时候性能就会很高。\n那具体怎么解决呢？\n在表中数据量大的时候，为某个索引分配空间的时候就不再按照页为单位分配了，而是按照区（extent）为单位分配。每个区的大小为 1MB，对于 16KB 的页来说，连续的 64 个页会被划为一个区，这样就使得链表中相邻的页的物理位置也相邻，就能使用顺序 I/O 了。\n2.2.4 段（segment） # 表空间是由各个段（segment）组成的，段是由多个区（extent）组成的。段一般分为数据段、索引段和回滚段等。\n索引段：存放 B + 树的非叶子节点的区的集合； 数据段：存放 B + 树的叶子节点的区的集合； 回滚段：存放的是回滚数据的区的集合，MVCC 利用了回滚段实现了多版本查询数据。 好了，终于说完表空间的结构了。接下来，就具体讲一下 InnoDB 的行格式了。\n之所以要绕一大圈才讲行记录的格式，主要是想让大家知道行记录是存储在哪个文件，以及行记录在这个表空间文件中的哪个区域，有一个从上往下切入的视角，这样理解起来不会觉得很抽象。\n2.3 InnoDB 行格式 # 行格式（row_format），就是一条记录的存储结构。\nInnoDB 提供了 4 种行格式，分别是 Redundant、Compact、Dynamic和 Compressed 行格式。\nRedundant 是很古老的行格式了， MySQL 5.0 版本之前用的行格式，现在基本没人用了。 由于 Redundant 不是一种紧凑的行格式，所以 MySQL 5.0 之后引入了 Compact 行记录存储方式，Compact 是一种紧凑的行格式，设计的初衷就是为了让一个数据页中可以存放更多的行记录，从 MySQL 5.1 版本之后，行格式默认设置成 Compact。 Dynamic 和 Compressed 两个都是紧凑的行格式，它们的行格式都和 Compact 差不多，因为都是基于 Compact 改进一点东西。从 MySQL5.7 版本之后，默认使用 Dynamic 行格式。 Redundant 行格式我这里就不讲了，因为现在基本没人用了，这次重点介绍 Compact 行格式，因为 Dynamic 和 Compressed 这两个行格式跟 Compact 非常像。\n2.4 COMPACT 行格式 # 可以看到，一条完整的记录分为「记录的额外信息」和「记录的真实数据」两个部分。\n接下里，分别详细说下。\n2.4.1 记录的额外信息 # 记录的额外信息包含 3 个部分：变长字段长度列表、NULL 值列表、记录头信息。\n1.变长字段长度列表\nvarchar(n) 和 char(n) 的区别是什么，相信大家都非常清楚，char 是定长的，varchar 是变长的，变长字段实际存储的数据的长度（大小）不固定的。\n所以，在存储数据的时候，也要把数据占用的大小存起来，存到「变长字段长度列表」里面，读取数据的时候才能根据这个「变长字段长度列表」去读取对应长度的数据。其他 TEXT、BLOB 等变长字段也是这么实现的。\n为了展示「变长字段长度列表」具体是怎么保存「变长字段的真实数据占用的字节数」，我们先创建这样一张表，字符集是 ascii（所以每一个字符占用的 1 字节），行格式是 Compact，t_user 表中 name 和 phone 字段都是变长字段：\nCREATE TABLE `t_user` ( `id` int(11) NOT NULL, `name` VARCHAR(20) DEFAULT NULL, `phone` VARCHAR(20) DEFAULT NULL, `age` int(11) DEFAULT NULL, PRIMARY KEY (`id`) USING BTREE ) ENGINE = InnoDB DEFAULT CHARACTER SET = ascii ROW_FORMAT = COMPACT; 现在 t_user 表里有这三条记录：\n接下来，我们看看看看这三条记录的行格式中的 「变长字段长度列表」是怎样存储的。\n先来看第一条记录：\nname 列的值为 a，真实数据占用的字节数是 1 字节，十六进制 0x01； phone 列的值为 123，真实数据占用的字节数是 3 字节，十六进制 0x03； age 列和 id 列不是变长字段，所以这里不用管。 这些变长字段的真实数据占用的字节数会按照列的顺序逆序存放（等下会说为什么要这么设计），所以「变长字段长度列表」里的内容是「 03 01」，而不是 「01 03」。\n同样的道理，我们也可以得出第二条记录的行格式中，「变长字段长度列表」里的内容是「 04 02」，如下图：\n第三条记录中 phone 列的值是 NULL，NULL 是不会存放在行格式中记录的真实数据部分里的，所以「变长字段长度列表」里不需要保存值为 NULL 的变长字段的长度。\n为什么「变长字段长度列表」的信息要按照逆序存放？\n这个设计是有想法的，主要是因为「记录头信息」中指向下一个记录的指针，指向的是下一条记录的「记录头信息」和「真实数据」之间的位置，这样的好处是向左读就是记录头信息，向右读就是真实数据，比较方便。\n「变长字段长度列表」中的信息之所以要逆序存放，是因为这样可以使得位置靠前的记录的真实数据和数据对应的字段长度信息可以同时在一个 CPU Cache Line 中，这样就可以提高 CPU Cache 的命中率。\n同样的道理， NULL 值列表的信息也需要逆序存放。\n每个数据库表的行格式都有「变长字段字节数列表」吗？\n其实变长字段字节数列表不是必须的。\n当数据表没有变长字段的时候，比如全部都是 int 类型的字段，这时候表里的行格式就不会有「变长字段长度列表」了，因为没必要，不如去掉以节省空间。\n所以「变长字段长度列表」只出现在数据表有变长字段的时候。\n2.NULL 值列表\n表中的某些列可能会存储 NULL 值，如果把这些 NULL 值都放到记录的真实数据中会比较浪费空间，所以 Compact 行格式把这些值为 NULL 的列存储到 NULL值列表中。\n如果存在允许 NULL 值的列，则每个列对应一个二进制位（bit），二进制位按照列的顺序逆序排列。\n二进制位的值为1时，代表该列的值为NULL。 二进制位的值为0时，代表该列的值不为NULL。 另外，NULL 值列表必须用整数个字节的位表示（1字节8位），如果使用的二进制位个数不足整数个字节，则在字节的高位补 0。\n还是以 t_user 表的这三条记录作为例子：\n接下来，我们看看看看这三条记录的行格式中的 NULL 值列表是怎样存储的。\n先来看第一条记录，第一条记录所有列都有值，不存在 NULL 值，所以用二进制来表示是酱紫的：\n但是 InnoDB 是用整数字节的二进制位来表示 NULL 值列表的，现在不足 8 位，所以要在高位补 0，最终用二进制来表示是酱紫的：\n所以，对于第一条数据，NULL 值列表用十六进制表示是 0x00。\n接下来看第二条记录，第二条记录 age 列是 NULL 值，所以，对于第二条数据，NULL值列表用十六进制表示是 0x04。\n最后第三条记录，第三条记录 phone 列 和 age 列是 NULL 值，所以，对于第三条数据，NULL 值列表用十六进制表示是 0x06。\n我们把三条记录的 NULL 值列表都填充完毕后，它们的行格式是这样的：\n每个数据库表的行格式都有「NULL 值列表」吗？\nNULL 值列表也不是必须的。\n当数据表的字段都定义成 NOT NULL 的时候，这时候表里的行格式就不会有 NULL 值列表了。\n所以在设计数据库表的时候，通常都是建议将字段设置为 NOT NULL，这样可以至少节省 1 字节的空间（NULL 值列表至少占用 1 字节空间）。\n「NULL 值列表」是固定 1 字节空间吗？如果这样的话，一条记录有 9 个字段值都是 NULL，这时候怎么表示？\n「NULL 值列表」的空间不是固定 1 字节的。\n当一条记录有 9 个字段值都是 NULL，那么就会创建 2 字节空间的「NULL 值列表」，以此类推。\n3.记录头信息\n记录头信息中包含的内容很多，我就不一一列举了，这里说几个比较重要的：\ndelete_mask ：标识此条数据是否被删除。从这里可以知道，我们执行 detele 删除记录的时候，并不会真正的删除记录，只是将这个记录的 delete_mask 标记为 1。 next_record：下一条记录的位置。从这里可以知道，记录与记录之间是通过链表组织的。在前面我也提到了，指向的是下一条记录的「记录头信息」和「真实数据」之间的位置，这样的好处是向左读就是记录头信息，向右读就是真实数据，比较方便。 record_type：表示当前记录的类型，0表示普通记录，1表示B+树非叶子节点记录，2表示最小记录，3表示最大记录 2.4.2 记录的真实数据 # 记录真实数据部分除了我们定义的字段，还有三个隐藏字段，分别为：row_id、trx_id、roll_pointer，我们来看下这三个字段是什么。\nrow_id 如果我们建表的时候指定了主键或者唯一约束列，那么就没有 row_id 隐藏字段了。如果既没有指定主键，又没有唯一约束，那么 InnoDB 就会为记录添加 row_id 隐藏字段。row_id不是必需的，占用 6 个字节。\ntrx_id 事务id，表示这个数据是由哪个事务生成的。 trx_id是必需的，占用 6 个字节。\nroll_pointer 这条记录上一个版本的指针。roll_pointer 是必需的，占用 7 个字节。\n","date":"22 September 2023","permalink":"/basic/mysql/review/01/","section":"Basics","summary":"MySQL 相关知识复习","title":"MySQL 基础"},{"content":"","date":"21 September 2023","permalink":"/tags/redis/","section":"Tags","summary":"","title":"Redis"},{"content":"Redis 相关知识复习\n1.基础知识 # Redis：基于内存的数据库，数据的读写操作都是在内存中，常用于缓存、消息队列、分布式锁\nRedis 对数据类型的操作都是原子性的（因为执行命令由单线程负责，不存在并发竞争问题）\nRedis 可以作为 MySQL 的缓存，因为 Redis 具备高性能和高并发两种特性\n2.Redis 数据结构 # Redis 提供了丰富的数据类型，常见的有五种数据类型：String（字符串），Hash（哈希），List（列表），Set（集合）、Zset（有序集合）\n随着 Redis 版本的更新，后面又支持了四种数据类型： BitMap（2.2 版新增）、HyperLogLog（2.8 版新增）、GEO（3.2 版新增）、Stream（5.0 版新增）。 Redis 五种数据类型的应用场景：\nString 类型的应用场景：缓存对象、常规计数、分布式锁、共享 session 信息等。 List 类型的应用场景：消息队列（但是有两个问题：1. 生产者需要自行实现全局唯一 ID；2. 不能以消费组形式消费数据）等。 Hash 类型：缓存对象、购物车等。 Set 类型：聚合计算（并集、交集、差集）场景，比如点赞、共同关注、抽奖活动等。 Zset 类型：排序场景，比如排行榜、电话和姓名排序等。 Redis 后续版本又支持四种数据类型，它们的应用场景如下：\nBitMap（2.2 版新增）：二值状态统计的场景，比如签到、判断用户登陆状态、连续签到用户总数等； HyperLogLog（2.8 版新增）：海量数据基数统计的场景，比如百万级网页 UV 计数等； GEO（3.2 版新增）：存储地理位置信息的场景，比如滴滴叫车； Stream（5.0 版新增）：消息队列，相比于基于 List 类型实现的消息队列，有这两个特有的特性：自动生成全局唯一消息ID，支持以消费组形式消费数据。 3.Redis 线程模型 # Redis 单线程指的是「接收客户端请求-\u0026gt;解析请求 -\u0026gt;进行数据读写等操作-\u0026gt;发送数据给客户端」这个过程是由一个线程（主线程）来完成的，这也是我们常说 Redis 是单线程的原因。\n但是，Redis 程序并不是单线程的，Redis 在启动的时候，是会启动后台线程（BIO）的：\nRedis 在 2.6 版本，会启动 2 个后台线程，分别处理关闭文件、AOF 刷盘这两个任务； Redis 在 4.0 版本之后，新增了一个新的后台线程，用来异步释放 Redis 内存，也就是 lazyfree 线程。例如执行 unlink key / flushdb async / flushall async 等命令，会把这些删除操作交给后台线程来执行，好处是不会导致 Redis 主线程卡顿。因此，当我们要删除一个大 key 的时候，不要使用 del 命令删除，因为 del 是在主线程处理的，这样会导致 Redis 主线程卡顿，因此我们应该使用 unlink 命令来异步删除大key。 之所以 Redis 为「关闭文件、AOF 刷盘、释放内存」这些任务创建单独的线程来处理，是因为这些任务的操作都是很耗时的，如果把这些任务都放在主线程来处理，那么 Redis 主线程就很容易发生阻塞，这样就无法处理后续的请求了。\n后台线程相当于一个消费者，生产者把耗时任务丢到任务队列中，消费者（BIO）不停轮询这个队列，拿出任务就去执行对应的方法即可。\n关闭文件、AOF 刷盘、释放内存这三个任务都有各自的任务队列：\nBIO_CLOSE_FILE，关闭文件任务队列：当队列有任务后，后台线程会调用 close(fd) ，将文件关闭； BIO_AOF_FSYNC，AOF刷盘任务队列：当 AOF 日志配置成 everysec 选项后，主线程会把 AOF 写日志操作封装成一个任务，也放到队列中。当发现队列有任务后，后台线程会调用 fsync(fd)，将 AOF 文件刷盘， BIO_LAZY_FREE，lazy free 任务队列：当队列有任务后，后台线程会 free(obj) 释放对象 / free(dict) 删除数据库所有对象 / free(skiplist) 释放跳表对象； 3.1 Redis 单线程模式 # 图中的蓝色部分是一个事件循环，是由主线程负责的，可以看到网络 I/O 和命令处理都是单线程。 Redis 初始化的时候，会做下面这几件事情：\n首先，调用 epoll_create() 创建一个 epoll 对象和调用 socket() 创建一个服务端 socket 然后，调用 bind() 绑定端口和调用 listen() 监听该 socket； 然后，将调用 epoll_ctl() 将 listen socket 加入到 epoll，同时注册「连接事件」处理函数。 初始化完后，主线程就进入到一个事件循环函数，主要会做以下事情：\n首先，先调用处理发送队列函数，看是发送队列里是否有任务，如果有发送任务，则通过 write 函数将客户端发送缓存区里的数据发送出去，如果这一轮数据没有发送完，就会注册写事件处理函数，等待 epoll_wait 发现可写后再处理 。 接着，调用 epoll_wait 函数等待事件的到来： 如果是连接事件到来，则会调用连接事件处理函数，该函数会做这些事情：调用 accpet 获取已连接的 socket -\u0026gt; 调用 epoll_ctl 将已连接的 socket 加入到 epoll -\u0026gt; 注册「读事件」处理函数； 如果是读事件到来，则会调用读事件处理函数，该函数会做这些事情：调用 read 获取客户端发送的数据 -\u0026gt; 解析命令 -\u0026gt; 处理命令 -\u0026gt; 将客户端对象添加到发送队列 -\u0026gt; 将执行结果写到发送缓存区等待发送； 如果是写事件到来，则会调用写事件处理函数，该函数会做这些事情：通过 write 函数将客户端发送缓存区里的数据发送出去，如果这一轮数据没有发送完，就会继续注册写事件处理函数，等待 epoll_wait 发现可写后再处理 。 Redis 采用单线程（网络 I/O 和执行命令）那么快，有如下几个原因：\nRedis 的大部分操作都在内存中完成，并且采用了高效的数据结构，因此 Redis 瓶颈可能是机器的内存或者网络带宽，而并非 CPU，既然 CPU 不是瓶颈，那么自然就采用单线程的解决方案了； Redis 采用单线程模型可以避免了多线程之间的竞争，省去了多线程切换带来的时间和性能上的开销，而且也不会导致死锁问题。 Redis 采用了 I/O 多路复用机制处理大量的客户端 Socket 请求，IO 多路复用机制是指一个线程处理多个 IO 流，就是我们经常听到的 select/epoll 机制。简单来说，在 Redis 只运行单线程的情况下，该机制允许内核中，同时存在多个监听 Socket 和已连接 Socket。内核会一直监听这些 Socket 上的连接请求或数据请求。一旦有请求到达，就会交给 Redis 线程处理，这就实现了一个 Redis 线程处理多个 IO 流的效果。 3.2 Redis 性能瓶颈 # Redis的性能瓶颈有时会出现在网络I/O的处理上，这是因为Redis是一个内存数据库，它的性能主要受限于以下几个因素：\n单线程模型： Redis 采用了单线程模型，意味着所有的命令都在一个单独的线程中执行。这个线程负责处理所有的客户端请求、命令解析、数据读写和持久化操作。虽然这样的设计简单且易于维护，但在高并发的情况下，单线程的性能可能会受限。 内存访问： Redis 的数据通常存储在内存中，因此快速的内存访问对性能至关重要。然而，当数据量很大时，内存访问可能会变得相对较慢，因为需要更多的内存管理和缓存机制来处理大量的数据。 网络 I/O： Redis 通常用于客户端和服务器之间的通信，因此网络I/O的性能对于Redis非常关键。在高并发情况下，大量的客户端请求需要在网络上传输，而网络带宽和延迟可能会成为性能瓶颈。 持久化操作： 如果 Redis 启用了持久化操作（如 RDB 快照或 AOF 日志），这些操作可能会对性能产生影响。在持久化期间，Redis需要将数据写入磁盘，这可能会导致磁盘 I/O 成为性能瓶颈。 解决Redis性能瓶颈的方法包括：\n使用多实例： 在多核 CPU 上运行多个 Redis 实例，每个实例可以处理不同的命令或数据集，以提高并发性能。 使用缓存： 将热点数据缓存在 Redis 中，以减少对数据库的访问。 使用集群： Redis 提供了集群模式，可以在多个 Redis 节点之间分布数据和负载，以提高性能和可伸缩性。 优化网络配置： 调整网络配置，包括 TCP 参数、最大连接数和超时设置，以改善网络 I/O 性能. 合理使用持久化： 根据需求合理配置持久化操作，可以选择关闭持久化或将持久化数据存储在快速的存储介质上。 3.3 Redis 6.0 之后引入多线程 # 虽然 Redis 的主要工作（网络 I/O 和执行命令）一直是单线程模型，但是在 Redis 6.0 版本之后，也采用了多个 I/O 线程来处理网络请求，这是因为随着网络硬件的性能提升，Redis 的性能瓶颈有时会出现在网络 I/O 的处理上。\n所以为了提高网络 I/O 的并行度，Redis 6.0 对于网络 I/O 采用多线程来处理。但是对于命令的执行，Redis 仍然使用单线程来处理，所以大家不要误解 Redis 有多线程同时执行命令。\nRedis 官方表示，Redis 6.0 版本引入的多线程 I/O 特性对性能提升至少是一倍以上。\nRedis 6.0 版本支持的 I/O 多线程特性，默认情况下 I/O 多线程只针对发送响应数据（write client socket），并不会以多线程的方式处理读请求（read client socket）。要想开启多线程处理客户端读请求，就需要把 Redis.conf 配置文件中的 io-threads-do-reads 配置项设为 yes。\n//读请求也使用io多线程 io-threads-do-reads yes 同时， Redis.conf 配置文件中提供了 IO 多线程个数的配置项。\n// io-threads N，表示启用 N-1 个 I/O 多线程（主线程也算一个 I/O 线程） io-threads 4 关于线程数的设置，官方的建议是如果为 4 核的 CPU，建议线程数设置为 2 或 3，如果为 8 核 CPU 建议线程数设置为 6，线程数一定要小于机器核数，线程数并不是越大越好。\n因此， Redis 6.0 版本之后，Redis 在启动的时候，默认情况下会额外创建 6 个线程（这里的线程数不包括主线程）：\nRedis-server ： Redis的主线程，主要负责执行命令； bio_close_file、bio_aof_fsync、bio_lazy_free：三个后台线程，分别异步处理关闭文件任务、AOF刷盘任务、释放内存任务； io_thd_1、io_thd_2、io_thd_3：三个 I/O 线程，io-threads 默认是 4 ，所以会启动 3（4-1）个 I/O 多线程，用来分担 Redis 网络 I/O 的压力。 4.Redis 持久化 # Redis 的读写操作都是在内存中，所以 Redis 性能才会高，但是当 Redis 重启后，内存中的数据就会丢失，那为了保证内存中的数据不会丢失，Redis 实现了数据持久化的机制，这个机制会把数据存储到磁盘，这样在 Redis 重启就能够从磁盘中恢复原有的数据。\nRedis 共有三种数据持久化的方式：\nAOF 日志：每执行一条写操作命令，就把该命令以追加的方式写入到一个文件里； RDB 快照：将某一时刻的内存数据，以二进制的方式写入磁盘； 混合持久化方式：Redis 4.0 新增的方式，集成了 AOF 和 RDB 的优点； 4.1 AOF 日志 # Redis 在执行完一条写操作命令后，就会把该命令以追加的方式写入到一个文件里，然后 Redis 重启时，会读取该文件记录的命令，然后逐一执行命令的方式来进行数据恢复。\n我这里以「set name xiaolin」命令作为例子，Redis 执行了这条命令后，记录在 AOF 日志里的内容如下图：\n为什么先执行命令，再把数据写入日志呢？\nReids 是先执行写操作命令后，才将该命令记录到 AOF 日志里的，这么做其实有两个好处。\n避免额外的检查开销：因为如果先将写操作命令记录到 AOF 日志里，再执行该命令的话，如果当前的命令语法有问题，那么如果不进行命令语法检查，该错误的命令记录到 AOF 日志里后，Redis 在使用日志恢复数据时，就可能会出错。 不会阻塞当前写操作命令的执行：因为当写操作命令执行成功后，才会将命令记录到 AOF 日志。 当然，这样做也会带来风险：\n数据可能会丢失： 执行写操作命令和记录日志是两个过程，那当 Redis 在还没来得及将命令写入到硬盘时，服务器发生宕机了，这个数据就会有丢失的风险。 可能阻塞其他操作： 由于写操作命令执行成功后才记录到 AOF 日志，所以不会阻塞当前命令的执行，但因为 AOF 日志也是在主线程中执行，所以当 Redis 把日志文件写入磁盘的时候，还是会阻塞后续的操作无法执行。 AOF 写回策略有几种？\nRedis 写入 AOF 日志的过程，如下图：\n具体说说：\nRedis 执行完写操作命令后，会将命令追加到 server.aof_buf 缓冲区； 然后通过 write() 系统调用，将 aof_buf 缓冲区的数据写入到 AOF 文件，此时数据并没有写入到硬盘，而是拷贝到了内核缓冲区 page cache，等待内核将数据写入硬盘； 具体内核缓冲区的数据什么时候写入到硬盘，由内核决定。 Redis 提供了 3 种写回硬盘的策略，控制的就是上面说的第三步的过程。 在 Redis.conf 配置文件中的 appendfsync 配置项可以有以下 3 种参数可填：\nAlways，这个单词的意思是「总是」，所以它的意思是每次写操作命令执行完后，同步将 AOF 日志数据写回硬盘； Everysec，这个单词的意思是「每秒」，所以它的意思是每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，然后每隔一秒将缓冲区里的内容写回到硬盘； No，意味着不由 Redis 控制写回硬盘的时机，转交给操作系统控制写回的时机，也就是每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，再由操作系统决定何时将缓冲区内容写回硬盘。 AOF 日志过大，会触发什么机制？\nAOF 日志是一个文件，随着执行的写操作命令越来越多，文件的大小会越来越大。 如果当 AOF 日志文件过大就会带来性能问题，比如重启 Redis 后，需要读 AOF 文件的内容以恢复数据，如果文件过大，整个恢复的过程就会很慢。\n所以，Redis 为了避免 AOF 文件越写越大，提供了 AOF 重写机制，当 AOF 文件的大小超过所设定的阈值后，Redis 就会启用 AOF 重写机制，来压缩 AOF 文件。\nAOF 重写机制是在重写时，读取当前数据库中的所有键值对，然后将每一个键值对用一条命令记录到「新的 AOF 文件」，等到全部记录完后，就将新的 AOF 文件替换掉现有的 AOF 文件。\n举个例子，在没有使用重写机制前，假设前后执行了「set name xiaolin」和「set name xiaolincoding」这两个命令的话，就会将这两个命令记录到 AOF 文件。\n但是在使用重写机制后，就会读取 name 最新的 value（键值对） ，然后用一条 「set name xiaolincoding」命令记录到新的 AOF 文件，之前的第一个命令就没有必要记录了，因为它属于「历史」命令，没有作用了。这样一来，一个键值对在重写日志中只用一条命令就行了。\n重写工作完成后，就会将新的 AOF 文件覆盖现有的 AOF 文件，这就相当于压缩了 AOF 文件，使得 AOF 文件体积变小了。\n重写 AOF 日志的过程是怎样的？\nRedis 的重写 AOF 过程是由后台子进程 bgrewriteaof 来完成的，这么做可以达到两个好处：\n子进程进行 AOF 重写期间，主进程可以继续处理命令请求，从而避免阻塞主进程； 子进程带有主进程的数据副本，这里使用子进程而不是线程，因为如果是使用线程，多线程之间会共享内存，那么在修改共享内存数据的时候，需要通过加锁来保证数据的安全，而这样就会降低性能。而使用子进程，创建子进程时，父子进程是共享内存数据的，不过这个共享的内存只能以只读的方式，而当父子进程任意一方修改了该共享内存，就会发生「写时复制」，于是父子进程就有了独立的数据副本，就不用加锁来保证数据安全。 触发重写机制后，主进程就会创建重写 AOF 的子进程，此时父子进程共享物理内存，重写子进程只会对这个内存进行只读，重写 AOF 子进程会读取数据库里的所有数据，并逐一把内存数据的键值对转换成一条命令，再将命令记录到重写日志（新的 AOF 文件）。\n但是重写过程中，主进程依然可以正常处理命令，那问题来了，重写 AOF 日志过程中，如果主进程修改了已经存在 key-value，那么会发生写时复制，此时这个 key-value 数据在子进程的内存数据就跟主进程的内存数据不一致了，这时要怎么办呢？\n为了解决这种数据不一致问题，Redis 设置了一个 AOF 重写缓冲区，这个缓冲区在创建 bgrewriteaof 子进程之后开始使用。\n在重写 AOF 期间，当 Redis 执行完一个写命令之后，它会同时将这个写命令写入到 「AOF 缓冲区」和 「AOF 重写缓冲区」。\n也就是说，在 bgrewriteaof 子进程执行 AOF 重写期间，主进程需要执行以下三个工作:\n执行客户端发来的命令； 将执行后的写命令追加到 「AOF 缓冲区」； 将执行后的写命令追加到 「AOF 重写缓冲区」； 当子进程完成 AOF 重写工作（扫描数据库中所有数据，逐一把内存数据的键值对转换成一条命令，再将命令记录到重写日志）后，会向主进程发送一条信号，信号是进程间通讯的一种方式，且是异步的。\n主进程收到该信号后，会调用一个信号处理函数，该函数主要做以下工作：\n将 AOF 重写缓冲区中的所有内容追加到新的 AOF 的文件中，使得新旧两个 AOF 文件所保存的数据库状态一致； 新的 AOF 的文件进行改名，覆盖现有的 AOF 文件。 信号函数执行完后，主进程就可以继续像往常一样处理命令了。\nRedis 的 AOF 是在主线程进行的还是子线程？\nRedis 的 AOF（Append-Only File）持久化操作是在主线程中进行的，而不是在子线程中。\nRedis 的主要线程负责处理所有的客户端请求、命令执行、数据读写、AOF 日志的更新等任务。当有命令需要被持久化到 AOF 文件时，Redis 主线程会将该命令追加到 AOF 缓冲区中，然后异步地将 AOF 缓冲区中的内容写入 AOF 文件。这个写入操作通常在后台执行，不会阻塞主线程的正常工作。\n由于 AOF 持久化是以追加方式进行的，它不会直接修改已有的 AOF 文件，而是在文件末尾追加新的命令。这意味着即使 AOF 写入操作失败或中断，也不会影响到原有的 AOF 文件的完整性，因此 AOF 持久化具有一定的容错性。\n总之，Redis 的 AOF 持久化是在主线程中进行的，但不会阻塞主线程的正常处理能力，因为 AOF 写入操作通常是异步的。\n4.2 RDB 快照 # 因为 AOF 日志记录的是操作命令，不是实际的数据，所以用 AOF 方法做故障恢复时，需要全量把日志都执行一遍，一旦 AOF 日志非常多，势必会造成 Redis 的恢复操作缓慢。\n为了解决这个问题，Redis 增加了 RDB 快照。所谓的快照，就是记录某一个瞬间东西，比如当我们给风景拍照时，那一个瞬间的画面和信息就记录到了一张照片。\n所以，RDB 快照就是记录某一个瞬间的内存数据，记录的是实际数据，而 AOF 文件记录的是命令操作的日志，而不是实际的数据。\n因此在 Redis 恢复数据时， RDB 恢复数据的效率会比 AOF 高些，因为直接将 RDB 文件读入内存就可以，不需要像 AOF 那样还需要额外执行操作命令的步骤才能恢复数据。\nRDB 做快照时会阻塞线程吗？\nRedis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave，他们的区别就在于是否在「主线程」里执行：\n执行了 save 命令，就会在主线程生成 RDB 文件，由于和执行操作命令在同一个线程，所以如果写入 RDB 文件的时间太长，会阻塞主线程； 执行了 bgsave 命令，会创建一个子进程来生成 RDB 文件，这样可以避免主线程的阻塞； Redis 还可以通过配置文件的选项来实现每隔一段时间自动执行一次 bgsave 命令，默认会提供以下配置：\nsave 900 1 save 300 10 save 60 10000 别看选项名叫 save，实际上执行的是 bgsave 命令，也就是会创建子进程来生成 RDB 快照文件。 只要满足上面条件的任意一个，就会执行 bgsave，它们的意思分别是：\n900 秒之内，对数据库进行了至少 1 次修改； 300 秒之内，对数据库进行了至少 10 次修改； 60 秒之内，对数据库进行了至少 10000 次修改。 这里提一点，Redis 的快照是全量快照，也就是说每次执行快照，都是把内存中的「所有数据」都记录到磁盘中。所以执行快照是一个比较重的操作，如果频率太频繁，可能会对 Redis 性能产生影响。如果频率太低，服务器故障时，丢失的数据会更多。\nRDB 在执行快照的时候，数据能修改吗？\n可以的，执行 bgsave 过程中，Redis 依然可以继续处理操作命令的，也就是数据是能被修改的，关键的技术就在于写时复制技术（Copy-On-Write, COW）。\n执行 bgsave 命令的时候，会通过 fork() 创建子进程，此时子进程和父进程是共享同一片内存数据的，因为创建子进程的时候，会复制父进程的页表，但是页表指向的物理内存还是一个，此时如果主线程执行读操作，则主线程和 bgsave 子进程互相不影响。\n如果主线程执行写操作，则被修改的数据会复制一份副本，然后 bgsave 子进程会把该副本数据写入 RDB 文件，在这个过程中，主线程仍然可以直接修改原来的数据。\n4.3 混合持久化 # RDB 优点是数据恢复速度快，但是快照的频率不好把握。频率太低，丢失的数据就会比较多，频率太高，就会影响性能。\nAOF 优点是丢失数据少，但是数据恢复不快。\n为了集成了两者的优点， Redis 4.0 提出了混合使用 AOF 日志和内存快照，也叫混合持久化，既保证了 Redis 重启速度，又降低数据丢失风险。\n混合持久化工作在 AOF 日志重写过程，当开启了混合持久化时，在 AOF 重写日志时，fork 出来的重写子进程会先将与主线程共享的内存数据以 RDB 方式写入到 AOF 文件，然后主线程处理的操作命令会被记录在重写缓冲区里，重写缓冲区里的增量命令会以 AOF 方式写入到 AOF 文件，写入完成后通知主进程将新的含有 RDB 格式和 AOF 格式的 AOF 文件替换旧的的 AOF 文件。\n也就是说，使用了混合持久化，AOF 文件的前半部分是 RDB 格式的全量数据，后半部分是 AOF 格式的增量数据。\n这样的好处在于，重启 Redis 加载数据的时候，由于前半部分是 RDB 内容，这样加载的时候速度会很快。\n加载完 RDB 的内容后，才会加载后半部分的 AOF 内容，这里的内容是 Redis 后台子进程重写 AOF 期间，主线程处理的操作命令，可以使得数据更少的丢失。\n混合持久化优点：\n混合持久化结合了 RDB 和 AOF 持久化的优点，开头为 RDB 的格式，使得 Redis 可以更快的启动，同时结合 AOF 的优点，有减低了大量数据丢失的风险。 混合持久化缺点：\nAOF 文件中添加了 RDB 格式的内容，使得 AOF 文件的可读性变得很差； 兼容性差，如果开启混合持久化，那么此混合持久化 AOF 文件，就不能用在 Redis 4.0 之前版本了。 5.Redis 过期删除与内存淘汰 # 5.1 Redis 的过期删除策略 # Redis 是可以对 key 设置过期时间的，因此需要有相应的机制将已过期的键值对删除，而做这个工作的就是过期键值删除策略。\n每当我们对一个 key 设置了过期时间时，Redis 会把该 key 带上过期时间存储到一个过期字典（expires dict）中，也就是说「过期字典」保存了数据库中所有 key 的过期时间。\n当我们查询一个 key 时，Redis 首先检查该 key 是否存在于过期字典中：\n如果不在，则正常读取键值； 如果存在，则会获取该 key 的过期时间，然后与当前系统时间进行比对，如果比系统时间大，那就没有过期，否则判定该 key 已过期。 Redis 使用的过期删除策略是「惰性删除+定期删除」这两种策略配和使用。\n什么是惰性删除策略？\n惰性删除策略的做法是，不主动删除过期键，每次从数据库访问 key 时，都检测 key 是否过期，如果过期则删除该 key。\n惰性删除的流程图如下：\n惰性删除策略的优点：\n因为每次访问时，才会检查 key 是否过期，所以此策略只会使用很少的系统资源，因此，惰性删除策略对 CPU 时间最友好。 惰性删除策略的缺点：\n如果一个 key 已经过期，而这个 key 又仍然保留在数据库中，那么只要这个过期 key 一直没有被访问，它所占用的内存就不会释放，造成了一定的内存空间浪费。所以，惰性删除策略对内存不友好。 什么是定期删除策略？\n定期删除策略的做法是，每隔一段时间「随机」从数据库中取出一定数量的 key 进行检查，并删除其中的过期key。\nRedis 的定期删除的流程：\n从过期字典中随机抽取 20 个 key； 检查这 20 个 key 是否过期，并删除已过期的 key； 如果本轮检查的已过期 key 的数量，超过 5 个（20/4），也就是「已过期 key 的数量」占比「随机抽取 key 的数量」大于 25%，则继续重复步骤 1；如果已过期的 key 比例小于 25%，则停止继续删除过期 key，然后等待下一轮再检查。 可以看到，定期删除是一个循环的流程。那 Redis 为了保证定期删除不会出现循环过度，导致线程卡死现象，为此增加了定期删除循环流程的时间上限，默认不会超过 25ms。\n定期删除的流程如下：\n定期删除策略的优点：\n通过限制删除操作执行的时长和频率，来减少删除操作对 CPU 的影响，同时也能删除一部分过期的数据减少了过期键对空间的无效占用。 定期删除策略的缺点：\n难以确定删除操作执行的时长和频率。如果执行的太频繁，就会对 CPU 不友好；如果执行的太少，那又和惰性删除一样了，过期 key 占用的内存不会及时得到释放。 可以看到，惰性删除策略和定期删除策略都有各自的优点，所以 Redis 选择「惰性删除+定期删除」这两种策略配和使用，以求在合理使用 CPU 时间和避免内存浪费之间取得平衡。\n5.2 Redis 持久化时，如何处理过期键 # Redis 持久化文件有两种格式：RDB（Redis Database）和 AOF（Append Only File），分别来看过期键在这两种格式中的呈现状态。\nRDB 文件分为两个阶段，RDB 文件生成阶段和加载阶段。\nRDB 文件生成阶段：从内存状态持久化成 RDB（文件）的时候，会对 key 进行过期检查，过期的键「不会」被保存到新的 RDB 文件中，因此 Redis 中的过期键不会对生成新 RDB 文件产生任何影响。 RDB 加载阶段：RDB 加载阶段时，要看服务器是主服务器还是从服务器，分别对应以下两种情况： 如果 Redis 是「主服务器」运行模式的话，在载入 RDB 文件时，程序会对文件中保存的键进行检查，过期键「不会」被载入到数据库中。所以过期键不会对载入 RDB 文件的主服务器造成影响； 如果 Redis 是「从服务器」运行模式的话，在载入 RDB 文件时，不论键是否过期都会被载入到数据库中。但由于主从服务器在进行数据同步时，从服务器的数据会被清空。所以一般来说，过期键对载入 RDB 文件的从服务器也不会造成影响。 AOF 文件分为两个阶段，AOF 文件写入阶段和 AOF 重写阶段。\nAOF 文件写入阶段：当 Redis 以 AOF 模式持久化时，如果数据库某个过期键还没被删除，那么 AOF 文件会保留此过期键，当此过期键被删除后，Redis 会向 AOF 文件追加一条 DEL 命令来显式地删除该键值。 AOF 重写阶段：执行 AOF 重写时，会对 Redis 中的键值对进行检查，已过期的键不会被保存到重写后的 AOF 文件中，因此不会对 AOF 重写造成任何影响。 5.3 Redis 主从模式中，如何处理过期键 # 当 Redis 运行在主从模式下时，从库不会进行过期扫描，从库对过期的处理是被动的。也就是即使从库中的 key 过期了，如果有客户端访问从库时，依然可以得到 key 对应的值，像未过期的键值对一样返回。\n从库的过期键处理依靠主服务器控制，主库在 key 到期时，会在 AOF 文件里增加一条 del 指令，同步到所有的从库，从库通过执行这条 del 指令来删除过期的 key。\n5.4 Redis 内存满了，会发生什么 # 在 Redis 的运行内存达到了某个阀值，就会触发内存淘汰机制，这个阀值就是我们设置的最大运行内存，此值在 Redis 的配置文件中可以找到，配置项为 maxmemory。\n5.5 Redis 内存淘汰策略 # Redis 内存淘汰策略共有八种，这八种策略大体分为「不进行数据淘汰」和「进行数据淘汰」两类策略。\n1、不进行数据淘汰的策略\nnoeviction（Redis3.0之后，默认的内存淘汰策略） ：它表示当运行内存超过最大设置内存时，不淘汰任何数据，而是不再提供服务，直接返回错误。\n2、进行数据淘汰的策略\n针对「进行数据淘汰」这一类策略，又可以细分为「在设置了过期时间的数据中进行淘汰」和「在所有数据范围内进行淘汰」这两类策略。 在设置了过期时间的数据中进行淘汰：\nvolatile-random：随机淘汰设置了过期时间的任意键值； volatile-ttl：优先淘汰更早过期的键值。 volatile-lru（Redis3.0 之前，默认的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最久未使用的键值； volatile-lfu（Redis 4.0 后新增的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最少使用的键值； 在所有数据范围内进行淘汰：\nallkeys-random：随机淘汰任意键值; allkeys-lru：淘汰整个键值中最久未使用的键值； allkeys-lfu（Redis 4.0 后新增的内存淘汰策略）：淘汰整个键值中最少使用的键值。 5.6 LRU 算法和 LFU 算法区别 # 什么是 LRU 算法？\nLRU 全称是 Least Recently Used 翻译为最近最少使用，会选择淘汰最近最少使用的数据。\n传统 LRU 算法的实现是基于「链表」结构，链表中的元素按照操作顺序从前往后排列，最新操作的键会被移动到表头，当需要内存淘汰时，只需要删除链表尾部的元素即可，因为链表尾部的元素就代表最久未被使用的元素。\nRedis 并没有使用这样的方式实现 LRU 算法，因为传统的 LRU 算法存在两个问题：\n需要用链表管理所有的缓存数据，这会带来额外的空间开销； 当有数据被访问时，需要在链表上把该数据移动到头端，如果有大量数据被访问，就会带来很多链表移动操作，会很耗时，进而会降低 Redis 缓存性能。 Redis 是如何实现 LRU 算法的？\nRedis 实现的是一种近似 LRU 算法，目的是为了更好的节约内存，它的实现方式是在 Redis 的对象结构体中添加一个额外的字段，用于记录此数据的最后一次访问时间。\n当 Redis 进行内存淘汰时，会使用随机采样的方式来淘汰数据，它是随机取 5 个值（此值可配置），然后淘汰最久没有使用的那个。\nRedis 实现的 LRU 算法的优点：\n不用为所有的数据维护一个大链表，节省了空间占用； 不用在每次数据访问时都移动链表项，提升了缓存的性能； 但是 LRU 算法有一个问题，无法解决缓存污染问题，比如应用一次读取了大量的数据，而这些数据只会被读取这一次，那么这些数据会留存在 Redis 缓存中很长一段时间，造成缓存污染。\n因此，在 Redis 4.0 之后引入了 LFU 算法来解决这个问题。\n什么是 LFU 算法？\nLFU 全称是 Least Frequently Used 翻译为最近最不常用的，LFU 算法是根据数据访问次数来淘汰数据的，它的核心思想是“如果数据过去被访问多次，那么将来被访问的频率也更高”。\n所以， LFU 算法会记录每个数据的访问次数。当一个数据被再次访问时，就会增加该数据的访问次数。这样就解决了偶尔被访问一次之后，数据留存在缓存中很长一段时间的问题，相比于 LRU 算法也更合理一些。\nRedis 是如何实现 LFU 算法的？\nLFU 算法相比于 LRU 算法的实现，多记录了「数据的访问频次」的信息。Redis 对象的结构如下：\ntypedef struct redisObject { ... // 24 bits，用于记录对象的访问信息 unsigned lru:24; ... } robj; Redis 对象头中的 lru 字段，在 LRU 算法下和 LFU 算法下使用方式并不相同。\n在 LRU 算法中，Redis 对象头的 24 bits 的 lru 字段是用来记录 key 的访问时间戳，因此在 LRU 模式下，Redis可以根据对象头中的 lru 字段记录的值，来比较最后一次 key 的访问时间长，从而淘汰最久未被使用的 key。\n在 LFU 算法中，Redis对象头的 24 bits 的 lru 字段被分成两段来存储，高 16bit 存储 ldt(Last Decrement Time)，用来记录 key 的访问时间戳；低 8bit 存储 logc(Logistic Counter)，用来记录 key 的访问频次。\n6.缓存更新策略 # 常见的缓存更新策略共有3种：\nCache Aside（旁路缓存）策略； Read/Write Through（读穿 / 写穿）策略； Write Back（写回）策略； 实际开发中，Redis 和 MySQL 的更新策略用的是 Cache Aside，另外两种策略应用不了。\n6.1 Cache Aside（旁路缓存）策略 # Cache Aside（旁路缓存）策略是最常用的，应用程序直接与「数据库、缓存」交互，并负责对缓存的维护，该策略又可以细分为「读策略」和「写策略」。\n写策略的步骤：\n先更新数据库中的数据，再删除缓存中的数据。 读策略的步骤：\n如果读取的数据命中了缓存，则直接返回数据； 如果读取的数据没有命中缓存，则从数据库中读取数据，然后将数据写入到缓存，并且返回给用户。 注意，写策略的步骤的顺序不能倒过来，即不能先删除缓存再更新数据库，原因是在「读+写」并发的时候，会出现缓存和数据库的数据不一致性的问题。\n举个例子，假设某个用户的年龄是 20，请求 A 要更新用户年龄为 21，所以它会删除缓存中的内容。这时，另一个请求 B 要读取这个用户的年龄，它查询缓存发现未命中后，会从数据库中读取到年龄为 20，并且写入到缓存中，然后请求 A 继续更改数据库，将用户的年龄更新为 21。\n最终，该用户年龄在缓存中是 20（旧值），在数据库中是 21（新值），缓存和数据库的数据不一致。\n为什么「先更新数据库再删除缓存」不会有数据不一致的问题？\n继续用「读 + 写」请求的并发的场景来分析。\n假如某个用户数据在缓存中不存在，请求 A 读取数据时从数据库中查询到年龄为 20，在未写入缓存中时另一个请求 B 更新数据。它更新数据库中的年龄为 21，并且清空缓存。这时请求 A 把从数据库中读到的年龄为 20 的数据写入到缓存中。\n最终，该用户年龄在缓存中是 20（旧值），在数据库中是 21（新值），缓存和数据库数据不一致。 从上面的理论上分析，先更新数据库，再删除缓存也是会出现数据不一致性的问题，但是在实际中，这个问题出现的概率并不高。\n因为缓存的写入通常要远远快于数据库的写入，所以在实际中很难出现请求 B 已经更新了数据库并且删除了缓存，请求 A 才更新完缓存的情况。而一旦请求 A 早于请求 B 删除缓存之前更新了缓存，那么接下来的请求就会因为缓存不命中而从数据库中重新读取数据，所以不会出现这种不一致的情况。\nCache Aside 策略适合读多写少的场景，不适合写多的场景，因为当写入比较频繁时，缓存中的数据会被频繁地清理，这样会对缓存的命中率有一些影响。如果业务对缓存命中率有严格的要求，那么可以考虑两种解决方案：\n一种做法是在更新数据时也更新缓存，只是在更新缓存前先加一个分布式锁，因为这样在同一时间只允许一个线程更新缓存，就不会产生并发问题了。当然这么做对于写入的性能会有一些影响； 另一种做法同样也是在更新数据时更新缓存，只是给缓存加一个较短的过期时间，这样即使出现缓存不一致的情况，缓存的数据也会很快过期，对业务的影响也是可以接受。 6.2 Read/Write Through（读穿 / 写穿）策略 # Read/Write Through（读穿 / 写穿）策略原则是应用程序只和缓存交互，不再和数据库交互，而是由缓存和数据库交互，相当于更新数据库的操作由缓存自己代理了。\n1、Read Through 策略\n先查询缓存中数据是否存在，如果存在则直接返回，如果不存在，则由缓存组件负责从数据库查询数据，并将结果写入到缓存组件，最后缓存组件将数据返回给应用。\n2、Write Through 策略\n当有数据更新的时候，先查询要写入的数据在缓存中是否已经存在：\n如果缓存中数据已经存在，则更新缓存中的数据，并且由缓存组件同步更新到数据库中，然后缓存组件告知应用程序更新完成。 如果缓存中数据不存在，直接更新数据库，然后返回； 下面是 Read Through/Write Through 策略的示意图：\nRead Through/Write Through 策略的特点是由缓存节点而非应用程序来和数据库打交道，在我们开发过程中相比 Cache Aside 策略要少见一些，原因是我们经常使用的分布式缓存组件，无论是 Memcached 还是 Redis 都不提供写入数据库和自动加载数据库中的数据的功能。而我们在使用本地缓存的时候可以考虑使用这种策略。\n6.3 Write Back（写回）策略 # Write Back（写回）策略在更新数据的时候，只更新缓存，同时将缓存数据设置为脏的，然后立马返回，并不会更新数据库。对于数据库的更新，会通过批量异步更新的方式进行。\n实际上，Write Back（写回）策略也不能应用到我们常用的数据库和缓存的场景中，因为 Redis 并没有异步更新数据库的功能。\nWrite Back 是计算机体系结构中的设计，比如 CPU 的缓存、操作系统中文件系统的缓存都采用了 Write Back（写回）策略。\nWrite Back 策略特别适合写多的场景，因为发生写操作的时候， 只需要更新缓存，就立马返回了。比如，写文件的时候，实际上是写入到文件系统的缓存就返回了，并不会写磁盘。\n但是带来的问题是，数据不是强一致性的，而且会有数据丢失的风险，因为缓存一般使用内存，而内存是非持久化的，所以一旦缓存机器掉电，就会造成原本缓存中的脏数据丢失。所以你会发现系统在掉电之后，之前写入的文件会有部分丢失，就是因为 Page Cache 还没有来得及刷盘造成的。\n这里贴一张 CPU 缓存与内存使用 Write Back 策略的流程图：\n","date":"21 September 2023","permalink":"/basic/redis/review/01/","section":"Basics","summary":"Redis 相关知识复习","title":"Redis 基础"},{"content":"Redis 中 大 Key 相关问题总结\n1.Redis 的大 key 如何处理 # 什么是 Redis 大 key？\n大 key 并不是指 key 的值很大，而是 key 对应的 value 很大。\n一般而言，下面这两种情况被称为大 key：\nString 类型的值大于 10 KB； Hash、List、Set、ZSet 类型的元素的个数超过 5000个； 大 key 会造成什么问题？\n大 key 会带来以下四种影响：\n客户端超时阻塞。由于 Redis 执行命令是单线程处理，然后在操作大 key 时会比较耗时，那么就会阻塞 Redis，从客户端这一视角看，就是很久很久都没有响应。 引发网络阻塞。每次获取大 key 产生的网络流量较大，如果一个 key 的大小是 1 MB，每秒访问量为 1000，那么每秒会产生 1000MB 的流量，这对于普通千兆网卡的服务器来说是灾难性的。 阻塞工作线程。如果使用 del 删除大 key 时，会阻塞工作线程，这样就没办法处理后续的命令。 内存分布不均。集群模型在 slot 分片均匀情况下，会出现数据和查询倾斜情况，部分有大 key 的 Redis 节点占用内存多，QPS 也会比较大。 如何找到大 key ？\n1、redis-cli \u0026ndash;bigkeys 查找大key\n可以通过 redis-cli \u0026ndash;bigkeys 命令查找大 key：\nredis-cli -h 127.0.0.1 -p6379 -a \u0026#34;password\u0026#34; -- bigkeys 使用的时候注意事项：\n最好选择在从节点上执行该命令。因为主节点上执行时，会阻塞主节点； 如果没有从节点，那么可以选择在 Redis 实例业务压力的低峰阶段进行扫描查询，以免影响到实例的正常运行；或者可以使用 -i 参数控制扫描间隔，避免长时间扫描降低 Redis 实例的性能。 该方式的不足之处：\n这个方法只能返回每种类型中最大的那个 bigkey，无法得到大小排在前 N 位的 bigkey； 对于集合类型来说，这个方法只统计集合元素个数的多少，而不是实际占用的内存量。但是，一个集合中的元素个数多，并不一定占用的内存就多。因为，有可能每个元素占用的内存很小，这样的话，即使元素个数有很多，总内存开销也不大； 2、使用 SCAN 命令查找大 key\n使用 SCAN 命令对数据库扫描，然后用 TYPE 命令获取返回的每一个 key 的类型。\n对于 String 类型，可以直接使用 STRLEN 命令获取字符串的长度，也就是占用的内存空间字节数。\n对于集合类型来说，有两种方法可以获得它占用的内存大小：\n如果能够预先从业务层知道集合元素的平均大小，那么，可以使用下面的命令获取集合元素的个数，然后乘以集合元素的平均大小，这样就能获得集合占用的内存大小了。List 类型：LLEN 命令；Hash 类型：HLEN 命令；Set 类型：SCARD 命令；Sorted Set 类型：ZCARD 命令； 如果不能提前知道写入集合的元素大小，可以使用 MEMORY USAGE 命令（需要 Redis 4.0 及以上版本），查询一个键值对占用的内存空间。 3、使用 RdbTools 工具查找大 key\n使用 RdbTools 第三方开源工具，可以用来解析 Redis 快照（RDB）文件，找到其中的大 key。\n比如，下面这条命令，将大于 10 kb 的 key 输出到一个表格文件。\nrdb dump.rdb -c memory --bytes 10240 -f redis.csv 如何删除大 key？\n删除操作的本质是要释放键值对占用的内存空间，不要小瞧内存的释放过程。\n释放内存只是第一步，为了更加高效地管理内存空间，在应用程序释放内存时，操作系统需要把释放掉的内存块插入一个空闲内存块的链表，以便后续进行管理和再分配。这个过程本身需要一定时间，而且会阻塞当前释放内存的应用程序。\n所以，如果一下子释放了大量内存，空闲内存块链表操作时间就会增加，相应地就会造成 Redis 主线程的阻塞，如果主线程发生了阻塞，其他所有请求可能都会超时，超时越来越多，会造成 Redis 连接耗尽，产生各种异常。\n因此，删除大 key 这一个动作，我们要小心。具体要怎么做呢？这里给出两种方法：\n分批次删除 异步删除（Redis 4.0版本以上） 1、分批次删除\n对于删除大 Hash，使用 hscan 命令，每次获取 100 个字段，再用 hdel 命令，每次删除 1 个字段。\nPython代码：\ndef del_large_hash(): r = redis.StrictRedis(host=\u0026#39;redis-host1\u0026#39;, port=6379) large_hash_key =\u0026#34;xxx\u0026#34; #要删除的大hash键名 cursor = \u0026#39;0\u0026#39; while cursor != 0: # 使用 hscan 命令，每次获取 100 个字段 cursor, data = r.hscan(large_hash_key, cursor=cursor, count=100) for item in data.items(): # 再用 hdel 命令，每次删除1个字段 r.hdel(large_hash_key, item[0]) 对于删除大 List，通过 ltrim 命令，每次删除少量元素。\nPython代码：\ndef del_large_list(): r = redis.StrictRedis(host=\u0026#39;redis-host1\u0026#39;, port=6379) large_list_key = \u0026#39;xxx\u0026#39; #要删除的大list的键名 while r.llen(large_list_key)\u0026gt;0: #每次只删除最右100个元素 r.ltrim(large_list_key, 0, -101) 对于删除大 Set，使用 sscan 命令，每次扫描集合中 100 个元素，再用 srem 命令每次删除一个键。\nPython代码：\ndef del_large_set(): r = redis.StrictRedis(host=\u0026#39;redis-host1\u0026#39;, port=6379) large_set_key = \u0026#39;xxx\u0026#39; # 要删除的大set的键名 cursor = \u0026#39;0\u0026#39; while cursor != 0: # 使用 sscan 命令，每次扫描集合中 100 个元素 cursor, data = r.sscan(large_set_key, cursor=cursor, count=100) for item in data: # 再用 srem 命令每次删除一个键 r.srem(large_size_key, item) 对于删除大 ZSet，使用 zremrangebyrank 命令，每次删除 top 100个元素。\nPython代码：\ndef del_large_sortedset(): r = redis.StrictRedis(host=\u0026#39;large_sortedset_key\u0026#39;, port=6379) large_sortedset_key=\u0026#39;xxx\u0026#39; while r.zcard(large_sortedset_key)\u0026gt;0: # 使用 zremrangebyrank 命令，每次删除 top 100个元素 r.zremrangebyrank(large_sortedset_key,0,99) 2、异步删除\n从 Redis 4.0 版本开始，可以采用异步删除法，用 unlink 命令代替 del 来删除。\n这样 Redis 会将这个 key 放入到一个异步线程中进行删除，这样不会阻塞主线程。\n除了主动调用 unlink 命令实现异步删除之外，我们还可以通过配置参数，达到某些条件的时候自动进行异步删除。\n主要有 4 种场景，默认都是关闭的：\nlazyfree-lazy-eviction no lazyfree-lazy-expire no lazyfree-lazy-server-del noslave-lazy-flush no 它们代表的含义如下：\nlazyfree-lazy-eviction：表示当 Redis 运行内存超过 maxmeory 时，是否开启 lazy free 机制删除； lazyfree-lazy-expire：表示设置了过期时间的键值，当过期之后是否开启 lazy free 机制删除； lazyfree-lazy-server-del：有些指令在处理已存在的键时，会带有一个隐式的 del 键的操作，比如 rename 命令，当目标键已存在，Redis 会先删除目标键，如果这些目标键是一个 big key，就会造成阻塞删除的问题，此配置表示在这种场景中是否开启 lazy free 机制删除； slave-lazy-flush：针对 slave (从节点) 进行全量数据同步，slave 在加载 master 的 RDB 文件前，会运行 flushall 来清理自己的数据，它表示此时是否开启 lazy free 机制删除。 建议开启其中的 lazyfree-lazy-eviction、lazyfree-lazy-expire、lazyfree-lazy-server-del 等配置，这样就可以有效的提高主线程的执行效率。\n2.大 Key 对 AOF 日志的影响 # 先说说 AOF 日志三种写回磁盘的策略\nRedis 提供了 3 种 AOF 日志写回硬盘的策略，分别是：\nAlways，这个单词的意思是「总是」，所以它的意思是每次写操作命令执行完后，同步将 AOF 日志数据写回硬盘； Everysec，这个单词的意思是「每秒」，所以它的意思是每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，然后每隔一秒将缓冲区里的内容写回到硬盘； No，意味着不由 Redis 控制写回硬盘的时机，转交给操作系统控制写回的时机，也就是每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，再由操作系统决定何时将缓冲区内容写回硬盘。 这三种策略只是在控制 fsync() 函数的调用时机。\n当应用程序向文件写入数据时，内核通常先将数据复制到内核缓冲区中，然后排入队列，然后由内核决定何时写入硬盘。\n如果想要应用程序向文件写入数据后，能立马将数据同步到硬盘，就可以调用 fsync() 函数，这样内核就会将内核缓冲区的数据直接写入到硬盘，等到硬盘写操作完成后，该函数才会返回。\nAlways 策略就是每次写入 AOF 文件数据后，就执行 fsync() 函数； Everysec 策略就会创建一个异步任务来执行 fsync() 函数； No 策略就是永不执行 fsync() 函数; 分别说说这三种策略，在持久化大 Key 的时候，会影响什么？\n在使用 Always 策略的时候，主线程在执行完命令后，会把数据写入到 AOF 日志文件，然后会调用 fsync() 函数，将内核缓冲区的数据直接写入到硬盘，等到硬盘写操作完成后，该函数才会返回。\n当使用 Always 策略的时候，如果写入是一个大 Key，主线程在执行 fsync() 函数的时候，阻塞的时间会比较久，因为当写入的数据量很大的时候，数据同步到硬盘这个过程是很耗时的。\n当使用 Everysec 策略的时候，由于是异步执行 fsync() 函数，所以大 Key 持久化的过程（数据同步磁盘）不会影响主线程。\n当使用 No 策略的时候，由于永不执行 fsync() 函数，所以大 Key 持久化的过程不会影响主线程。\n3.大 Key 对 AOF 重写和 RDB 的影响 # 当 AOF 日志写入了很多的大 Key，AOF 日志文件的大小会很大，那么很快就会触发 AOF 重写机制。\nAOF 重写机制和 RDB 快照（bgsave 命令）的过程，都会分别通过 fork() 函数创建一个子进程来处理任务。\n在创建子进程的过程中，操作系统会把父进程的「页表」复制一份给子进程，这个页表记录着虚拟地址和物理地址映射关系，而不会复制物理内存，也就是说，两者的虚拟空间不同，但其对应的物理空间是同一个。\n这样一来，子进程就共享了父进程的物理内存数据了，这样能够节约物理内存资源，页表对应的页表项的属性会标记该物理内存的权限为只读。\n随着 Redis 存在越来越多的大 Key，那么 Redis 就会占用很多内存，对应的页表就会越大。\n在通过 fork() 函数创建子进程的时候，虽然不会复制父进程的物理内存，但是内核会把父进程的页表复制一份给子进程，如果页表很大，那么这个复制过程是会很耗时的，那么在执行 fork 函数的时候就会发生阻塞现象。\n而且，fork 函数是由 Redis 主线程调用的，如果 fork 函数发生阻塞，那么意味着就会阻塞 Redis 主线程。由于 Redis 执行命令是在主线程处理的，所以当 Redis 主线程发生阻塞，就无法处理后续客户端发来的命令。\n我们可以执行 info 命令获取到 latest_fork_usec 指标，表示 Redis 最近一次 fork 操作耗时。\n# 最近一次 fork 操作耗时 latest_fork_usec:315 如果 fork 耗时很大，比如超过1秒，则需要做出优化调整：\n单个实例的内存占用控制在 10 GB 以下，这样 fork 函数就能很快返回。 如果 Redis 只是当作纯缓存使用，不关心 Redis 数据安全性问题，可以考虑关闭 AOF 和 AOF 重写，这样就不会调用 fork 函数了。 在主从架构中，要适当调大 repl-backlog-size，避免因为 repl_backlog_buffer 不够大，导致主节点频繁地使用全量同步的方式，全量同步的时候，是会创建 RDB 文件的，也就是会调用 fork 函数。 那什么时候会发生物理内存的复制呢？\n当父进程或者子进程在向共享内存发起写操作时，CPU 就会触发写保护中断，这个「写保护中断」是由于违反权限导致的，然后操作系统会在「写保护中断处理函数」里进行物理内存的复制，并重新设置其内存映射关系，将父子进程的内存读写权限设置为可读写，最后才会对内存进行写操作，这个过程被称为「写时复制(Copy On Write)」。\n写时复制顾名思义，在发生写操作的时候，操作系统才会去复制物理内存，这样是为了防止 fork 创建子进程时，由于物理内存数据的复制时间过长而导致父进程长时间阻塞的问题。\n如果创建完子进程后，父进程对共享内存中的大 Key 进行了修改，那么内核就会发生写时复制，会把物理内存复制一份，由于大 Key 占用的物理内存是比较大的，那么在复制物理内存这一过程中，也是比较耗时的，于是父进程（主线程）就会发生阻塞。\n所以，有两个阶段会导致阻塞父进程：\n创建子进程的途中，由于要复制父进程的页表等数据结构，阻塞的时间跟页表的大小有关，页表越大，阻塞的时间也越长； 创建完子进程后，如果子进程或者父进程修改了共享数据，就会发生写时复制，这期间会拷贝物理内存，如果内存越大，自然阻塞的时间也越长； 这里额外提一下， 如果 Linux 开启了内存大页，会影响 Redis 的性能的。\nLinux 内核从 2.6.38 开始支持内存大页机制，该机制支持 2MB 大小的内存页分配，而常规的内存页分配是按 4KB 的粒度来执行的。\n如果采用了内存大页，那么即使客户端请求只修改 100B 的数据，在发生写时复制后，Redis 也需要拷贝 2MB 的大页。相反，如果是常规内存页机制，只用拷贝 4KB。\n两者相比，你可以看到，每次写命令引起的复制内存页单位放大了 512 倍，会拖慢写操作的执行时间，最终导致 Redis 性能变慢。\n那该怎么办呢？很简单，关闭内存大页（默认是关闭的）。\n禁用方法如下：\necho never \u0026gt; /sys/kernel/mm/transparent_hugepage/enabled ","date":"21 September 2023","permalink":"/basic/redis/review/02/","section":"Basics","summary":"Redis 中 大 Key 相关问题总结","title":"Redis 大 Key"},{"content":"Redis 持久化过程中的相关问题\n1.AOF 日志 # Redis 里的 AOF(Append Only File) 持久化功能，注意只会记录写操作命令，读操作命令是不会被记录的，因为没意义。\nAOF 日志文件其实就是普通的文本，我们可以通过 cat 命令查看里面的内容：\n「*3」表示当前命令有三个部分，每部分都是以「$+数字」开头，后面紧跟着具体的命令、键或值。然后，这里的「数字」表示这部分中的命令、键或值一共有多少字节。例如，「$3 set」表示这部分有 3 个字节，也就是「set」命令这个字符串的长度。\n不知道大家注意到没有，Redis 是先执行写操作命令后，才将该命令记录到 AOF 日志里的，这么做其实有两个好处。\n第一个好处，避免额外的检查开销。\n因为如果先将写操作命令记录到 AOF 日志里，再执行该命令的话，如果当前的命令语法有问题，那么如果不进行命令语法检查，该错误的命令记录到 AOF 日志里后，Redis 在使用日志恢复数据时，就可能会出错。\n而如果先执行写操作命令再记录日志的话，只有在该命令执行成功后，才将命令记录到 AOF 日志里，这样就不用额外的检查开销，保证记录在 AOF 日志里的命令都是可执行并且正确的。\n第二个好处，不会阻塞当前写操作命令的执行，因为当写操作命令执行成功后，才会将命令记录到 AOF 日志。\nAOF 持久化功能的潜在风险：\n第一个风险，执行写操作命令和记录日志是两个过程，那当 Redis 在还没来得及将命令写入到硬盘时，服务器发生宕机了，这个数据就会有丢失的风险。\n第二个风险，前面说道，由于写操作命令执行成功后才记录到 AOF 日志，所以不会阻塞当前写操作命令的执行，但是可能会给「下一个」命令带来阻塞风险。\n因为将命令写入到日志的这个操作也是在主进程完成的（执行命令也是在主进程），也就是说这两个操作是同步的。\n如果在将日志内容写入到硬盘时，服务器的硬盘的 I/O 压力太大，就会导致写硬盘的速度很慢，进而阻塞住了，也就会导致后续的命令无法执行。\n认真分析一下，其实这两个风险都有一个共性，都跟「 AOF 日志写回硬盘的时机」有关。\n1.1 AOF 三种写回策略 # Redis 执行完写操作命令后，会将命令追加到 server.aof_buf 缓冲区； 然后通过 write() 系统调用，将 aof_buf 缓冲区的数据写入到 AOF 文件，此时数据并没有写入到硬盘，而是拷贝到了内核缓冲区 page cache，等待内核将数据写入硬盘； 具体内核缓冲区的数据什么时候写入到硬盘，由内核决定。 Redis 提供了 3 种写回硬盘的策略，控制的就是上面说的第三步的过程。\n在 redis.conf 配置文件中的 appendfsync 配置项可以有以下 3 种参数可填：\nAlways，这个单词的意思是「总是」，所以它的意思是每次写操作命令执行完后，同步将 AOF 日志数据写回硬盘； Everysec，这个单词的意思是「每秒」，所以它的意思是每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，然后每隔一秒将缓冲区里的内容写回到硬盘； No，意味着不由 Redis 控制写回硬盘的时机，转交给操作系统控制写回的时机，也就是每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，再由操作系统决定何时将缓冲区内容写回硬盘。 这 3 种写回策略都无法能完美解决「主进程阻塞」和「减少数据丢失」的问题，因为两个问题是对立的，偏向于一边的话，就会要牺牲另外一边，原因如下：\nAlways 策略的话，可以最大程度保证数据不丢失，但是由于它每执行一条写操作命令就同步将 AOF 内容写回硬盘，所以是不可避免会影响主进程的性能； No 策略的话，是交由操作系统来决定何时将 AOF 日志内容写回硬盘，相比于 Always 策略性能较好，但是操作系统写回硬盘的时机是不可预知的，如果 AOF 日志内容没有写回硬盘，一旦服务器宕机，就会丢失不定数量的数据。 Everysec 策略的话，是折中的一种方式，避免了 Always 策略的性能开销，也比 No 策略更能避免数据丢失，当然如果上一秒的写操作命令日志没有写回到硬盘，发生了宕机，这一秒内的数据自然也会丢失。 大家根据自己的业务场景进行选择：\n如果要高性能，就选择 No 策略； 如果要高可靠，就选择 Always 策略； 如果允许数据丢失一点，但又想性能高，就选择 Everysec 策略。 把这 3 个写回策略的优缺点总结成了一张表格：\n深入到源码后，你就会发现这三种策略只是在控制 fsync() 函数的调用时机。\n当应用程序向文件写入数据时，内核通常先将数据复制到内核缓冲区中，然后排入队列，然后由内核决定何时写入硬盘。\n如果想要应用程序向文件写入数据后，能立马将数据同步到硬盘，就可以调用 fsync() 函数，这样内核就会将内核缓冲区的数据直接写入到硬盘，等到硬盘写操作完成后，该函数才会返回。\nAlways 策略就是每次写入 AOF 文件数据后，就执行 fsync() 函数； Everysec 策略就会创建一个异步任务来执行 fsync() 函数； No 策略就是永不执行 fsync() 函数; AOF 文件是在磁盘中还是在内存中？\nAOF（Append-Only File）文件既存在于磁盘中，也会在一定情况下存在于内存中。\nAOF 文件在磁盘中：AOF 文件是 Redis 用来持久化数据的一种方式。它是一个磁盘上的文件，用于记录 Redis 接收到的写命令，以便在Redis 重启时重新执行这些命令来恢复数据。AOF 文件的内容会随着时间的推移不断追加写入到磁盘中，以持久化数据的变化。 AOF 文件内容在内存中：Redis 在内存中维护了一个叫做\u0026quot;重写缓冲区\u0026quot;（Rewrite Buffer）的数据结构。当执行写命令时，这些命令会首先被追加到内存中的重写缓冲区中，然后定期或根据配置触发的情况下，Redis会使用重写缓冲区的内容来生成新的 AOF 文件。这个过程中，AOF 文件的内容是在内存中进行处理的。 所以，AOF 文件既存在于磁盘上，用于持久化数据，又在一定情况下存在于内存中，用于优化 AOF 文件的重写。在内存中的 AOF 数据主要用于 AOF 文件重写操作，而磁盘上的 AOF 文件是实际的持久化数据文件。\nRedis 主线程会将该命令追加到 AOF 缓冲区中，然后异步地将 AOF 缓冲区中的内容写入 AOF 文件。这段话中的异步是在主线程还是在子线程中呢？\n在 Redis 中，AOF 缓冲区中的内容是在主线程中异步写入 AOF 文件的，而不是在子线程中进行的。Redis 的主线程负责处理所有的客户端请求、命令执行、数据读写以及 AOF 文件的写入操作。当有命令需要被持久化到 AOF 文件时，主线程会将这些命令追加到 AOF 缓冲区中，然后将 AOF 缓冲区中的内容异步写入 AOF 文件。\n这种异步写入的方式可以确保 Redis 的主线程不会因为 AOF 持久化操作而被阻塞，从而保持了 Redis 的高性能和低延迟特性。异步写入意味着主线程可以继续处理其他请求，而不必等待 AOF 写入操作完成。\n需要注意的是，虽然 AOF 写入操作是异步的，但 Redis 会定期执行 AOF 文件的后台写入操作，以确保数据被安全地持久化到磁盘。这样，即使 Redis 非正常关闭，也可以通过 AOF 文件来恢复数据。\n异步是指 fork 了一个新的子线程吗？\nRedis 的 AOF 写入操作并不是通过 fork 新的子线程来实现的异步写入，而是通过异步 I/O 的方式来进行的。\n在 Redis 中，主线程是一个单线程的事件循环，它处理客户端请求、命令执行、数据读写以及 AOF 持久化操作等任务。当执行写命令时，主线程会将这些写命令追加到 AOF 缓冲区中。然后，Redis 通过异步 I/O 机制将 AOF 缓冲区中的内容异步写入 AOF 文件。\n异步 I/O 是一种非阻塞的 I/O 操作方式，它使得 I/O 操作不会阻塞主线程的执行。在 Redis 中，当主线程执行 AOF 写入操作时，它不会等待数据写入磁盘完成，而是将写入任务交给操作系统的异步 I/O 层处理。这样，主线程可以立即继续处理其他请求，而不必等待磁盘写入完成。\n所以，Redis 的 AOF 写入操作是通过异步 I/O 来实现的，而不是通过 fork 新的子线程。这种方式可以确保 Redis 的高性能和低延迟，因为主线程不会被 I/O 操作所阻塞。异步 I/O 允许操作系统在后台负责将数据写入磁盘，而主线程可以继续执行其他任务。\n既然是异步的，为什么仍然有阻塞下一个命令的风险呢？\n虽然 Redis 的 AOF 写入操作是通过异步 I/O 实现的，但仍然存在一些情况下可能会导致阻塞下一个命令的风险，主要与以下因素有关：\nAOF 缓冲区溢出：如果 AOF 缓冲区的大小有限，并且写入的命令量超过了缓冲区的容量，那么主线程可能会阻塞，直到 AOF 缓冲区中的一些数据被异步写入 AOF 文件以腾出空间。这种情况下，后续的写入操作可能会受到阻塞的影响。 磁盘性能问题：如果磁盘写入速度较慢，无论采用异步 I/O 还是同步 I/O，都有可能导致写入操作的延迟。如果 Redis 主线程在等待 AOF 写入完成时又接收到了大量的写入请求，那么这些请求可能会等待磁盘写入完成，从而导致阻塞。 AOF 文件重写：当执行 AOF 文件重写操作时，Redis 会生成新的 AOF 文件。在这个过程中，主线程会停止将命令追加到 AOF 文件，以便生成新的 AOF 文件。这个过程可能会导致一些写入操作被阻塞，直到新的 AOF 文件生成完成。 为了减少这些风险，可以采取一些措施，例如：\n优化 AOF 缓冲区的大小：确保 AOF 缓冲区足够大，以容纳一段时间内的写入操作，从而减少缓冲区溢出的可能性。 监控磁盘性能：监控磁盘的写入性能，确保它能够满足 Redis 的写入需求。可以考虑使用更高性能的磁盘或者使用 SSD 来改善性能。 定期执行 AOF 文件重写：通过定期执行AOF文件重写操作，可以减小 AOF 文件的大小，从而降低 AOF 写入的延迟。 总之，虽然 Redis 的 AOF 写入操作是异步的，但在特定情况下仍然需要关注可能导致阻塞的因素，并采取适当的措施来降低风险。\n1.2 AOF 重写机制 # AOF 日志是一个文件，随着执行的写操作命令越来越多，文件的大小会越来越大。\n如果当 AOF 日志文件过大就会带来性能问题，比如重启 Redis 后，需要读 AOF 文件的内容以恢复数据，如果文件过大，整个恢复的过程就会很慢。\n所以，Redis 为了避免 AOF 文件越写越大，提供了 AOF 重写机制，当 AOF 文件的大小超过所设定的阈值后，Redis 就会启用 AOF 重写机制，来压缩 AOF 文件。\nAOF 重写机制是在重写时，读取当前数据库中的所有键值对，然后将每一个键值对用一条命令记录到「新的 AOF 文件」，等到全部记录完后，就将新的 AOF 文件替换掉现有的 AOF 文件。\n举个例子，在没有使用重写机制前，假设前后执行了「set name xiaolin」和「set name xiaolincoding」这两个命令的话，就会将这两个命令记录到 AOF 文件。\n但是在使用重写机制后，就会读取 name 最新的 value（键值对） ，然后用一条 「set name xiaolincoding」命令记录到新的 AOF 文件，之前的第一个命令就没有必要记录了，因为它属于「历史」命令，没有作用了。这样一来，一个键值对在重写日志中只用一条命令就行了。\n重写工作完成后，就会将新的 AOF 文件覆盖现有的 AOF 文件，这就相当于压缩了 AOF 文件，使得 AOF 文件体积变小了。\n然后，在通过 AOF 日志恢复数据时，只用执行这条命令，就可以直接完成这个键值对的写入了。\n所以，重写机制的妙处在于，尽管某个键值对被多条写命令反复修改，最终也只需要根据这个「键值对」当前的最新状态，然后用一条命令去记录键值对，代替之前记录这个键值对的多条命令，这样就减少了 AOF 文件中的命令数量。最后在重写工作完成后，将新的 AOF 文件覆盖现有的 AOF 文件。\n这里说一下为什么重写 AOF 的时候，不直接复用现有的 AOF 文件，而是先写到新的 AOF 文件再覆盖过去。\n因为如果 AOF 重写过程中失败了，现有的 AOF 文件就会造成污染，可能无法用于恢复使用。\n所以 AOF 重写过程，先重写到新的 AOF 文件，重写失败的话，就直接删除这个文件就好，不会对现有的 AOF 文件造成影响。\n1.3 AOF 后台重写 # 写入 AOF 日志的操作虽然是在主进程完成的，因为它写入的内容不多，所以一般不太影响命令的操作。\n但是在触发 AOF 重写时，比如当 AOF 文件大于 64M 时，就会对 AOF 文件进行重写，这时是需要读取所有缓存的键值对数据，并为每个键值对生成一条命令，然后将其写入到新的 AOF 文件，重写完后，就把现在的 AOF 文件替换掉。\n这个过程其实是很耗时的，所以重写的操作不能放在主进程里。\n所以，Redis 的重写 AOF 过程是由后台子进程 bgrewriteaof 来完成的，这么做可以达到两个好处：\n子进程进行 AOF 重写期间，主进程可以继续处理命令请求，从而避免阻塞主进程； 子进程带有主进程的数据副本（数据副本怎么产生的后面会说），这里使用子进程而不是线程，因为如果是使用线程，多线程之间会共享内存，那么在修改共享内存数据的时候，需要通过加锁来保证数据的安全，而这样就会降低性能。而使用子进程，创建子进程时，父子进程是共享内存数据的，不过这个共享的内存只能以只读的方式，而当父子进程任意一方修改了该共享内存，就会发生「写时复制」，于是父子进程就有了独立的数据副本，就不用加锁来保证数据安全。 子进程是怎么拥有主进程一样的数据副本的呢？\n主进程在通过 fork 系统调用生成 bgrewriteaof 子进程时，操作系统会把主进程的「页表」复制一份给子进程，这个页表记录着虚拟地址和物理地址映射关系，而不会复制物理内存，也就是说，两者的虚拟空间不同，但其对应的物理空间是同一个。\n这样一来，子进程就共享了父进程的物理内存数据了，这样能够节约物理内存资源，页表对应的页表项的属性会标记该物理内存的权限为只读。\n不过，当父进程或者子进程在向这个内存发起写操作时，CPU 就会触发写保护中断，这个写保护中断是由于违反权限导致的，然后操作系统会在「写保护中断处理函数」里进行物理内存的复制，并重新设置其内存映射关系，将父子进程的内存读写权限设置为可读写，最后才会对内存进行写操作，这个过程被称为「写时复制(Copy On Write)」。\n写时复制顾名思义，在发生写操作的时候，操作系统才会去复制物理内存，这样是为了防止 fork 创建子进程时，由于物理内存数据的复制时间过长而导致父进程长时间阻塞的问题。\n当然，操作系统复制父进程页表的时候，父进程也是阻塞中的，不过页表的大小相比实际的物理内存小很多，所以通常复制页表的过程是比较快的。\n不过，如果父进程的内存数据非常大，那自然页表也会很大，这时父进程在通过 fork 创建子进程的时候，阻塞的时间也越久。\n所以，有两个阶段会导致阻塞父进程：\n创建子进程的途中，由于要复制父进程的页表等数据结构，阻塞的时间跟页表的大小有关，页表越大，阻塞的时间也越长； 创建完子进程后，如果子进程或者父进程修改了共享数据，就会发生写时复制，这期间会拷贝物理内存，如果内存越大，自然阻塞的时间也越长； 触发重写机制后，主进程就会创建重写 AOF 的子进程，此时父子进程共享物理内存，重写子进程只会对这个内存进行只读，重写 AOF 子进程会读取数据库里的所有数据，并逐一把内存数据的键值对转换成一条命令，再将命令记录到重写日志（新的 AOF 文件）。\n但是子进程重写过程中，主进程依然可以正常处理命令。\n如果此时主进程修改了已经存在 key-value，就会发生写时复制，注意这里只会复制主进程修改的物理内存数据，没修改物理内存还是与子进程共享的。\n所以如果这个阶段修改的是一个 bigkey，也就是数据量比较大的 key-value 的时候，这时复制的物理内存数据的过程就会比较耗时，有阻塞主进程的风险。\n还有个问题，重写 AOF 日志过程中，如果主进程修改了已经存在 key-value，此时这个 key-value 数据在子进程的内存数据就跟主进程的内存数据不一致了，这时要怎么办呢？\n为了解决这种数据不一致问题，Redis 设置了一个 AOF 重写缓冲区，这个缓冲区在创建 bgrewriteaof 子进程之后开始使用。\n在重写 AOF 期间，当 Redis 执行完一个写命令之后，它会同时将这个写命令写入到 「AOF 缓冲区」和 「AOF 重写缓冲区」。\n也就是说，在 bgrewriteaof 子进程执行 AOF 重写期间，主进程需要执行以下三个工作:\n执行客户端发来的命令； 将执行后的写命令追加到 「AOF 缓冲区」； 将执行后的写命令追加到 「AOF 重写缓冲区」； 当子进程完成 AOF 重写工作（扫描数据库中所有数据，逐一把内存数据的键值对转换成一条命令，再将命令记录到重写日志）后，会向主进程发送一条信号，信号是进程间通讯的一种方式，且是异步的。\n主进程收到该信号后，会调用一个信号处理函数，该函数主要做以下工作：\n将 AOF 重写缓冲区中的所有内容追加到新的 AOF 的文件中，使得新旧两个 AOF 文件所保存的数据库状态一致； 新的 AOF 的文件进行改名，覆盖现有的 AOF 文件。 信号函数执行完后，主进程就可以继续像往常一样处理命令了。\n在整个 AOF 后台重写过程中，除了发生写时复制会对主进程造成阻塞，还有信号处理函数执行时也会对主进程造成阻塞，在其他时候，AOF 后台重写都不会阻塞主进程。\n2.RDB 快照 # 虽说 Redis 是内存数据库，但是它为数据的持久化提供了两个技术。\n分别是「 AOF 日志和 RDB 快照」。\n这两种技术都会用各用一个日志文件来记录信息，但是记录的内容是不同的。\nAOF 文件的内容是操作命令； RDB 文件的内容是二进制数据。 所谓的快照，就是记录某一个瞬间东西，比如当我们给风景拍照时，那一个瞬间的画面和信息就记录到了一张照片。\n所以，RDB 快照就是记录某一个瞬间的内存数据，记录的是实际数据，而 AOF 文件记录的是命令操作的日志，而不是实际的数据。\n因此在 Redis 恢复数据时， RDB 恢复数据的效率会比 AOF 高些，因为直接将 RDB 文件读入内存就可以，不需要像 AOF 那样还需要额外执行操作命令的步骤才能恢复数据。\n2.1 快照的使用方式 # Redis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave，他们的区别就在于是否在「主线程」里执行：\n执行了 save 命令，就会在主线程生成 RDB 文件，由于和执行操作命令在同一个线程，所以如果写入 RDB 文件的时间太长，会阻塞主线程； 执行了 bgsave 命令，会创建一个子进程来生成 RDB 文件，这样可以避免主线程的阻塞； RDB 文件的加载工作是在服务器启动时自动执行的，Redis 并没有提供专门用于加载 RDB 文件的命令。\nRedis 还可以通过配置文件的选项来实现每隔一段时间自动执行一次 bgsave 命令，默认会提供以下配置：\nsave 900 1 save 300 10 save 60 10000 别看选项名叫 save，实际上执行的是 bgsave 命令，也就是会创建子进程来生成 RDB 快照文件。\n只要满足上面条件的任意一个，就会执行 bgsave，它们的意思分别是：\n900 秒之内，对数据库进行了至少 1 次修改； 300 秒之内，对数据库进行了至少 10 次修改； 60 秒之内，对数据库进行了至少 10000 次修改。 这里提一点，Redis 的快照是全量快照，也就是说每次执行快照，都是把内存中的「所有数据」都记录到磁盘中。\n所以可以认为，执行快照是一个比较重的操作，如果频率太频繁，可能会对 Redis 性能产生影响。如果频率太低，服务器故障时，丢失的数据会更多。\n通常可能设置至少 5 分钟才保存一次快照，这时如果 Redis 出现宕机等情况，则意味着最多可能丢失 5 分钟数据。\n这就是 RDB 快照的缺点，在服务器发生故障时，丢失的数据会比 AOF 持久化的方式更多，因为 RDB 快照是全量快照的方式，因此执行的频率不能太频繁，否则会影响 Redis 性能，而 AOF 日志可以以秒级的方式记录操作命令，所以丢失的数据就相对更少。\n2.2 执行快照时，数据是否可以被修改 # 那问题来了，执行 bgsave 过程中，由于是交给子进程来构建 RDB 文件，主线程还是可以继续工作的，此时主线程可以修改数据吗？\n如果不可以修改数据的话，那这样性能一下就降低了很多。如果可以修改数据，又是如何做到到呢？\n直接说结论吧，执行 bgsave 过程中，Redis 依然可以继续处理操作命令的，也就是数据是能被修改的。\n那具体如何做到到呢？关键的技术就在于写时复制技术（Copy-On-Write, COW）。\n执行 bgsave 命令的时候，会通过 fork() 创建子进程，此时子进程和父进程是共享同一片内存数据的，因为创建子进程的时候，会复制父进程的页表，但是页表指向的物理内存还是一个。\n只有在发生修改内存数据的情况时，物理内存才会被复制一份。\n这样的目的是为了减少创建子进程时的性能损耗，从而加快创建子进程的速度，毕竟创建子进程的过程中，是会阻塞主线程的。\n所以，创建 bgsave 子进程后，由于共享父进程的所有内存数据，于是就可以直接读取主线程（父进程）里的内存数据，并将数据写入到 RDB 文件。\n当主线程（父进程）对这些共享的内存数据也都是只读操作，那么，主线程（父进程）和 bgsave 子进程相互不影响。\n但是，如果主线程（父进程）要修改共享数据里的某一块数据（比如键值对 A）时，就会发生写时复制，于是这块数据的物理内存就会被复制一份（键值对 A'），然后主线程在这个数据副本（键值对 A'）进行修改操作。与此同时，bgsave 子进程可以继续把原来的数据（键值对 A）写入到 RDB 文件。\n就是这样，Redis 使用 bgsave 对当前内存中的所有数据做快照，这个操作是由 bgsave 子进程在后台完成的，执行时不会阻塞主线程，这就使得主线程同时可以修改数据。\n细心的同学，肯定发现了，bgsave 快照过程中，如果主线程修改了共享数据，发生了写时复制后，RDB 快照保存的是原本的内存数据，而主线程刚修改的数据，是没办法在这一时间写入 RDB 文件的，只能交由下一次的 bgsave 快照。\n所以 Redis 在使用 bgsave 快照过程中，如果主线程修改了内存数据，不管是否是共享的内存数据，RDB 快照都无法写入主线程刚修改的数据，因为此时主线程（父进程）的内存数据和子进程的内存数据已经分离了，子进程写入到 RDB 文件的内存数据只能是原本的内存数据。\n如果系统恰好在 RDB 快照文件创建完毕后崩溃了，那么 Redis 将会丢失主线程在快照期间修改的数据。\n另外，写时复制的时候会出现这么个极端的情况。\n在 Redis 执行 RDB 持久化期间，刚 fork 时，主进程和子进程共享同一物理内存，但是途中主进程处理了写操作，修改了共享内存，于是当前被修改的数据的物理内存就会被复制一份。\n那么极端情况下，如果所有的共享内存都被修改，则此时的内存占用是原先的 2 倍。\n所以，针对写操作多的场景，我们要留意下快照过程中内存的变化，防止内存被占满了。\n3.RDB 和 AOF 合体：混合持久化 # 尽管 RDB 比 AOF 的数据恢复速度快，但是快照的频率不好把握：\n如果频率太低，两次快照间一旦服务器发生宕机，就可能会比较多的数据丢失； 如果频率太高，频繁写入磁盘和创建子进程会带来额外的性能开销。 那有没有什么方法不仅有 RDB 恢复速度快的优点和，又有 AOF 丢失数据少的优点呢？\n当然有，那就是将 RDB 和 AOF 合体使用，这个方法是在 Redis 4.0 提出的，该方法叫混合使用 AOF 日志和内存快照，也叫混合持久化。\n如果想要开启混合持久化功能，可以在 Redis 配置文件将下面这个配置项设置成 yes：\naof-use-rdb-preamble yes 混合持久化工作在 AOF 日志重写过程。\n当开启了混合持久化时，在 AOF 重写日志时，fork 出来的重写子进程会先将与主线程共享的内存数据以 RDB 方式写入到 AOF 文件，然后主线程处理的操作命令会被记录在重写缓冲区里，重写缓冲区里的增量命令会以 AOF 方式写入到 AOF 文件，写入完成后通知主进程将新的含有 RDB 格式和 AOF 格式的 AOF 文件替换旧的的 AOF 文件。\n也就是说，使用了混合持久化，AOF 文件的前半部分是 RDB 格式的全量数据，后半部分是 AOF 格式的增量数据。\n这样的好处在于，重启 Redis 加载数据的时候，由于前半部分是 RDB 内容，这样加载的时候速度会很快。\n加载完 RDB 的内容后，才会加载后半部分的 AOF 内容，这里的内容是 Redis 后台子进程重写 AOF 期间，主线程处理的操作命令，可以使得数据更少的丢失。\n","date":"21 September 2023","permalink":"/basic/redis/review/03/","section":"Basics","summary":"Redis 持久化过程中的相关问题","title":"Redis 持久化"},{"content":"HTTP 相关技术\n1.HTTP / 1.1 # 1.1 HTTP/1.1 的优点 # HTTP 最突出的优点是「简单、灵活和易于扩展、应用广泛和跨平台」。\n1. 简单\nHTTP 基本的报文格式就是 header + body，头部信息也是 key-value 简单文本的形式，易于理解，降低了学习和使用的门槛。\n2. 灵活和易于扩展\nHTTP 协议里的各类请求方法、URI/URL、状态码、头字段等每个组成要求都没有被固定死，都允许开发人员自定义和扩充。\n同时 HTTP 由于是工作在应用层（ OSI 第七层），则它下层可以随意变化，比如：\nHTTPS 就是在 HTTP 与 TCP 层之间增加了 SSL/TLS 安全传输层； HTTP/1.1 和 HTTP/2.0 传输协议使用的是 TCP 协议，而到了 HTTP/3.0 传输协议改用了 UDP 协议。 3. 应用广泛和跨平台\n互联网发展至今，HTTP 的应用范围非常的广泛，从台式机的浏览器到手机上的各种 APP，从看新闻、刷贴吧到购物、理财、吃鸡，HTTP 的应用遍地开花，同时天然具有跨平台的优越性。\n1.2 HTTP/1.1 的缺点 # HTTP 协议里有优缺点一体的双刃剑，分别是「无状态、明文传输」，同时还有一大缺点「不安全」。\n1. 无状态\n无状态的好处，因为服务器不会去记忆 HTTP 的状态，所以不需要额外的资源来记录状态信息，这能减轻服务器的负担，能够把更多的 CPU 和内存用来对外提供服务。\n无状态的坏处，既然服务器没有记忆能力，它在完成有关联性的操作时会非常麻烦。\n对于无状态的问题，解法方案有很多种，其中比较简单的方式用 Cookie 技术。\nCookie 通过在请求和响应报文中写入 Cookie 信息来控制客户端的状态。\n相当于，在客户端第一次请求后，服务器会下发一个装有客户信息的「小贴纸」，后续客户端请求服务器的时候，带上「小贴纸」，服务器就能认得了了。\n2. 明文传输\n明文意味着在传输过程中的信息，是可方便阅读的，比如 Wireshark 抓包都可以直接肉眼查看，为我们调试工作带了极大的便利性。\n但是这正是这样，HTTP 的所有信息都容易被窃取。\n3. 不安全\nHTTP 比较严重的缺点就是不安全：\n通信使用明文（不加密），内容可能会被窃听。 不验证通信方的身份，因此有可能遭遇伪装。 无法证明报文的完整性，所以有可能已遭篡改。 HTTP 的安全问题，可以用 HTTPS 的方式解决，也就是通过引入 SSL/TLS 层，使得在安全上达到了极致。\n1.3 HTTP/1.1 的性能 # HTTP 协议是基于 TCP/IP，并且使用了「请求 - 应答」的通信模式，所以性能的关键就在这两点里。\n1. 长连接\n早期 HTTP/1.0 性能上的一个很大的问题，那就是每发起一个请求，都要新建一次 TCP 连接（三次握手），而且是串行请求，做了无谓的 TCP 连接建立和断开，增加了通信开销。\n为了解决上述 TCP 连接问题，HTTP/1.1 提出了长连接的通信方式，也叫持久连接。这种方式的好处在于减少了 TCP 连接的重复建立和断开所造成的额外开销，减轻了服务器端的负载。\n持久连接的特点是，只要任意一端没有明确提出断开连接，则保持 TCP 连接状态。\n当然，如果某个 HTTP 长连接超过一定时间没有任何数据交互，服务端就会主动断开这个连接。\n2. 管道网络传输\nHTTP/1.1 采用了长连接的方式，这使得管道（pipeline）网络传输成为了可能。\n即可在同一个 TCP 连接里面，客户端可以发起多个请求，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去，可以减少整体的响应时间。\n举例来说，客户端需要请求两个资源。以前的做法是，在同一个 TCP 连接里面，先发送 A 请求，然后等待服务器做出回应，收到后再发出 B 请求。那么，管道机制则是允许浏览器同时发出 A 请求和 B 请求。\n但是服务器必须按照接收请求的顺序发送对这些管道化请求的响应。\n如果服务端在处理 A 请求时耗时比较长，那么后续的请求的处理都会被阻塞住，这称为「队头堵塞」。\n所以，HTTP/1.1 管道解决了请求的队头阻塞，但是没有解决响应的队头阻塞。\n3. 队头阻塞\n「请求 - 应答」的模式会造成 HTTP 的性能问题。\n因为当顺序发送的请求序列中的一个请求因为某种原因被阻塞时，在后面排队的所有请求也一同被阻塞了，会招致客户端一直请求不到数据，这也就是「队头阻塞」，好比上班的路上塞车。\n总之 HTTP/1.1 的性能一般般，后续的 HTTP/2 和 HTTP/3 就是在优化 HTTP 的性能。\n2.HTTP 与 HTTPS # 2.1 HTTP 与 HTTPS 区别 # HTTP 是超文本传输协议，信息是明文传输，存在安全风险的问题。HTTPS 则解决 HTTP 不安全的缺陷，在 TCP 和 HTTP 网络层之间加入了 SSL/TLS 安全协议，使得报文能够加密传输。 HTTP 连接建立相对简单， TCP 三次握手之后便可进行 HTTP 的报文传输。而 HTTPS 在 TCP 三次握手之后，还需进行 SSL/TLS 的握手过程，才可进入加密报文传输。 两者的默认端口不一样，HTTP 默认端口号是 80，HTTPS 默认端口号是 443。 HTTPS 协议需要向 CA（证书权威机构）申请数字证书，来保证服务器的身份是可信的。 2.2 HTTPS 解决了 HTTP 的哪些问题 # 窃听风险，比如通信链路上可以获取通信内容。 篡改风险，比如强制植入垃圾广告，视觉污染。 冒充风险，比如冒充淘宝网站。 HTTPS 在 HTTP 与 TCP 层之间加入了 SSL/TLS 协议，可以很好的解决了上述的风险：\n信息加密：交互信息无法被窃取。 校验机制：无法篡改通信内容，篡改了就不能正常显示。 身份证书：证明淘宝是真的淘宝网。 HTTPS 是如何解决上面的三个风险的：\n混合加密的方式实现信息的机密性，解决了窃听的风险。 摘要算法的方式来实现完整性，它能够为数据生成独一无二的「指纹」，指纹用于校验数据的完整性，解决了篡改的风险。 将服务器公钥放入到数字证书中，解决了冒充的风险。 2.3 HTTPS 是如何建立连接（四次握手） # SSL/TLS 协议基本流程：\n客户端向服务器索要并验证服务器的公钥。 双方协商生产「会话秘钥」。 双方采用「会话秘钥」进行加密通信。 前两步也就是 SSL/TLS 的建立过程，也就是 TLS 握手阶段。\nTLS 的「握手阶段」涉及四次通信，使用不同的密钥交换算法，TLS 握手流程也会不一样的，现在常用的密钥交换算法有两种：RSA 算法 和 ECDHE 算法。\nTLS 协议建立的详细流程：\n1. ClientHello\n首先，由客户端向服务器发起加密通信请求，也就是 ClientHello 请求。\n客户端主要向服务器发送以下信息：\n（1）客户端支持的 TLS 协议版本，如 TLS 1.2 版本。\n（2）客户端生产的随机数（Client Random），后面用于生成「会话秘钥」条件之一。\n（3）客户端支持的密码套件列表，如 RSA 加密算法。\n2. SeverHello\n服务器收到客户端请求后，向客户端发出响应，也就是 SeverHello。服务器回应的内容有如下内容：\n（1）确认 TLS 协议版本，如果浏览器不支持，则关闭加密通信。\n（2）服务器生产的随机数（Server Random），也是后面用于生产「会话秘钥」条件之一。\n（3）确认的密码套件列表，如 RSA 加密算法。\n（4）服务器的数字证书。\n3.客户端回应\n客户端收到服务器的回应之后，首先通过浏览器或者操作系统中的 CA 公钥，确认服务器的数字证书的真实性。\n如果证书没有问题，客户端会从数字证书中取出服务器的公钥，然后使用它加密报文，向服务器发送：\n（1）一个随机数（pre-master key）。该随机数会被服务器公钥加密。\n（2）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。\n（3）客户端握手结束通知，表示客户端的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供服务端校验。\n上面第一项的随机数是整个握手阶段的第三个随机数，会发给服务端，所以这个随机数客户端和服务端都是一样的。\n服务器和客户端有了这三个随机数（Client Random、Server Random、pre-master key），接着就用双方协商的加密算法，各自生成本次通信的「会话秘钥」。\n4. 服务器的最后回应\n服务器收到客户端的第三个随机数（pre-master key）之后，通过协商的加密算法，计算出本次通信的「会话秘钥」。\n然后，向客户端发送最后的信息：\n（1）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。\n（2）服务器握手结束通知，表示服务器的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供客户端校验。\n至此，整个 TLS 的握手阶段全部结束。接下来，客户端与服务器进入加密通信，就完全是使用普通的 HTTP 协议，只不过用「会话秘钥」加密内容。\n2.4 数字证书签发和验证流程 # CA 签发证书的过程：\n首先 CA 会把持有者的公钥、用途、颁发者、有效时间等信息打成一个包，然后对这些信息进行 Hash 计算，得到一个 Hash 值； 然后 CA 会使用自己的私钥将该 Hash 值加密，生成 Certificate Signature，也就是 CA 对证书做了签名； 最后将 Certificate Signature 添加在文件证书上，形成数字证书； 客户端校验服务端的数字证书的过程，如上图右边部分：\n首先客户端会使用同样的 Hash 算法获取该证书的 Hash 值 H1； 通常浏览器和操作系统中集成了 CA 的公钥信息，浏览器收到证书后可以使用 CA 的公钥解密 Certificate Signature 内容，得到一个 Hash 值 H2 ； 最后比较 H1 和 H2，如果值相同，则为可信赖的证书，否则则认为证书不可信。 2.5 HTTPS 的应用数据是如何保证完整性的 # TLS 在实现上分为握手协议和记录协议两层：\nTLS 握手协议负责协商加密算法和生成对称密钥，后续用此密钥来保护应用程序数据（即 HTTP 数据）； TLS 记录协议负责保护应用程序数据并验证其完整性和来源，所以对 HTTP 数据加密是使用记录协议； TLS 记录协议主要负责消息（HTTP 数据）的压缩，加密及数据的认证，过程如下图：\n具体过程如下：\n首先，消息被分割成多个较短的片段,然后分别对每个片段进行压缩。 接下来，经过压缩的片段会被加上消息认证码（MAC 值，这个是通过哈希算法生成的），这是为了保证完整性，并进行数据的认证。通过附加消息认证码的 MAC 值，可以识别出篡改。与此同时，为了防止重放攻击，在计算消息认证码时，还加上了片段的编码。 再接下来，经过压缩的片段再加上消息认证码会一起通过对称密码进行加密。 最后，上述经过加密的数据再加上由数据类型、版本号、压缩后的长度组成的报头就是最终的报文数据。 记录协议完成后，最终的报文数据将传递到传输控制协议 (TCP) 层进行传输。\n2.6 中间人服务器 # 客户端通过浏览器向服务端发起 HTTPS 请求时，被「假基站」转发到了一个「中间人服务器」，于是客户端是和「中间人服务器」完成了 TLS 握手，然后这个「中间人服务器」再与真正的服务端完成 TLS 握手。\n具体过程如下：\n客户端向服务端发起 HTTPS 建立连接请求时，然后被「假基站」转发到了一个「中间人服务器」，接着中间人向服务端发起 HTTPS 建立连接请求，此时客户端与中间人进行 TLS 握手，中间人与服务端进行 TLS 握手； 在客户端与中间人进行 TLS 握手过程中，中间人会发送自己的公钥证书给客户端，客户端验证证书的真伪，然后从证书拿到公钥，并生成一个随机数，用公钥加密随机数发送给中间人，中间人使用私钥解密，得到随机数，此时双方都有随机数，然后通过算法生成对称加密密钥（A），后续客户端与中间人通信就用这个对称加密密钥来加密数据了。 在中间人与服务端进行 TLS 握手过程中，服务端会发送从 CA 机构签发的公钥证书给中间人，从证书拿到公钥，并生成一个随机数，用公钥加密随机数发送给服务端，服务端使用私钥解密，得到随机数，此时双方都有随机数，然后通过算法生成对称加密密钥（B），后续中间人与服务端通信就用这个对称加密密钥来加密数据了。 后续的通信过程中，中间人用对称加密密钥（A）解密客户端的 HTTPS 请求的数据，然后用对称加密密钥（B）加密 HTTPS 请求后，转发给服务端，接着服务端发送 HTTPS 响应数据给中间人，中间人用对称加密密钥（B）解密 HTTPS 响应数据，然后再用对称加密密钥（A）加密后，转发给客户端。 从客户端的角度看，其实并不知道网络中存在中间人服务器这个角色。那么中间人就可以解开浏览器发起的 HTTPS 请求里的数据，也可以解开服务端响应给浏览器的 HTTPS 响应数据。相当于，中间人能够 “偷看” 浏览器与服务端之间的 HTTPS 请求和响应的数据。\n但是要发生这种场景是有前提的，前提是用户点击接受了中间人服务器的证书。\n中间人服务器与客户端在 TLS 握手过程中，实际上发送了自己伪造的证书给浏览器，而这个伪造的证书是能被浏览器（客户端）识别出是非法的，于是就会提醒用户该证书存在问题。\n所以，HTTPS 协议本身到目前为止还是没有任何漏洞的，即使你成功进行中间人攻击，本质上是利用了客户端的漏洞（用户点击继续访问或者被恶意导入伪造的根证书），并不是 HTTPS 不够安全。\n2.7 如何避免中间人攻击 # 为什么抓包工具能截取 HTTPS 数据？\n很多抓包工具 之所以可以明文看到 HTTPS 数据，工作原理与中间人一致的。\n对于 HTTPS 连接来说，中间人要满足以下两点，才能实现真正的明文代理:\n中间人，作为客户端与真实服务端建立连接这一步不会有问题，因为服务端不会校验客户端的身份； 中间人，作为服务端与真实客户端建立连接，这里会有客户端信任服务端的问题，也就是服务端必须有对应域名的私钥； 中间人要拿到私钥只能通过如下方式：\n去网站服务端拿到私钥； 去CA处拿域名签发私钥； 自己签发证书，切要被浏览器信任； 不用解释，抓包工具只能使用第三种方式取得中间人的身份。\n使用抓包工具进行 HTTPS 抓包的时候，需要在客户端安装 Fiddler 的根证书，这里实际上起认证中心（CA）的作用。\n抓包工具能够抓包的关键是客户端会往系统受信任的根证书列表中导入抓包工具生成的证书，而这个证书会被浏览器信任，也就是抓包工具给自己创建了一个认证中心 CA，客户端拿着中间人签发的证书去中间人自己的 CA 去认证，当然认为这个证书是有效的。\n如何避免被中间人抓取数据？\n我们要保证自己电脑的安全，不要被病毒乘虚而入，而且也不要点击任何证书非法的网站，这样 HTTPS 数据就不会被中间人截取到了。\n当然，我们还可以通过 HTTPS 双向认证来避免这种问题。\n一般我们的 HTTPS 是单向认证，客户端只会验证了服务端的身份，但是服务端并不会验证客户端的身份。\n如果用了双向认证方式，不仅客户端会验证服务端的身份，而且服务端也会验证客户端的身份。服务端一旦验证到请求自己的客户端为不可信任的，服务端就拒绝继续通信，客户端如果发现服务端为不可信任的，那么也中止通信。\n3.HTTP/1.1、HTTP/2、HTTP/3 演变 # 3.1 HTTP/1.1 相比 HTTP/1.0 提高了什么性能 # HTTP/1.1 相比 HTTP/1.0 性能上的改进：\n使用长连接的方式改善了 HTTP/1.0 短连接造成的性能开销。 支持管道（pipeline）网络传输，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去，可以减少整体的响应时间。 但 HTTP/1.1 还是有性能瓶颈：\n请求 / 响应头部（Header）未经压缩就发送，首部信息越多延迟越大。只能压缩 Body 的部分； 发送冗长的首部。每次互相发送相同的首部造成的浪费较多； 服务器是按请求的顺序响应的，如果服务器响应慢，会招致客户端一直请求不到数据，也就是队头阻塞； 没有请求优先级控制； 请求只能从客户端开始，服务器只能被动响应。 3.2 HTTP/2 的优化 # HTTP/2 相比 HTTP/1.1 性能上的改进：\n头部压缩：通过HPACK 算法，在客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，之后就不发送同样字段了，只发送索引号，可以提高速度。 二进制格式：HTTP/2 不再像 HTTP/1.1 里的纯文本形式的报文，而是全面采用了二进制格式**，头信息和数据体都是二进制，并且统称为帧（frame）：头信息帧（Headers Frame）和数据帧（Data Frame）。 并发传输：HTTP/1.1 的实现是基于请求-响应模型的，可能会出现队头阻塞的情况，而 HTTP/2 针对不同的 HTTP 请求用独一无二的 Stream ID 来区分，接收端可以通过 Stream ID 有序组装成 HTTP 消息，不同 Stream 的帧是可以乱序发送的，因此可以并发不同的 Stream ，也就是 HTTP/2 可以并行交错地发送请求和响应。 服务器主动推送资源：HTTP/2 还在一定程度上改善了传统的「请求 - 应答」工作模式，服务端不再是被动地响应，可以主动向客户端发送消息。（客户端和服务器双方都可以建立 Stream， Stream ID 也是有区别的，客户端建立的 Stream 必须是奇数号，而服务器建立的 Stream 必须是偶数号）。 3.3 HTTP/3 的优化 # HTTP/2 队头阻塞的问题是因为 TCP，所以 HTTP/3 把 HTTP 下层的 TCP 协议改成了 UDP，基于 UDP 的 QUIC 协议 可以实现类似 TCP 的可靠性传输：\n**无队头阻塞：QUIC 有自己的一套机制可以保证传输的可靠性的。当某个流发生丢包时，只会阻塞这个流，其他流不会受到影响，因此不存在队头阻塞问题。 **更快的连接建立：HTTP/3 在传输数据前虽然需要 QUIC 协议握手，但是这个握手过程只需要 1 RTT，握手的目的是为确认双方的「连接 ID」，连接迁移就是基于连接 ID 实现的 连接迁移：基于 TCP 传输协议的 HTTP 协议，由于是通过四元组（源 IP、源端口、目的 IP、目的端口）确定一条 TCP 连接，所以当 IP 地址改变就需要断开连接，重新进行建立连接，而 QUIC 协议没有用四元组的方式来“绑定”连接，而是通过连接 ID** 来标记通信的两个端点，客户端和服务器可以各自选择一组 ID 来标记自己，因此即使移动设备的网络变化后，导致 IP 地址变化了，只要仍保有上下文信息（比如连接 ID、TLS 密钥等），就可以“无缝”地复用原连接，消除重连的成本，没有丝毫卡顿感，达到了连接迁移的功能。 4.HTTP 缓存技术 # 4.1 强制缓存 # 指的是只要浏览器判断缓存没有过期，则直接使用浏览器的本地缓存，决定是否使用缓存的主动性在于浏览器这边。\nsize 项中标识的是 from disk cache，就是使用了强制缓存 Cache-Control， 是一个相对时间； Expires，是一个绝对时间； 当浏览器第一次请求访问服务器资源时，服务器会在返回这个资源的同时，在 Response 头部加上 Cache-Control，Cache-Control 中设置了过期时间大小； 浏览器再次请求访问服务器中的该资源时，会先通过请求资源的时间与 Cache-Control 中设置的过期时间大小，来计算出该资源是否过期，如果没有，则使用该缓存，否则重新请求服务器； 服务器再次收到请求后，会再次更新 Response 头部的 Cache-Control 4.2 协商缓存 # 某些请求的响应码是 304，这个是告诉浏览器可以使用本地缓存的资源，通过服务端告知客户端是否可以使用缓存的方式被称为协商缓存\n协商缓存可以基于两种头部来实现：\n第一种：请求头部中的 If-Modified-Since 字段与响应头部中的 Last-Modified 字段实现，这两个字段的意思是：\n响应头部中的 Last-Modified：标示这个响应资源的最后修改时间； 请求头部中的 If-Modified-Since：当资源过期了，发现响应头中具有 Last-Modified 声明，则再次发起请求的时候带上 Last-Modified 的时间，服务器收到请求后发现有 If-Modified-Since 则与被请求资源的最后修改时间进行对比（Last-Modified），如果最后修改时间较新（大），说明资源又被改过，则返回最新资源，HTTP 200 OK；如果最后修改时间较旧（小），说明资源无新修改，响应 HTTP 304 走缓存。 第二种：请求头部中的 If-None-Match 字段与响应头部中的 ETag 字段：\n响应头部中 Etag：唯一标识响应资源； 请求头部中的 If-None-Match：当资源过期时，浏览器发现响应头里有 Etag，则再次向服务器发起请求时，会将请求头 If-None-Match 值设置为 Etag 的值。服务器收到请求后进行比对，如果资源没有变化返回 304，如果资源变化了返回 200。 ","date":"18 September 2023","permalink":"/basic/network/review/02/","section":"Basics","summary":"HTTP 相关技术","title":"HTTP"},{"content":"","date":"18 September 2023","permalink":"/tags/network/","section":"Tags","summary":"","title":"network"},{"content":" 1.TCP 三次握手过程 # 首先客户端和服务端都处于 CLOSE 状态。先是服务端主动监听某个端口，处于 LISTEN 状态 客户端会随机初始化序号（client_isn），将此序号置于 TCP 首部的「序列号」字段中，同时把 SYN 标志位置为 1，表示 SYN 报文。接着把第一个 SYN 报文发送给服务端，表示向服务端发起连接，该报文不包含应用层数据，之后客户端处于 SYN-SENT 状态 服务端收到客户端的 SYN 报文后，首先服务端也随机初始化自己的序号（server_isn），将此序号填入 TCP 首部的「序列号」字段中，其次把 TCP 首部的「确认应答号」字段填入 client_isn + 1, 接着把 SYN 和 ACK 标志位置为 1。最后把该报文发给客户端，该报文也不包含应用层数据，之后服务端处于 SYN-RCVD 状态 客户端收到服务端报文后，还要向服务端回应最后一个应答报文，首先该应答报文 TCP 首部 ACK 标志位置为 1 ，其次「确认应答号」字段填入 server_isn + 1 ，最后把报文发送给服务端，这次报文可以携带客户到服务端的数据，之后客户端处于 ESTABLISHED 状态 服务端收到客户端的应答报文后，也进入 ESTABLISHED 状态 三次握手的原因：\n三次握手才可以阻止重复历史连接的初始化（主要原因）：两次握手的情况下，服务端没有中间状态给客户端来阻止历史连接，导致服务端可能建立一个历史连接，造成资源浪费 三次握手才可以同步双方的初始序列号 三次握手才可以避免资源浪费：如果客户端发送的 SYN 报文在网络中阻塞了，重复发送多次 SYN 报文，那么服务端在收到请求后就会建立多个冗余的无效链接，造成不必要的资源浪费 2.TCP 四次挥手过程 # 如果客户端打算关闭连接，首先会发送一个 TCP 首部 FIN 标志位被置为 1 的 FIN 报文，之后客户端进入 FIN_WAIT_1 状态。 服务端收到该报文后，就向客户端发送 ACK 应答报文，接着服务端进入 CLOSE_WAIT 状态。 客户端收到服务端的 ACK 应答报文后，之后进入 FIN_WAIT_2 状态。 等待服务端处理完数据后，也向客户端发送 FIN 报文，之后服务端进入 LAST_ACK 状态。 客户端收到服务端的 FIN 报文后，回一个 ACK 应答报文，之后进入 TIME_WAIT 状态 服务端收到了 ACK 应答报文后，就进入了 CLOSE 状态，至此服务端已经完成连接的关闭。 客户端在经过 2MSL 一段时间后，自动进入 CLOSE 状态，至此客户端也完成连接的关闭。 3.TCP 和 UDP 区别 # 1. 连接\nTCP 是面向连接的传输层协议，传输数据前先要建立连接。 UDP 是不需要连接，即刻传输数据。 2. 服务对象\nTCP 是一对一的两点服务，即一条连接只有两个端点。 UDP 支持一对一、一对多、多对多的交互通信 3. 可靠性\nTCP 是可靠交付数据的，数据可以无差错、不丢失、不重复、按序到达。 UDP 是尽最大努力交付，不保证可靠交付数据。但是我们可以基于 UDP 传输协议实现一个可靠的传输协议，比如 QUIC 协议。 4. 拥塞控制、流量控制\nTCP 有拥塞控制和流量控制机制，保证数据传输的安全性。 UDP 则没有，即使网络非常拥堵了，也不会影响 UDP 的发送速率。 5. 首部开销\nTCP 首部长度较长，会有一定的开销，首部在没有使用「选项」字段时是 20 个字节，如果使用了「选项」字段则会变长的。 UDP 首部只有 8 个字节，并且是固定不变的，开销较小。 6. 传输方式\nTCP 是流式传输，没有边界，但保证顺序和可靠。 UDP 是一个包一个包的发送，是有边界的，但可能会丢包和乱序。 7. 分片不同\nTCP 的数据大小如果大于 MSS 大小，则会在传输层进行分片，目标主机收到后，也同样在传输层组装 TCP 数据包，如果中途丢失了一个分片，只需要传输丢失的这个分片。 UDP 的数据大小如果大于 MTU 大小，则会在 IP 层进行分片，目标主机收到后，在 IP 层组装完数据，接着再传给传输层。 TCP 和 UDP 应用场景：\n由于 TCP 是面向连接，能保证数据的可靠性交付，因此经常用于：\nFTP 文件传输； HTTP / HTTPS； 由于 UDP 面向无连接，它可以随时发送数据，再加上 UDP 本身的处理既简单又高效，因此经常用于：\n包总量较少的通信，如 DNS 、SNMP 等； 视频、音频等多媒体通信； 广播通信。 4.为什么挥手需要四次 # 关闭连接时，客户端向服务端发送 FIN 时，仅仅表示客户端不再发送数据了但是还能接收数据。 服务端收到客户端的 FIN 报文时，先回一个 ACK 应答报文，而服务端可能还有数据需要处理和发送，等服务端不再发送数据时，才发送 FIN 报文给客户端来表示同意现在关闭连接。 而服务端通常需要等待完成数据的发送和处理，所以服务端的 ACK 和 FIN 一般都会分开发送，所以是需要四次挥手，但是在特定情况下，四次挥手是可以变成三次挥手的。（当被动关闭方在 TCP 挥手过程中，如果「没有数据要发送」，同时「没有开启 TCP_QUICKACK（默认情况就是没有开启，没有开启 TCP_QUICKACK，等于就是在使用 TCP 延迟确认机制）」，那么第二和第三次挥手就会合并传输，这样就出现了三次挥手。）\n5.重传机制 # **超时重传：数据包丢失、确认应答丢失：**在发送数据时，设定一个定时器，当超过指定的时间后，没有收到对方的 ACK 确认应答报文，就会重发该数据 **快速重传：不以时间为驱动，而是以数据驱动重传：**当收到三个相同的 ACK 报文时，会在定时器过期之前，重传丢失的报文段。（存在问题：重传一个，还是重传所有的问题） SACK：选择性确认：在 TCP 头部「选项」字段里加一个 SACK ，可以将已收到的数据的信息发送给「发送方」，这样发送方就可以知道哪些数据收到了，哪些数据没收到，可以只重传丢失的数据。 D-SACK：使用了 SACK 来告诉「发送方」有哪些数据被重复接收。 6.滑动窗口 # 解决了往返时间越长，通信效率越低的问题，窗口大小就是指无需等待确认应答，而可以继续发送数据的最大值。（通常窗口的大小是由接收方的窗口大小来决定的，发送方发送的数据大小不能超过接收方的窗口大小，否则接收方就无法正常接收到数据）\n7.流量控制 # 发送方将数据发送给接收方，要考虑接收方处理能力：TCP 提供一种机制可以让「发送方」根据「接收方」的实际接收能力控制发送的数据量，即流量控制。\n8.拥塞控制 # 因为流量控制是避免「发送方」的数据填满「接收方」的缓存，但是并不知道网络的中发生了什么。而计算机网络都处在一个共享的环境。因此也有可能会因为其他主机之间的通信使得网络拥堵。拥塞控制的目的是避免「发送方」的数据填满整个网络。\n拥塞控制主要是四个算法：\n慢启动 拥塞避免 拥塞发生：超时重传、快速重传 快速恢复 9.TCP 序列号和确认号 # 发送的 TCP 报文：\n公式一：序列号 = 上一次发送的序列号 + len（数据长度）。特殊情况，如果上一次发送的报文是 SYN 报文或者 FIN 报文，则改为 上一次发送的序列号 + 1。 公式二：确认号 = 上一次收到的报文中的序列号 + len（数据长度）。特殊情况，如果收到的是 SYN 报文或者 FIN 报文，则改为上一次收到的报文中的序列号 + 1。 三个字段的作用：\n序列号：在建立连接时由内核生成的随机数作为其初始值，通过 SYN 报文传给接收端主机，每发送一次数据，就「累加」一次该「数据字节数」的大小。用来解决网络包乱序问题。 确认号：指下一次「期望」收到的数据的序列号，发送端收到接收方发来的 ACK 确认报文以后，就可以认为在这个序号以前的数据都已经被正常接收。用来解决丢包的问题。 控制位：用来标识 TCP 报文是什么类型的报文，比如是 SYN 报文、数据报文、ACK 报文，FIN 报文等。 10.TIME_WAIT 的作用 # 防止历史连接中的数据，被后面相同四元组的连接错误的接收 保证「被动关闭连接」的一方，能被正确的关闭 11.TIME_WAIT 过多的危害 # 占用系统资源，比如文件描述符、内存资源、CPU 资源、线程资源等。 占用端口资源，端口资源也是有限的，一般可以开启的端口为 32768～61000，也可以通过 net.ipv4.ip_local_port_range参数指定范围。 12.TIME_WAIT 优化 # 打开 net.ipv4.tcp_tw_reuse 和 net.ipv4.tcp_timestamps 选项（可以复用处于 TIME_WAIT 的 socket 为新的连接所用） net.ipv4.tcp_max_tw_buckets（当系统中处于 TIME_WAIT 的连接一旦超过这个值时，系统就会将后面的 TIME_WAIT 连接状态重置） 程序中使用 SO_LINGER ，应用强制使用 RST 关闭 13.服务器出现大量 TIME_WAIT 状态的原因 # 首先要知道 TIME_WAIT 状态是主动关闭连接方才会出现的状态，所以如果服务器出现大量的 TIME_WAIT 状态的 TCP 连接，就是说明服务器主动断开了很多 TCP 连接，服务端会主动断开连接呢的场景：\nHTTP 没有使用长连接 HTTP 长连接超时 HTTP 长连接的请求数量达到上限 14.TCP 服务端的流程 # 创建服务端 socket，bind 绑定端口、listen 监听端口 将服务端 socket 注册到 epoll epoll_wait 等待连接到来，连接到来时，调用 accpet 获取已连接的 socket 将已连接的 socket 注册到 epoll epoll_wait 等待事件发生 对方连接关闭时，我方调用 close ","date":"18 September 2023","permalink":"/basic/network/review/03/","section":"Basics","summary":"1.","title":"TCP"},{"content":"计算机网络模型的相关基础\n1.TCP/IP 网络模型 # TCP/IP 网络通常是由上到下分成 4 层，分别是应用层，传输层，网络层和网络接口层：\n每一层的封装格式：\n网络接口层的传输单位是帧（frame），IP 层的传输单位是包（packet），TCP 层的传输单位是段（segment），HTTP 的传输单位则是消息或报文（message）。但这些名词并没有什么本质的区分，可以统称为数据包。\n2.OSI 网络模型 # 为了使得多种设备能通过网络相互通信，和为了解决各种不同设备在网络互联中的兼容性问题，国际标准化组织制定了开放式系统互联通信参考模型（Open System Interconnection Reference Model），也就是 OSI 网络模型，该模型主要有 7 层，分别是应用层、表示层、会话层、传输层、网络层、数据链路层以及物理层。\n每一层负责的职能都不同，如下：\n应用层，负责给应用程序提供统一的接口； 表示层，负责把数据转换成兼容另一个系统能识别的格式； 会话层，负责建立、管理和终止表示层实体之间的通信会话； 传输层，负责端到端的数据传输； 网络层，负责数据的路由、转发、分片； 数据链路层，负责数据的封帧和差错检测，以及 MAC 寻址； 物理层，负责在物理网络中传输数据帧； TCP/IP 网络模型共有 4 层，分别是应用层、传输层、网络层和网络接口层，每一层负责的职能如下：\n应用层，负责向用户提供一组应用程序，比如 HTTP、DNS、FTP 等; 传输层，负责端到端的通信，比如 TCP、UDP 等； 网络层，负责网络包的封装、分片、路由、转发，比如 IP、ICMP 等； 网络接口层，负责网络包在物理网络中的传输，比如网络包的封帧、 MAC 寻址、差错检测，以及通过网卡传输网络帧等； 网络包的报文如下图：\n路由器和交换机是有区别的。\n因为路由器是基于 IP 设计的，俗称三层网络设备，路由器的各个端口都具有 MAC 地址和 IP 地址； 而交换机是基于以太网设计的，俗称二层网络设备，交换机的端口不具有 MAC 地址。 3.Linux 网络协议栈 # 应用层数据在每一层的封装格式：\n其中：\n传输层，给应用数据前面增加了 TCP 头； 网络层，给 TCP 数据包前面增加了 IP 头； 网络接口层，给 IP 数据包前后分别增加了帧头和帧尾； 这些新增的头部和尾部，都有各自的作用，也都是按照特定的协议格式填充，这每一层都增加了各自的协议头，那自然网络包的大小就增大了，但物理链路并不能传输任意大小的数据包，所以在以太网中，规定了最大传输单元（MTU）是 1500 字节，也就是规定了单次传输的最大 IP 包大小。\n当网络包超过 MTU 的大小，就会在网络层分片，以确保分片后的 IP 包不会超过 MTU 大小，如果 MTU 越小，需要的分包就越多，那么网络吞吐能力就越差，相反的，如果 MTU 越大，需要的分包就越少，那么网络吞吐能力就越好。\n知道了 TCP/IP 网络模型，以及网络包的封装原理后，那么 Linux 网络协议栈的样子，你想必猜到了大概，它其实就类似于 TCP/IP 的四层结构：\n从上图的的网络协议栈，可以看到：\n应用程序需要通过系统调用，来跟 Socket 层进行数据交互； Socket 层的下面就是传输层、网络层和网络接口层； 最下面的一层，则是网卡驱动程序和硬件网卡设备； ","date":"18 September 2023","permalink":"/basic/network/review/01/","section":"Basics","summary":"计算机网络模型的相关基础","title":"计算机网络模型基础"},{"content":"","date":"3 September 2023","permalink":"/tags/questions/","section":"Tags","summary":"","title":"Questions"},{"content":"搜集来的一些问题，每天问问自己\nL4、L7 ","date":"3 September 2023","permalink":"/diary/questions/","section":"Diaries","summary":"搜集来的一些问题，每天问问自己","title":"问题日志"},{"content":"","date":"16 August 2023","permalink":"/cloud/","section":"Clouds","summary":"","title":"Clouds"},{"content":"","date":"16 August 2023","permalink":"/tags/grafana/","section":"Tags","summary":"","title":"Grafana"},{"content":"在自己的电脑上配了个 Grafana 的环境，有一些坑，作为记录\n1.ubuntu 桌面安装 Grafana 前传（Docker） # 因为打算在自己电脑上起一个 grafana 的镜像，所以没有使用 portainer 的方式用 compose 去管理、拉取镜像和运行容器（占内存（虽然不大））\n1.1 Docker 安装 # 1.1.1 安装 Docker Engine # 设置存储库 # 更新apt包索引并安装包以允许apt通过 HTTPS 使用存储库： sudo apt-get update sudo apt-get install ca-certificates curl gnupg 添加Docker官方GPG密钥： sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg 使用以下命令设置存储库： echo \\ \u0026#34;deb [arch=\u0026#34;$(dpkg --print-architecture)\u0026#34; signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ \u0026#34;$(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;$VERSION_CODENAME\u0026#34;)\u0026#34; stable\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null 更新apt包索引： sudo apt-get update 安装 Docker 引擎 # 安装 Docker 引擎、containerd 和 Docker Compose sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin 通过运行镜像来验证Docker Engine安装是否成功 hello-world sudo docker run hello-world 如果本地没有该镜像，这个命令将会下载测试镜像，在容器中运行它，打印出 \u0026ldquo;Hello from Docker\u0026rdquo;，并且退出\n至此，已经成功安装并启动了 Docker Engine 官方文档：https://docs.docker.com/engine/install/ubuntu/\n1.1.2 安装 Docker Desktop（非必要） # 在 Ubuntu 上安装 Docker Desktop 的推荐方法：\n设置 Docker 的包存储库 下载最新的 DEB 包 使用 apt 安装软件包，如下所示： sudo apt-get update sudo apt-get install ./docker-desktop-\u0026lt;version\u0026gt;-\u0026lt;arch\u0026gt;.deb 官方文档：https://docs.docker.com/desktop/install/ubuntu/\n1.2 Docker 设置 # 1.2.1 添加用户组（以非 root 用户身份管理 Docker） # Docker 守护进程绑定到 Unix 套接字而不是 TCP 端口，默认情况下，Unix 套接字归 root 用户所有，其他用户只能使用sudo，Docker 守护程序始终以 root 用户身份运行\n如果不想在 docker 命令前加上 sudo，可以创建 docker 组并将用户添加到其中，当 Docker 守护进程启动时，它会创建一个可供 docker 组成员访问的 Unix 套接字\n创建 docker 组 sudo groupadd docker 将用户添加到 docker 组中 sudo usermod -aG docker $USER $USER是一个环境变量，代表当前用户名\n刷新docker组，使其改动直接生效 newgrp docker 1.2.2 将 Docker 配置为开机启动 # 大多数当前的 Linux 发行版（RHEL、CentOS、Fedora、Debian、Ubuntu 16.04 和更高版本）用于systemd管理系统启动时启动的服务。在 Debian 和 Ubuntu 上，Docker 服务默认配置为在启动时启动。要在启动时为 Docker 和 Containerd自动启动其他发行版，请使用以下命令：\nsudo systemctl enable docker.service sudo systemctl enable containerd.service 要禁用此行为，请改用 disable\nsudo systemctl disable docker.service sudo systemctl disable containerd.service 1.2.3 Docker 常用挂载的三种方式 # 绑定挂载：绑定挂载是将主机上的文件或目录挂载到容器中。这种挂载方式允许容器与主机之间共享文件和目录，并且对其中一个的更改会直接影响到另一个。可以通过在运行容器时使用 -v 或 \u0026ndash;mount 参数来指定绑定挂载，例如： docker run -v /host/path:/container/path image_name 卷挂载（Volume Mounts）：卷挂载将 Docker 数据卷挂载到容器中。数据卷是一个可供一个或多个容器使用的特殊目录，用于存储数据和共享数据。它独立于容器的生命周期，容器可以在挂载点读取和写入数据，就像使用普通目录一样。可以通过在运行容器时使用 -v 或 \u0026ndash;mount 参数指定卷挂载，例如： docker run -v volume_name:/container/path image_name 临时文件系统（tmpfs）挂载：临时文件系统挂载允许将临时文件系统挂载到容器的指定路径。与绑定挂载和卷挂载不同，临时文件系统挂载仅在容器的生命周期内存在，并且不会对主机文件系统产生影响。可以通过在运行容器时使用 \u0026ndash;tmpfs 参数来指定临时文件系统挂载，例如： docker run --tmpfs /container/path image_name 2.Grafana 安装（正题） # 2.1 试错1 # docker run -d -p 3000:3000 -i --name grafana -v /home/static/csv:/var/lib/grafana/static/csv grafana/grafana 可以，但是缺少 ini 的映射，以及 grafana local mode has been disabled by your administrator 问题，后文解决\n2.2 试错2 # docker run --user root -d -p 3000:3000 --name grafana -v /home/static/csv:/opt/static/csv -v /home/swap/grafana:/etc/grafana/grafana.ini grafana/grafana 补充 ini 的配置映射后，有权限问题，无法直接映射（during container init: error mounting to rootfs at \u0026ldquo;/etc/grafana/grafana.ini\u0026rdquo;: mount /etc/grafana/grafana.ini (via /proc/self/fd/9)）\n2.3 试错3 # 关于 grafana local mode has been disabled by your administrator 的问题，根据 ChatGPT 的提示，去设置 allow_embedding 或类似的设置来控制本地模式的启用或禁用\ndocker run -d -p 3000:3000 -i --name grafana -v /home/static/csv:/opt/static/csv -e \u0026#34;GF_ALLOW_EMBEDDING=true\u0026#34; grafana/grafana 遇到了 Error response from daemon: invalid mode 的错误，使用了无效的模式\n2.4 解决 # 开启特权模式，同时允许 csv 相关数据的映射\ndocker run -d -p 3000:3000 -i --name grafana -v /home/static/csv:/opt/static/csv -e \u0026#34;GF_PLUGIN_ALLOW_LOCAL_MODE=true\u0026#34; grafana/grafana 更新：由于希望用户（被分享者）可以匿名查看看板（无密码访问）\ndocker run -d -p 3000:3000 -i --name grafana -v /home/static/csv:/opt/static/csv -e \u0026#34;GF_PLUGIN_ALLOW_LOCAL_MODE=true\u0026#34; -e \u0026#34;GF_AUTH_ANONYMOUS_ENABLED=true\u0026#34; grafana/grafana 总结：因为偷懒，没有做 grafana.ini 文件的映射，所以采取的是 GF_ 变量覆盖的方式去自定义 Grafana 相关的设置\n其他资料：https://www.cnblogs.com/woshimrf/p/docker-grafana.html\n3.Grafana csv 数据源使用技巧 # 因为相关数据的敏感性及 Python 大数据操作的便利性，所以存在以 csv 为数据源进行 Grafana 看板展示的需求（所以前面才会折腾本地模式的数据映射，就是为了 csv 数据源做铺垫（不涉及时序））\n前面的数据映射已经完成，现在只说一下如何在 Grafana 中配置数据源及相关使用技巧\n3.1 安装 csv 数据源插件 # 资料：http://zh-tw.dgrt.cn/a/1664648.html?action=onClick\n4.Grafana 免密登录 # 如果使用 ini 修改配置如下：\n[auth.anonymous] # enable anonymous access enabled = true #默认false # specify role for unauthenticated users org_role = Viewer #默认Viewer 使用环境变量，修改如下：\nGF_AUTH_ANONYMOUS_ENABLED=true 资料：（使用环境变量进行配置）\nhttps://www.freesion.com/article/2928606237/ https://www.mayanpeng.cn/archives/146.html ","date":"16 August 2023","permalink":"/cloud/grafana/skills/","section":"Clouds","summary":"在自己的电脑上配了个 Grafana 的环境，有一些坑，作为记录","title":"Grafana 本地安装"},{"content":"","date":"12 August 2023","permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker"},{"content":"Dockerfile 相关内容\n1.docker 相关操作技巧 # 如果 docker 的命令需要添加 sudo 的执行权限，每次都要输入 sudo 比较麻烦，可以通过 id nick 的命令查看当前系统中的用户角色及相关的 group，然后 addgroup -a nick docker，把当前角色加入到 docker 的组中（nick 是角色名）（去掉 sudo）\nid nick sudo addgroup -a nick docker sudo service docker restart 2.如何创建镜像 # 2.1 不使用缓存编译 # docker 缓存镜像的规则：每一行 dockerfile 是一层，dockerfile 内容没变 cache 不会变（dockerfile 没改过，他就一直用缓存），可以通过 \u0026ndash;no-cache 刷新缓存，强制重新编译\n2.2 dockerfile 关键词 # 关键词：\nFROM：使用的基础镜像，其实就是文件系统（root fs）（可以有多个 FROM 作为后面的基础镜像去使用） MAINTAINER：作者 LABEL：tag 版本（todo 1.2） RUN：执行的指令 EXPOSE：容器暴露的端口（可以做映射，但最好不做，更灵活，根据实际情况映射） CMD：容器的启动命令 ENV：容器的环境变量 ADD：添加网络文件，指定一个路径，下载并添加到容器；tar.gz 自动加压（有些压缩包解压不了，需要注意） COPY：只是把宿主机的文件拷贝到容器镜像（不能解压），或者从上一个编译结果 copy 到下一个编译过程（COPY \u0026ndash;from） entrypoint：容器的入口程序 volume：过载磁盘 user：运行的身份 workdir：工作目录 arg：编译镜像时可以加入参数 onbuild：下载一些指令 stopsignal：容器的退出信号（通过docker history 查看一下） 前一阶段的结果 copy 到第二阶段的编译过程，前面的镜像会被删掉抛弃掉，不占容器的空间，因为已经编译完了，拿到了想要的结果，最终的基础镜像是这个 alpine，只有 11M 这个有 1.1G，所以上面的方法是一个减小镜像大小的手段\ndocker rm 是删除容器\ndocker rmi 是删除镜像\n注意：镜像就是一个文件系统，不包括内核之类的依赖\n2.3 干净的 kill 一个容器 # 通过 docker events 观察\ndocker stop：本质 kill exitCode=0 signal=3（可以理解为 docker kill 的一种特殊情况，可以感觉的 kill 掉一个进程，可以帮助我们把进程的守卫收拾干净）\ndocker kill：exitCode=137 signal=9（可能进程终止了，没有去收拾守卫进程）\n","date":"12 August 2023","permalink":"/cloud/docker/dockerfile/","section":"Clouds","summary":"Dockerfile 相关内容","title":"Dockerfile 相关内容"},{"content":"Grafana docker compose 安装记录及问题总结\n1.网上相关写法 # 1.1 docker compose # version: \u0026#39;3\u0026#39; services: grafana: image: \u0026#34;grafana/grafana\u0026#34; container_name: grafana restart: always ports: - \u0026#39;3000:3000\u0026#39; volumes: - \u0026#34;./etc/data:/var/lib/grafana\u0026#34; - \u0026#34;./etc/grafana.ini:/etc/grafana/grafana.ini\u0026#34; - \u0026#34;./etc/localtime:/etc/localtime\u0026#34; environment: - GF_USERS_ALLOW_SIGN_UP=false - GF_USERS_ALLOW_ORG_CREATE=false - GF_USERS_AUTO_ASSIGN_ORG_ROLE=Read Only Editor - GF_DATABASE_TYPE=mysql - GF_DATABASE_HOST=host:port - GF_DATABASE_NAME=grafana - GF_DATABASE_USER=root - GF_DATABASE_PASSWORD=****** 1.2 字段解释 # GF_USERS_ALLOW_SIGN_UP：设置为 false 时表示禁止⽤用户注册、创建用户账号，默认为 false；但管理员用户仍然可以从 Grafana 管理界面创建用户 GF_USERS_ALLOW_ORG_CREATE：设置为 false 禁⽌用户创建新组织，默认为 false GF_USERS_AUTO_ASSIGN_ORG_ROLE：新⽤户将分配给主要组织的角色，默认为 viewer，其他有效选项为 admin 和 editor GF_DATABASE_TYPE：数据库类型，⽆非就是 mysql、postgres 或者 sqlites GF_DATABASE_HOST：仅适⽤于 mysql 或 postgres，包括 ip 或主机名和端口，例如，对于 Grafana 在同一台主机上运行时 mySQL: host = 127.0.0.1 : 3306 GF_DATABASE_NAME：Grafana 数据库名称，下述中为是定义为 grafana_test GF_DATABASE_USER：数据库用户，不适用于 sqlites3 GF_DATABASE_PASSWORD：数据库⽤户密码(不适⽤于 sqlites)，如果密码中包含 # 或 ; 则必须用3个 （以上 docker compose 及字段解释均源于网络）\n2.踩坑记录 # 上面的写法看起来没什么问题，但是在实际的部署、运行过程中存在着很多的坑\n2.1 问题1： # can\u0026rsquo;t create directory \u0026lsquo;/var/lib/grafana/plugins\u0026rsquo;: Permission denied\nlevel=error msg=\u0026ldquo;alert migration failure: could not get migration log\u0026rdquo; error=\u0026ldquo;failed to check table existence: dial tcp: address tcp/\u0026lt;25615\u0026gt;: unknown port\u0026rdquo;\n2. 问题： # GF_PATHS_DATA=\u0026rsquo;/var/lib/grafana\u0026rsquo; is not writable\nversion: \u0026#39;3\u0026#39; services: grafana: image: grafana/grafana container_name: grafana restart: always ports: - 3000:3000 user: \u0026#39;0\u0026#39; volumes: - /home/docker/grafana/data:/var/lib/grafana - /home/docker/grafana/plugins:/var/lib/grafana/plugins environment: - GF_DATABASE_TYPE=mysql - GF_DATABASE_HOST= - GF_DATABASE_PORT= - GF_DATABASE_NAME=grafana - GF_DATABASE_USER=root - GF_DATABASE_PASSWORD=******* ","date":"10 August 2023","permalink":"/cloud/grafana/install/","section":"Clouds","summary":"Grafana docker compose 安装记录及问题总结","title":"Grafana 安装配置"},{"content":"","date":"10 August 2023","permalink":"/tags/install/","section":"Tags","summary":"","title":"Install"},{"content":"Redis docker compose 安装记录\nversion: \u0026#34;3.0\u0026#34; services: redis: image: redis ports: - 6379:6379 volumes: #Redis持久化数据映射 - /home/docker/redis/data:/data #准备自己的redis配置文件，包含redis连接密码 - /home/docker/redis/config:/etc/redis/redis.conf #redis存储路径 - /home/docker/redis/logs:/logs # 容器启动后在容器中执行的命令，启动redis,appendonly参数可用来持久化redis数据参数 # 覆盖容器启动后默认执行的命令 # command: redis-server /usr/local/etc/redis/redis.conf command: redis-server --requirepass ****** restart: always ","date":"9 August 2023","permalink":"/cloud/redis/install/","section":"Clouds","summary":"Redis docker compose 安装记录","title":"Redis 安装配置"},{"content":"","date":"30 July 2023","permalink":"/tags/git/","section":"Tags","summary":"","title":"Git"},{"content":"介绍 Git 相关的操作指令\n1.Git 常用指令 # 切换分支 git checkout test 创建并切换分支 git checkout -b test 拉取更新代码 git pull 查看 git 配置文件（全局） vi ~/.gitconfig git config --global --edit 测试 ssh 方式访问 gitlab ssh -T git@git.n.***.com 查看 ssh 文件 cd ~/.ssh cat id_rsa.pub git config 配置 git config --global user.name \u0026#34;newtsun\u0026#34; git config --global user.email \u0026#34;newtsun@*.com\u0026#34; git config --global --replace-all user.（如果冲突，可以 ） 2.Git fetch 和 Git pull 区别 # git fetch\n在拉取代码过程中，git fetch会首先检查本地仓库和远程仓库的差异，检查哪些不存在于本地仓库，然后将这些变动的提交拉取到本地。\n但是，这里请注意，它是把远程提交拉取到本地仓库，而不是本地 工作目录，它不会自行将这些新数据合并到当前工作目录中，我们需要继续执行git merge才会把这些变动合并到当前工作目录。\ngit pull\ngit pull和git fetch刚好相反，它直接获取远程的最新提交，直接拉取并合并到本地工作目录，而且在合并过程中不会经过我们的审查，如果不仔细检查，这样很容易遇到冲突。\n理解了git pull和git fetch的区别，那么该用哪种方式呢？\n相比之下，git fetch是一个更安全的选择，因为它从你的远程仓库拉入所有的提交，但不会对你的本地文件做任何修改。\n这给了你足够时间去发现远程仓库自从你上次拉取后到现在为止发生的变化。\n你可以在合并前检查哪些文件有变化，哪些文件可能导致冲突。\n而git pull相当于运行git fetch，然后立即将你的改动合并到本地仓库。这样的确少了一个步骤，但是也会带来一些风险。\n","date":"30 July 2023","permalink":"/cloud/git/","section":"Clouds","summary":"介绍 Git 相关的操作指令","title":"Git 常用操作"},{"content":"","date":"23 July 2023","permalink":"/tags/update/","section":"Tags","summary":"","title":"Update"},{"content":"Blog 更新日志\n2023.07.30 # Update Linux、Diary Add Cloud 2023.07.23 # Update Linux、Diary、Golang 2023.07.15 # First Commit ","date":"23 July 2023","permalink":"/diary/update/","section":"Diaries","summary":"Blog 更新日志","title":"更新日志"},{"content":"","date":"22 July 2023","permalink":"/tags/commands/","section":"Tags","summary":"","title":"Commands"},{"content":"Linux 常用的命令及相关的基础知识\n1.Linux 常用指令 # 安装 sudo dpkg -i package_name.deb 复制 sudo cp -r ~/下载/jbr jbr 删除 sudo rm -r Goland-2023.1.2/ 查看进程并过滤所需的端口 netstat -anp | grep 5678 查看 lsof -i：5678 删除 kill -9 174189 goland 安装 sudo tar -C /usr/local -xzf goland-2020.3.4.tar.gz hostname hostname -I df 查看当前系统中挂载的文件系统类型、各文件系统 inode 使用情况及文件系统挂载点 df -i --print-type ctrl + h // 查看隐藏文件 // 使用该 file 实用程序指示行结尾的类型 file testfile.txt 找出文本行尾符： https://stackoverflow.com/questions/3569997/how-to-find-out-line-endings-in-a-text-file\n2.vim 学习及指令 # vim一共分为三种模式，分别为：命令模式，输入模式，退出模式。\n启动vim，进入命令模式，在这样的状态下输入任何东西都会被vim识别为命令，比如我们输入i，并不是输入的是字符而是i这个命令。那么“i” 就是常用的命令，他表示切换到输入模式，只有进入到输入模式才能输入字符。\n命令模式：:表示切换到命令模式，在最后一行输入命令 输入模式：进入到vim中，按下i命令，进入输入模式，在输入模式中可以使用以下指令 退出模式：在输入完之后想保存退出，那么就要用到以下 q：表示退出，没有做过任何编辑 wq：编辑完之后，保存并退出 q!：强制退出，放弃修改 wq!：强制退出并保存（对自己的文件或者root用户） 3.Linux 程序安装 # 3.1 deb 安装 # sudo dpkg -i package_name.deb 3.2 AppImage 安装 # // 在 /opt/ 创建程序文件夹 sudo mkdir /opt/obsidian // 将下载的 AppImage 文件移动至该文件下 sudo mv ~/Downloads/Obsidian-0.12.19.AppImage /opt/obsidian/Obsidian-0.12.19.AppImage // 赋予执行权限 chmod 777 /opt/obsidian/Obsidian-0.12.19.AppImage // 在 /bin/ 文件下做一个链接 sudo ln -s /opt/obsidian/Obsidian-0.12.19.AppImage /bin/ // 加入启动器 sudo vi /usr/share/applications/obsidian.desktop // 编辑内容 [Desktop Entry] Encoding=UTF-8 Name=Obsidian Exec=/opt/obsidian/Obsidian-0.12.19.AppImage Icon=/opt/obsidian/Obsidian.png Terminal=False Type=Application StartupNotify=False Categories=Application;Development // 好像也有简版 [Desktop Entry] Name=obsidian Exec=/opt/Obsidian-1.3.5-arm64.AppImage Icon=/home/orangepi/Pictures/obsidian.png Type=Application StartupNotify=true ","date":"22 July 2023","permalink":"/linux/learn/commands/","section":"Linuxes","summary":"Linux 常用的命令及相关的基础知识","title":"Linux 常用命令"},{"content":"","date":"22 July 2023","permalink":"/linux/","section":"Linuxes","summary":"","title":"Linuxes"},{"content":"把之前写的一些面经逐渐迁移过来了\u0026hellip;\n1.原子操作 # 原子操作的意思是说，这个操作在执行的过程中，其它协程不会看到执行一半的操作结果。\n在单处理器单核系统中，即使一个操作翻译成汇编不止一个指令，也有可能保持一致性。比如经常用来演示的并发场景下的 count++ 操作 （count++ 对应的汇编指令就有三条），如果像下面这样写：\nfunc main() { runtime.GOMAXPROCS(1) var w sync.WaitGroup count := int32(0) w.Add(100) for i := 0; i \u0026lt; 100; i++ { go func() { for j := 0; j \u0026lt; 20; j++ { count++ } w.Done() }() } w.Wait() fmt.Println(count) } 而在多核系统中，情况就变得复杂了许多。A核修改 count 的时候，由于 CPU 缓存的存在，B核读到的 count 值可能不是最新的值。如果我们将上面的例子中的第二行改成：\nruntime.GOMAXPROCS(2) 之后，程序每执行一次，结果都有可能不一样。\n解决思路除了使用前面介绍过的 Mutex，也可以使用今天要介绍的 atomic，具体使用方法是将 count++ 替换成：\natomic.AddInt32(\u0026amp;count, 1) 这样就能保证即使在多核系统下 count++ 也是一个原子操作。\n针对一些基本的原子操作，不同的 CPU 架构中有不同的机制来保证原子性，atomic 包将底层不同架构的实现进行了封装，对外提供通用的 API。\natomic 的基础方法：\n原子操作主要是两类：修改和加载存储。修改很好理解，就是在原来值的基础上改动；加载存储就是读写。\natomic 提供了 AddXXX、CompareAndSwapXXX、SwapXXX、LoadXXX、StoreXXX 等方法。\n需要注意的是，atomic 的操作对象是地址，所以传参的时候，需要传变量的地址，不能传变量的值。\n来源： https://zhuanlan.zhihu.com/p/359971105\n2.Map 的底层实现 # 底层是 hmap 结构体，通过调用runtime.makemap函数，主要工作就是初始化 hmap 结构体的各个字段，hmap 里维护着若干个 bucket 数组 (即桶数组)，bucket 数组中每个元素都是 bmap 结构，也即每个bucket（桶）都是bmap结构，每个桶中保存了8个kv对，如果8个满了，会使用 overflow 连接下一个桶(溢出桶)。\n计算 key 的hash值 通过最后的“B”位来确定在哪号桶，如果B为4，所以取 key 对应哈希值的后4位 根据 key 对应的 hash 值前8位快速确定是在这个桶的哪个位置（tophash） 对比key完整的hash是否匹配，如果匹配则获取对应value 如果都没有找到，就去连接的下一个溢出桶中找 扩容方式：\n相同容量扩容（元素会发生重排，但不会换桶） 2倍容量扩容（元素会重排，可能会发生桶迁移） type hmap struct { count int // 元素的个数 B uint8 // buckets 数组的长度就是 2^B 个 overflow uint16 // 溢出桶的数量 buckets unsafe.Pointer // 2^B个桶对应的数组指针 oldbuckets unsafe.Pointer // 发生扩容时，记录扩容前的buckets数组指针 extra *mapextra //用于保存溢出桶的地址 } type mapextra struct { overflow *[]*bmap oldoverflow *[]*bmap nextOverflow *bmap } type bmap struct { tophash [bucketCnt]uint8 } //在编译期间会产生新的结构体 type bmap struct { tophash [8]uint8 //存储哈希值的高8位 data byte[1] //key value数据:key/key/key/.../value/value/value... overflow *bmap //溢出bucket的地址 } 3.sync.Map 的底层实现 # sync.Map 的设计利用了 atmoic 和 mutex 的配合：\n使用了两个原生的 map 作为存储介质，分别是 read map 和 dirty map（只读字典和脏字典）。 只读字典使用 atomic.Value 来承载，保证原子性和高性能；脏字典则需要用互斥锁来保护，保证了互斥。 只读字典和脏字典中的键值对集合并不是实时同步的，它们在某些时间段内可能会有不同。 无论是 read 还是 dirty，本质上都是 *map[interface{}]entry 类型，这里的 entry 其实就是 Map 的 value 的容器。 entry 的本质，是一层封装，可以表示具体值的指针，也可以表示 key 已删除的状态（即逻辑假删除） 通过这种设计，规避了原生 map 无法并发安全 delete 的问题，同时在变更某个键所对应的值的时候，就也可以使用原子操作了\n4.goroutine 的原理 # 基于CSP并发模型开发了GMP调度器：\n1.G：是Goroutine的缩写，是Goroutine的控制结构，对 Goroutine 的抽象。其中包括执行的函数指令及参数；G 保存的任务对象；线程上下文切换，现场保护和现场恢复需要的寄存器(SP、IP)等信息。在 Go 语言中使用 runtime.g 结构表示。\n2.M：是内核线程，由操作系统调度以及管理，调度器最多可以创建 10000 个线程，在 Go 语言中使用 runtime.m 结构表示。（用户线程与内核线程的映射关系）\n3.P：是调度各个goroutine，使他们之间协调运行的逻辑处理器，但不代表真正的CPU的数量，真正决定并发程度的是P，初始化的时候一般会去读取GOMAXPROCS对应的值，如果没有显示设置，则会读取默认值，在Go1.5之后GOMAXPROCS被默认设置可用的核数，而之前则默认为1，在 Go 语言中使用 runtime.p 结构表示。\n4.指定cpu线程个数\n通过runtime.GOMAXPROCS()，可以指定P的个数,如果没有指定则默认跑满整个cpu\n5.GMP 调度模型 # goroutine 的调度是 Go 语言运行时（runtime）层面的实现，由 Go 语言本身实现的一套调度系统：go scheduler。是按照一定的规则将所有的 goroutine 调度到操作系统线程上执行。目前 Go 语言的调度器采用的是 GPM 调度模型。\nG：表示 goroutine，每执行一次go f()就创建一个 G，包含要执行的函数和上下文信息 全局队列（Global Queue）：存放等待运行的 G P：是调度各个goroutine，使他们之间协调运行的逻辑处理器，表示 goroutine 执行所需的资源，最多有 GOMAXPROCS 个 P 的本地队列：同全局队列类似，存放的也是等待运行的G，存的数量有限，不超过256个。新建 G 时，G 优先加入到 P 的本地队列，如果本地队列满了会批量移动部分 G 到全局队列 M：是内核线程，由操作系统调度以及管理，线程想运行任务就得获取 P，从 P 的本地队列获取 G，当 P 的本地队列为空时，M 也会尝试从全局队列或其他 P 的本地队列获取 G。M 运行 G，G 执行之后，M 会从 P 获取下一个 G，不断重复下去 Goroutine 调度器和操作系统调度器是通过 M 结合起来的，每个 M 都代表了1个内核线程，操作系统调度器负责把内核线程分配到 CPU 的核上执行 调度器的设计策略：\n复用线程：避免频繁的创建、销毁线程，而是对线程的复用\nwork stealing 机制：当本线程无可运行的G时，尝试从其他线程绑定的P偷取G，而不是销毁线程 hand off 机制：当本线程因为G进行系统调用阻塞时，线程释放绑定的P，把P转移给其他空闲的线程执行 利用并行：GOMAXPROCS设置P的数量，最多有GOMAXPROCS个线程分布在多个CPU上同时运行，GOMAXPROCS也限制了并发的程度，比如GOMAXPROCS = 核数/2，则最多利用了一半的CPU核进行并行\n抢占：在 coroutine 中要等待一个协程主动让出CPU才执行下一个协程，在Go中，一个goroutine最多占用CPU 10ms，防止其他goroutine被饿死，这就是goroutine不同于coroutine的一个地方\n全局G队列：在新的调度器中依然有全局G队列，但功能已经被弱化了，当M执行work stealing从其他P偷不到G时，它可以从全局G队列获取G\n6.channel 的底层实现 # channel 的操作封装在 runtime 包下的 chan.go 文件\ntype hchan struct { qcount uint // channel中环形队列数据总数，len()返回该值 dataqsiz uint // 环形队列的长度，make时指定，cap()返回该值 buf unsafe.Pointer // 指向环形队列的指针，缓存区基于环形队列实现 elemsize uint16 // 元素的大小 closed uint32 // channel关闭标志 elemtype *_type // 元素类型 sendx uint // 向channel发送数据时，写入的位置索引 recvx uint // 从channel读数据时，读取的位置索引 recvq waitq // buf空时，读取的goroutine等待队列 sendq waitq // buf满时，写入的goroutine等待队列 // lock protects all fields in hchan, as well as several // fields in sudogs blocked on this channel. // // Do not change another G\u0026#39;s status while holding this lock // (in particular, do not ready a G), as this can deadlock // with stack shrinking. lock mutex // 并发控制锁，同一时刻，只允许一个 } // 等待goroutine的双向链表结构 type waitq struct { first *sudog last *sudog } 7.unsafe.Pointer 的使用 # 7.1 使用 unsafe.Pointer 做类型转换 # 可以简洁适宜的转换两个在内存中结构一样的类型是使用 unsafe.Pointer 的一个主要原因。\n文档描述：\n如果T2与T1一样大，并且两者有相同的内存结构；那么就允许把一个类型的数据，重新定义成另一个类型的数据\n经典的例子，是文档中的一次使用，用来实现 math.Float64bits：\nfunc Float64bits(f float64) uint64 { return *(*uint64)(unsafe.Pointer(\u0026amp;f)) } 这似乎是一种非常简洁的完成这样转换的方法，但是这个过程中具体发生了什么？让我们一步步拆分一下：\n\u0026amp;f 拿到一个指向 f 存放 float64 值的指针。 unsafe.Pointer(\u0026amp;f) 将 *float64 类型转化成了 unsafe.Pointer 类型。 (uint64)(unsafe.Pointer(\u0026amp;f)) 将 unsafe.Pointer 类型转化成了 uint64。 _(_uint64)(unsafe.Pointer(\u0026amp;f)) 引用这个 *uint64 类型指针，转化为一个 uint64 类型的值。 第一个例子是下面过程的一个简洁表达：\nfunc Float64bits(floatVal float64) uint64 { // 获取一个指向存储这个float64类型值的指针。 floatPtr := \u0026amp;floatVal // 转化*float64类型到unsafe.Pointer类型。 unsafePtr := unsafe.Pointer(floatPtr) // 转化unsafe.Pointer类型到*uint64类型. uintPtr := (*uint64)(unsafePtr) // 解引用成一个uint64值 uintVal := *uintPtr return uintVal } 7.2 使用 unsafe.Pointer 处理系统调用 # 当处理系统调用时，有些时候需要传入一个指向某块内存的指针给内核，以允许它执行某些任务。这是 unsafe.Pointer 在 Go 中另一个重要的使用场景。当需要处理系统调用时，就必须使用 unsafe.Pointer，因为为了使用 syscall.Syscall 家族函数，它可以被转化成 uintptr 类型。\n对于许多不同的操作系统，都拥有大量的系统调用。但是在这个例子中，我们将重点关注 ioctl 。ioctl，在UNIX类系统中，经常被用来操作那些无法直接映射到典型的文件系统操作，例如读和写的文件描述符。事实上，由于 ioctl 系统调用十分灵活，它并不在Go的 syscall 或者 x/sys/unix 包中。\n让我看看另一个真实的例子。\n现实例子：ioctl / vsock\n在过去的几年里，Linux 增加了一个新的 socket 家族，AF_VSOCK，它可以使管理中心和它的虚拟机之间双向，多对一的通信。 这些套接字使用一个上下文 ID 进行通信。通过发送一个带有特殊请求号的 ioctl 到 /dev/vsock 驱动，可以取到这个上下文 ID。\n下面是 ioctl 函数的定义：\nfunc Ioctl(fd uintptr, request int, argp unsafe.Pointer) error { _, _, errno := unix.Syscall( unix.SYS_IOCTL, fd, uintptr(request), // 在这个调用表达式中，从 unsafe.Pointer 到 uintptr 的转换是必须做的。详情可以查看 unsafe 包的文档 uintptr(argp), ) if errno != 0 { return os.NewSyscallError(\u0026#34;ioctl\u0026#34;, fmt.Errorf(\u0026#34;%d\u0026#34;, int(errno))) } return nil } 像代码注释所写一样，在这种场景下使用 unsafe.Pointer 有一个很重要的说明：\n在 syscall 包中的系统调用函数通过它们的 uintptr 类型参数直接操作系统，然后根据调用的详细情况，将它们中的一些转化为指针。换句话说，系统调用的执行，是其中某些参数从 uintptr 类型到指针类型的隐式转换。 如果一个指针参数必须转换成 uintptr 才能使用，那么这种转换必须出现在表达式内部。\n但是为什么会这样？这是编译器识别的特殊模式，本质上是指示垃圾收集器在函数调用完成之前，不能将被指针引用的内存再次安排。\n你可以通过阅读文档来获得更多的技术细节，但是你在Go中处理系统调用时必须记住这个规则。事实上，在写这篇文章时，我意识到我的代码违反了这一规则，现在已经被修复了。\n意识到这一点，我们可以看到这个函数是如何使用的。\n在 VM 套接字的例子里，我们想传递一个 *uint32 到内核，以便它可以把我们当时的上下文ID赋值到这块内存地址中。\nf, err := fs.Open(\u0026#34;/dev/vsock\u0026#34;) if err != nil { return 0, err } defer f.Close() // 存储上下文ID var cid uint32 // 从这台机器的 /dev/vsock 中获取上下文ID err = Ioctl(f.Fd(), unix.IOCTL_VM_SOCKETS_GET_LOCAL_CID, unsafe.Pointer(\u0026amp;cid)) if err != nil { return 0, err } // 返回当前的上下文ID给调用者 return cid, nil 这只是在系统调用时使用 unsafe.Pointer 的一个例子。你可以使用这么模式发送、接收任何数据，或者是用一些特殊方式配置一个内核接口。有很多可能的情况\n","date":"22 July 2023","permalink":"/golang/interview/ed/","section":"Golang","summary":"把之前写的一些面经逐渐迁移过来了\u0026hellip;","title":"Go 面经"},{"content":"","date":"22 July 2023","permalink":"/tags/interview/","section":"Tags","summary":"","title":"Interview"},{"content":"","date":"22 July 2023","permalink":"/tags/linux/","section":"Tags","summary":"","title":"Linux"},{"content":"文件系统\n1.文件系统 # 任何技术的出现是为了解决问题，文件系统也是为了解决某些问题。那文件系统是为了解决什么问题呢？\n我们有了一个相对形象的概念，文件系统管理着很多文件。而这些文件其实就是数据，这些数据又是存储在磁盘上的。因此，实质上文件系统是管理磁盘的软件系统，它简化了用户对磁盘空间的使用方法，并降低了磁盘空间的使用难度，通过更加形象的方式将磁盘中的数据展示给用户。 赵二狗窃窃私语：“好啰嗦” 大家对磁盘都比较清楚，其实就是存储数据的地方，我们可以将磁盘与仓库类比。类似一个空仓库，一个没有格式化的磁盘就好像一个空仓库，空间非常大，我们可以随便使用。\n磁盘的内部虽然非常复杂，但磁盘生产厂商做了很多工作，将磁盘的复杂性掩盖起来了。对于普通用户来说，磁盘就是一个线性空间，就好像C语言中的数组一样，通过偏移就可以访问其空间（读写数据）。\n但是，我们虽然可以直接访问磁盘的空间，如果缺乏规划，那么使用的最终结果可能是这样样子的。数据被毫无规律的放到磁盘上，最后查找的时候会非常费劲，甚至可能找不到需要的数据。\n因此，文件系统出现了。文件系统实现对磁盘空间的统一管理，一方面文件系统对磁盘空间进行统一规划，另外一方面文件系统提供给普通用户人性化的接口。就好比仓库中的货架，将空间进行规划和编排，这样根据编号可以方便的找到具体的货物。而文件系统也是类似，将磁盘空间进行规划和编号处理，这样通过文件名就可以找到具体的数据，而不用关心数据到底是怎么存储的。\n以Ext4文件系统为例，它将磁盘空间进行划分，并通过元数据实现对磁盘空间的管理。这样，用户对文件的操作就转化为文件系统对磁盘空间的操作。 也就是说，文件系统解决了普通用户使用磁盘存储数据的问题。\n2.分布式文件系统 # 上面说的是普通本地文件系统的概念，比如Ext4、XFS、FAT32和Btrfs等文件系统。这些文件系统只能在本地进行磁盘格式化并使用。那么什么是分布式文件系统呢？下面是维基百科给出的定义。\n相对于本机端的文件系统而言，分布式文件系统（英语：Distributed file system, DFS），或是网络文件系统（英语：Network File System），是一种允许文件透过网络在多台主机上分享的文件系统，可让多机器上的多用户分享文件和存储空间。\n通过定义我肯可以看出，分布式文件系统解决的是资源共享的问题。我们先看一个实例，以NFS文件系统为例，它分为服务端和客户端，客户端通过某种协议连接到服务端，此时会在客户端的目录树中映射一个子树，这样在客户端就能访问服务端的文件系统。然而，对于客户端来说，这个目录树关系是透明的，也就是用户不知道这些内容是在远程计算机，也不用关心。\n分布式文件系统解决的最大的问题是资源共享的问题，因此分布式文件系统最大的特点是多个客户端可以访问相同的服务端。\n图 NFS可以供多个客户端访问，但其毕竟是单机，处理能力是有限的。因此在大规模数据领域，不如电商网站、大数据处理等，采用单机模式无法满足要求。问题有出来了，为了解决这个问题，谷歌开发了GFS分布式文件系统，该文件系统的服务端通过一个集群来实现，客户端可以并发的访问该集群的多达数万个节点，因此承载能力得到极大的提升。 如图10是HDFS的架构图，HDFS是GFS的开源实现，起基本架构是一样的。整个集群的节点分为2中角色，一个是Master节点，负责管理元数据；另外一个是数据节点，负责存储文件的数据。在这种架构中，客户端通过Master节点可以得到文件数据的具体位置，然后可以直接与数据节点交互。由于数据节点可以很多（数万个），因此承载能力得到极大的提升。\n除了上述HDFS和GFS等分布式文件系统外，还有GlusterFS、CephFS等很多分布式文件系统，但每种分布式文件系统的细节又有所差异，这个也是与它们所要解决的具体问题相关的。\n3.集群文件系统 # 虽然分布式文件系统可以处理高并发访问的问题，但对于同一个文件同时写会存在数据不一致的问题。这个需要客户端的应用做特殊处理。如图11所示，客户端1从服务端读取的某个文件的数据，而此时在客户端进行了追加写操作，由于网络延迟或者什么原因，数据并没有达到服务端。此时，客户端2读取了服务端的数据，显然此时数据是旧数据。如果此时客户端2进行写操作，无法保证服务端最终的数据是客户端1的还是客户端2的，因此存在不可预料的结果。\n为了解决同一个文件被不同客户端的应用并发写的问题，这个时候开发了集群文件系统。其中比较著名的是Oracle的OCFS2文件系统。OCFS2通过分布式锁解决了写并发的问题，如果有进程对某个文件的区域进行写操作时会加锁，这样其它客户端如果对相同区域写数据时就必须等待。这样，OCFS2文件系统就保证了数据的一致性。 到此为止，我们介绍文件系统的原理及市面上常见的各种类型的文件系统。通过分析我们看到，不同的文件系统解决的问题是不同的，因此应用场景也是有很大差异的。因此，大家在工作中如果选型时，也需要考虑这些差异。\n","date":"22 July 2023","permalink":"/linux/learn/file-system/","section":"Linuxes","summary":"文件系统","title":"文件系统"},{"content":"全文目录\n1.IO 多路复用 # 1.1 IO 多路复用解释 # 首先拆解多路复用一词：\n多路：存在多个待服务的对象 复用：只由一个执行单元提供服务 串联上述要点，多路复用指的是，由一个执行单元，同时对多个对象提供服务，形成一对多的服务关系\n打个比方：多名顾客在餐厅内用餐，考虑到经营成本，很难做到为每名顾客单独提供一名招待员作一对一服务，因此餐厅经理安排每名服务生固定负责几个餐桌，服务生在几个桌次间来回辗转提供服务，这个过程本质上就是一种多路复用\n在 linux 操作系统中，对 IO 多路复用的概念有着更加明确的定义：\n多路：存在多个需要处理 io event 的 fd（linux 中，一切皆文件，所有事物均可抽象为一个文件句柄 file descriptor，简称 fd） 复用：复用一个 loop thread 同时为多个 fd 提供处理服务（线程 thread 是内核视角下的最小调度单位；多路复用通常为循环模型 loop model，因此称为 loop thread） IO 多路复用中，loop thread 是提供服务的乙方；待处理 io event 的 fd 们是甲方。本着顾客是上帝的原则，乙方有义务为甲方提供更优质的服务，这里的服务质量就体现在一句话：”随叫随到，别让老板等久了”\n在餐厅顾客没有需求的时候，服务生趁着闲工夫摸个鱼打个盹也尚无不可。但是一旦顾客招呼时，服务生需要第一时间赶到对需求作出响应\n此外，由于服务生和顾客之间的服务关系是一对多，所以还要考虑到有多名顾客同时招呼时，服务生如何作兼容处理，让每名顾客都不至于产生被冷落的感觉。这是一门学问，也同样是计算机领域 IO 多路复用场景下需要解决的问题\n1.2 多路复用简单实现 # 1.2.1 阻塞 IO # 通过一段伪代码，来尝试让 IO 多路复用这个概念看起来更加具体一些：\n// 多个待服务的 fd fds = [fd1,fd2,fd3,...] // 遍历 fd 列表，末尾和首部相连，形成循环 i = 0 for { // 获取本轮待处理的 fd fd = fds[i] // 从 fd 中读数据 data = read(fd) // 处理数据 handle(data) // 推进遍历 i++ if i == len(fds){ i = 0 } } 上述搭了个架子，核心分为几步：\n定义了待处理的 fds 列表（多路） 循环遍历 fds 列表，每轮负责读一个 fd（复用） 这是个乞丐版的 IO 多路复用模型看起来似乎有那么点意思了. 然而其本质上是一种阻塞 IO 模型（Blocking IO，简称 BIO）. 事实上，上述实现存在一个致命的问题，那就是句柄 fd 默认的 io 操作是阻塞型的，因此倘若在读 fd1 的时候，io event 没到达，那么 loop thread 就会陷入阻塞，后续 fd2、fd3 哪怕有 io event 到达，也无法得到执行\n上述问题翻译成更形象的场景，大概就是：\nA桌顾客对服务生说，你先搁这候着，我看会儿菜单，一会点菜\n服务生于是站定A桌，打定主意在A桌点完菜之后再离开\n在此期间，服务生辖区内的B桌、C桌招呼有事，服务生也充耳不闻，只等A桌事情完结才肯挪动步子\n这样的服务显然不够到位，倘若人人如此，餐厅必然面临倒闭\n1.2.2 非阻塞 IO # 基于 BIO 存在的问题，我们进行一轮改进，核心是将 read 操作由同步阻塞操作改为带有尝试性的非阻塞操作。在读一个 fd 的时候，倘若 io event 已就绪就正常读取，否则就即时返回并抛出一个特定类型的错误，让 loop thread 能够正常执行下去，为其他 fd 提供服务\n// 多个待服务的 fd fds = [fd1,fd2,fd3,...] // 遍历 fd 列表，末尾和首部相连，形成循环 i = 0 for { // 获取本轮待处理的 fd fd = fds[i] // 尝试从 fd 中读数据，失败时不阻塞，而是抛出错误 data,err = tryRead(fd) // 读取数据成功，处理数据 if err == nil{ handle(data) } // 小睡一秒后再推进流程 sleep(1 second) // 推进遍历 i++ if i == len(fds){ i = 0 } } 上述伪代码核心步骤如下：\n定义了待处理的 fds 列表 遍历 fds 列表，每轮尝试从一个 fd 中读数据 倘若 io event 已就绪，则正常处理结果 倘若 io event 未就绪，只抛出错误，同样不阻塞流程 小睡一会儿，然后继续推进流程 这里确实解决阻塞 IO 中的问题，其本质上是一种非阻塞 IO 模型（Nonblocking IO，简称 NIO），但这里仍然存在问题，就是每轮处理之间的休眠时间。倘若在休眠期间，fd 中有 io event 到达，就无法被正常处理，这同样是一种不好的体验\n这一问题翻译成餐厅的场景，指的就是服务生每次主动问询或者为一名客人提供服务后，就要大喘气休息几分钟，期间对客人不管不顾，这样的服务态度客人同样不会买账\n那大家可能会问了，倘若把此处的休眠操作去除了如何？\n答案是同样有问题. 倘若不限制轮询的执行频率，那么不轮 fd 中是否有 io event，程序都会一直高强度运行，这会导致 CPU 空转，造成很大程度的资源浪费\n用餐厅的场景来聊，指的是餐厅招了个视听都不好的服务生，他感应不到客人的召唤，需要时时刻刻奔走在各个餐桌之间主动去询问客人们是否需要服务。这种情况下，哪怕客人们性子好不嫌烦，服务生自己也被这种高强度的反复横跳动作给累坏了\n那大家可能又问了. 餐厅就不能招个正常的服务生吗，让他在听到客人的招呼时就去提供服务，否则就在一边老实歇着\n没错，这就是正解，设计程序的码农们也是这么想的. 然而实际情况很悲催，在用户态视角下的程序正是哪一个耳目昏聩的服务生，对于 io event 的到达并没有能力做到准确地把握\n于是，这就需要引入操作系统内核的帮助，通过几个内核对外暴露的接口，来进行 IO 多路复用的优雅实现，做到真正意义上的“随叫随到”\n1.3 IO 多路复用的优雅实现 # linux 内核提供了三种经典的多路复用技术：\n从上图中可以看到，各个技术之间通过单向箭头连接，因此是一个持续演化改进的过程，select 最通用，但是相对粗糙；而 epoll 则最精致，在性能上也有着最优越的表现\npoll 在 select 的基础之上做了改进，但治标不治本，优化得不够彻底. 我们核心还是来对比看看 select 和 epoll 之间的共性和差异：\n（1）select\n一次可以处理多个 fd，体现多路，但 fd 数量有限，最多 1024 个 loop thread 通过 select 将一组 fd 提交到内核做监听 当 fd 中无 io event 就绪时，loop thread 会陷入阻塞 每当这组 fd 中有 io event 到达时，内核会唤醒 loop thread loop thread 无法精准感知到哪些 fd 就绪，需要遍历一轮 fd 列表，时间复杂度 O(N) 托付给内核的 fd 列表只具有一轮交互的时效，新的轮次中，loop thread 需要重新将监听的 fd 列表再传递给内核一次 （2）epoll\n每次处理的 fd 数量无上限 loop thread 通过 epoll_create 操作创建一个 epoll 池子 loop thread 通过 epoll_ctl 每次将一个待监听的 fd 添加到 epoll 池中 每当 fd 列表中有 fd 就绪事件到达时，会唤醒 loop thread，同时内核会将处于就绪态的 fd 直接告知 loop thread，无需额外遍历 综上所述，select 和 epoll 等多路复用操作利用了内核的能力，能在待监听 fd 中有 io event 到达时，将 loop thread 唤醒，避免无意义的主动轮询操作\n其中，epoll 相比于 select 的核心性能优势在于：\nloop thread 被唤醒时，能明确知道哪些 fd 需要处理，减少了一次额外遍历的操作，时间复杂度由 O(N) 优化到 O(1) epoll 通过将创建池子和添加 fd 两个操作解耦，实现了池中 fd 数据的复用，减少了用户态与内核态间的数据拷贝成本 2.EventPoll 原理 # 2.1 核心指令 # epoll 又称 EventPoll，使用很简单，包含三个指令：\nepoll_create epoll_ctl epoll_wait 未完待续\u0026hellip;\n","date":"19 July 2023","permalink":"/golang/learn/epoll/","section":"Golang","summary":"全文目录","title":"Go 网络 IO 模型之 EPOLL"},{"content":"最近计划学习 Linux 内核相关内容，Professional Linux Kernel Architecture\n第一章：简介及概述 # 1.内核的任务 # 内核是硬件与软件之间的一个中间层，其作用是将应用程序的请求传递给硬件，并充当底层驱动程序，对系统中的各种设备和组件进行寻址。\n从应用程序的视角来看，内核可以被认为是一台增强的计算机，将计算机抽象到一个高层次上。例如，在内核寻址硬盘时，它必须确定使用哪个路径来从磁盘向内存复制数据，数据的位置，经由哪个路径向磁盘发送哪一条命令，等等。另一方面，应用程序只需发出传输数据的命令。实际的工作如何完成与应用程序是不相干的，因为内核抽象了相关的细节。应用程序与硬件本身没有联系，只与内核有联系，内核是应用程序所知道的层次结构中的最底层，因此内核是一台增强的计算机。 当若干程序在同一系统中并发运行时，也可以将内核视为资源管理程序。在这种情况下，内核负责将可用共享资源（包括CPU时间、磁盘空间、网络连接等）分配到各个系统进程，同时还需要保证系统的完整性。 另一种研究内核的视角是将内核视为库，其提供了一组面向系统的命令。通常，系统调用用于向计算机发送请求。借助于C标准库，系统调用对于应用程序就像是普通函数一样，其调用方式与其他函数相同。 2.实现策略 # 微内核：这种范型中，只有最基本的功能直接由中央内核（即微内核）实现。所有其他的功能都委托给一些独立进程，这些进程通过明确定义的通信接口与中心内核通信。例如，独立进程可能负责实现各种文件系统、内存管理等。（当然，与系统本身的通信需要用到最基本的内存管理功能，这是由微内核实现的。但系统调用层次上的处理则由外部的服务器进程实现。）理论上，这是一种很完美的方法，因为系统的各个部分彼此都很清楚地划分开来，同时也迫使程序员使用“清洁的”程序设计技术。这种方法的其他好处包括：动态可扩展性和在运行时切换重要组件。但由于在各个组件之间支持复杂通信需要额外的CPU时间，所以尽管微内核在各种研究领域早已经成为活跃主题，但在实用性方面进展甚微。 宏内核：：与微内核相反，宏内核是构建系统内核的传统方法。在这种方法中，内核的全部代码，包括所有子系统（如内存管理、文件系统、设备驱动程序）都打包到一个文件中。内核中的每个函数都可以访问内核中所有其他部分。如果编程时不小心，很可能会导致源代码中出现复杂的嵌套。 因为在目前，宏内核的性能仍然强于微内核，Linux仍然是依据这种范型实现的（以前亦如此）。\n3.内核的组成部分 # Linux 是整体式的宏内核：\n图1 内核的组成部分 3.1 进程、进程切换、调度 # 各个进程的地址空间是完全独立的，因此进程并不会意识到彼此的存在，从进程的角度来看，它会任务自己是系统中唯一的进程。如果进程想要彼此通信（例如交换数据），那么必须使用特定的内核机制。\n由于 Linux 是多任务系统，它支持（看上去）并发执行的若干进程。系统中真正在运行的进程数目最多不超过CPU数目，因此内核会按照短的时间间隔在不同的进程之间切换（用户是注意不到 的），这样就造成了同时处理多进程的假象。\n(1) 内核借助于CPU的帮助，负责进程切换的技术细节。必须给各个进程造成一种错觉，即CPU总是可用的。通过在撤销进程的CPU资源之前保存进程所有与状态相关的要素，并将进程置于空闲状态，即可达到这一目的。在重新激活进程时，则将保存的状态原样恢复。进程之间的切换称之为进程切换。\n(2) 内核还必须确定如何在现存进程之间共享CPU时间。重要进程得到的CPU时间多一点，次要进程得到的少一点。确定哪个进程运行多长时间的过程称为调度。\n3.2 UNIX 进程 # Linux对进程采用了一种层次系统，每个进程都依赖于一个父进程。内核启动init程序作为第一个进程，该进程负责进一步的系统初始化操作，并显示登录提示符或图形登录界面（现在使用比较广泛）。因此init是进程树的根，所有进程都直接或间接起源自该进程，如下面的pstree程序的输出所示。其中init是一个树型结构的顶端，而树的分支不断向下扩展。\n该树型结构的扩展方式与新进程的创建方式密切相关。UNIX操作系统中有两种创建新进程的机制，分别是fork和exec。\n(1) fork可以创建当前进程的一个副本，父进程和子进程只有PID（进程ID）不同。在该系统调用执行之后，系统中有两个进程，都执行同样的操作。父进程内存的内容将被复制，至少从程序的角度来看是这样。Linux使用了一种众所周知的技术来使fork操作更高效，该技术称为写时复制（copy on write），主要的原理是将内存复制操作延迟到父进程或子进程向某内存页面写入数据之前，在只读访问的情况下父进程和子进程可以共用同一内存页。\n例如，使用fork的一种可能的情况是，用户打开另一个浏览器窗口。如果选中了对应的选项，浏览器将执行fork，复制其代码，接下来子进程中将启动适当的操作建立新窗口。\n(2) exec将一个新程序加载到当前进程的内存中并执行。旧程序的内存页将刷出，其内容将替换为新的数据。然后开始执行新程序。\n4.内核的特别性 # 5.行文注记 # 6.小结 # ","date":"16 July 2023","permalink":"/linux/book/kernel01/","section":"Linuxes","summary":"最近计划学习 Linux 内核相关内容，Professional Linux Kernel Architecture","title":"深入 Linux 内核架构 01"},{"content":"终于在这个周日的晚上，把这个博客搭建完成了，希望之后可以养成写博客的习惯\n","date":"16 July 2023","permalink":"/diary/daily/","section":"Diaries","summary":"终于在这个周日的晚上，把这个博客搭建完成了，希望之后可以养成写博客的习惯","title":"2023年7月16日"},{"content":"","date":"16 July 2023","permalink":"/tags/diary/","section":"Tags","summary":"","title":"Diary"},{"content":" This is title # 这个包比较简单，就是将文件进行打包和解包，要是熟悉 Linux 下的 tar 命令这个就很好理解了。 主要是通过 tar.Reader 读取 tar 包，通过 tar.Writer 写入 tar 包，在写入的过程中再设置一下头，详细的过程以示例的方式进行展示，可以查看代码里面的注释。\n参考：\n标准库 tar 中文文档\n标准库 tar 官方文档\n单个文件操作 # 这个非常简单，就是读取一个文件，进行打包及解包操作即可。\n单个文件打包 # 从 /etc/passwd 下复制了一个 passwd 文件到当前目录下，用来做压缩测试。什么文件都是可以的，自己随意写一个也行。这里的示例主要为了说明 tar ，没有处理路径，所以过程全部假设是在当前目录下执行。\ncp /etc/passwd . 关于文件的打包直接查看示例代码，已经在示例代码中做了详细的注释。\n示例代码（ pack_single_file.go ）：\npackage main import ( \u0026#34;os\u0026#34; \u0026#34;log\u0026#34; \u0026#34;archive/tar\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; ) func main() { // 准备打包的源文件 var srcFile = \u0026#34;passwd\u0026#34; // 打包后的文件 var desFile = fmt.Sprintf(\u0026#34;%s.tar\u0026#34;,srcFile) // 需要注意文件的打开即关闭的顺序，因为 defer 是后入先出，所以关闭顺序很重要 // 第一次写这个示例的时候就没注意，导致写完的 tar 包不完整 // ###### 第 1 步，先准备好一个 tar.Writer 结构，然后再向里面写入内容。 ###### // 创建一个文件，用来保存打包后的 passwd.tar 文件 fw, err := os.Create(desFile) ErrPrintln(err) defer fw.Close() // 通过 fw 创建一个 tar.Writer tw := tar.NewWriter(fw) // 这里不要忘记关闭，如果不能成功关闭会造成 tar 包不完整 // 所以这里在关闭的同时进行判断，可以清楚的知道是否成功关闭 defer func() { if err := tw.Close(); err != nil { ErrPrintln(err) } }() // ###### 第 2 步，处理文件信息，也就是 tar.Header 相关的 ###### // tar 包共有两部分内容：文件信息和文件数据 // 通过 Stat 获取 FileInfo，然后通过 FileInfoHeader 得到 hdr tar.*Header fi, err := os.Stat(srcFile) ErrPrintln(err) hdr, err := tar.FileInfoHeader(fi, \u0026#34;\u0026#34;) // 将 tar 的文件信息 hdr 写入到 tw err = tw.WriteHeader(hdr) ErrPrintln(err) // 将文件数据写入 // 打开准备写入的文件 fr, err := os.Open(srcFile) ErrPrintln(err) defer fr.Close() written, err := io.Copy(tw, fr) ErrPrintln(err) log.Printf(\u0026#34;共写入了 %d 个字符的数据\\n\u0026#34;,written) } // 定义一个用来打印的函数，少写点代码，因为要处理很多次的 err // 后面其他示例还会继续使用这个函数，就不单独再写，望看到此函数了解 func ErrPrintln(err error) { if err != nil { log.Println(err) os.Exit(1) } } 单个文件解包 # 这个也很简单，基本上将上面过程反过来，只需要处理 tar.Reader 即可，详细的描述见示例。\n这里就用刚刚打包的 passwd.tar 文件做示例，如果怕结果看不出效果，可以将之前用的 passwd 源文件删除。\nrm passwd 示例代码（ unpack_single_file.go ）：\npackage main import ( \u0026#34;os\u0026#34; \u0026#34;archive/tar\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; ) func main() { var srcFile = \u0026#34;passwd.tar\u0026#34; // 将 tar 包打开 fr, err := os.Open(srcFile) ErrPrintln(err) defer fr.Close() // 通过 fr 创建一个 tar.*Reader 结构，然后将 tr 遍历，并将数据保存到磁盘中 tr := tar.NewReader(fr) for hdr, err := tr.Next(); err != io.EOF; hdr, err = tr.Next(){ // 处理 err ！= nil 的情况 ErrPrintln(err) // 获取文件信息 fi := hdr.FileInfo() // 创建一个空文件，用来写入解包后的数据 fw, err := os.Create(fi.Name()) ErrPrintln(err) // 将 tr 写入到 fw n, err := io.Copy(fw, tr) ErrPrintln(err) log.Printf(\u0026#34;解包： %s 到 %s ，共处理了 %d 个字符的数据。\u0026#34;, srcFile,fi.Name(),n) // 设置文件权限，这样可以保证和原始文件权限相同，如果不设置，会根据当前系统的 umask 来设置。 os.Chmod(fi.Name(),fi.Mode().Perm()) // 注意，因为是在循环中，所以就没有使用 defer 关闭文件 // 如果想使用 defer 的话，可以将文件写入的步骤单独封装在一个函数中即可 fw.Close() } } func ErrPrintln(err error){ if err != nil { log.Fatalln(err) os.Exit(1) } } 操作整个目录 # 我们实际中 tar 很少会去打包单个文件，一般都是打包整个目录，并且打包的时候通过 gzip 或者 bzip2 压缩。\n如果要打包整个目录，可以通过递归的方式来实现。这里只演示了 gzip 方式压缩，这个实现非常简单，只需要在 fw 和 tw 之前加上一层压缩即可，详情见示例代码。\n为了测试打包整个目录，复制了一个 log 目录到当前路径下。什么目录和文件都可以，只是因为这个里面内容比较多，就拿这个来做测试了。\n# 出现没有权限的错误不用管它，复制过来多少是多少吧 cp -r /var/log/ . 详细的操作会在注释中说明，不过在之前单文件中出现过的步骤不再注释。\n打包压缩 # 示例代码（ targz.go ）：\npackage main import ( \u0026#34;archive/tar\u0026#34; \u0026#34;compress/gzip\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; \u0026#34;strings\u0026#34; ) func main() { // 修改日志格式，显示出错代码的所在行，方便调试，实际项目中一般不记录这个。 var src = \u0026#34;apt\u0026#34; var dst = fmt.Sprintf(\u0026#34;%s.tar.gz\u0026#34;, src) // 将步骤写入了一个函数中，这样处理错误方便一些 if err := Tar(src, dst); err != nil { log.Fatalln(err) } } func Tar(src, dst string) (err error) { // 创建文件 fw, err := os.Create(dst) if err != nil { return } defer fw.Close() // 将 tar 包使用 gzip 压缩，其实添加压缩功能很简单， // 只需要在 fw 和 tw 之前加上一层压缩就行了，和 Linux 的管道的感觉类似 gw := gzip.NewWriter(fw) defer gw.Close() // 创建 Tar.Writer 结构 tw := tar.NewWriter(gw) // 如果需要启用 gzip 将上面代码注释，换成下面的 defer tw.Close() // 下面就该开始处理数据了，这里的思路就是递归处理目录及目录下的所有文件和目录 // 这里可以自己写个递归来处理，不过 Golang 提供了 filepath.Walk 函数，可以很方便的做这个事情 // 直接将这个函数的处理结果返回就行，需要传给它一个源文件或目录，它就可以自己去处理 // 我们就只需要去实现我们自己的 打包逻辑即可，不需要再去路径相关的事情 return filepath.Walk(src, func(fileName string, fi os.FileInfo, err error) error { // 因为这个闭包会返回个 error ，所以先要处理一下这个 if err != nil { return err } // 这里就不需要我们自己再 os.Stat 了，它已经做好了，我们直接使用 fi 即可 hdr, err := tar.FileInfoHeader(fi, \u0026#34;\u0026#34;) if err != nil { return err } // 这里需要处理下 hdr 中的 Name，因为默认文件的名字是不带路径的， // 打包之后所有文件就会堆在一起，这样就破坏了原本的目录结果 // 例如： 将原本 hdr.Name 的 syslog 替换程 log/syslog // 这个其实也很简单，回调函数的 fileName 字段给我们返回来的就是完整路径的 log/syslog // strings.TrimPrefix 将 fileName 的最左侧的 / 去掉， // 熟悉 Linux 的都知道为什么要去掉这个 hdr.Name = strings.TrimPrefix(fileName, string(filepath.Separator)) // 写入文件信息 if err := tw.WriteHeader(hdr); err != nil { return err } // 判断下文件是否是标准文件，如果不是就不处理了， // 如： 目录，这里就只记录了文件信息，不会执行下面的 copy if !fi.Mode().IsRegular() { return nil } // 打开文件 fr, err := os.Open(fileName) defer fr.Close() if err != nil { return err } // copy 文件数据到 tw n, err := io.Copy(tw, fr) if err != nil { return err } // 记录下过程，这个可以不记录，这个看需要，这样可以看到打包的过程 log.Printf(\u0026#34;成功打包 %s ，共写入了 %d 字节的数据\\n\u0026#34;, fileName, n) return nil }) } 打包及压缩就搞定了，不过这个代码现在我还发现有个问题，就是不能处理软链接\n解包解压 # 这个过程基本就是把压缩的过程返回来，多了些创建目录的操作\npackage main import ( \u0026#34;archive/tar\u0026#34; \u0026#34;compress/gzip\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; ) func main() { var dst = \u0026#34;\u0026#34; // 不写就是解压到当前目录 var src = \u0026#34;log.tar.gz\u0026#34; UnTar(dst, src) } func UnTar(dst, src string) (err error) { // 打开准备解压的 tar 包 fr, err := os.Open(src) if err != nil { return } defer fr.Close() // 将打开的文件先解压 gr, err := gzip.NewReader(fr) if err != nil { return } defer gr.Close() // 通过 gr 创建 tar.Reader tr := tar.NewReader(gr) // 现在已经获得了 tar.Reader 结构了，只需要循环里面的数据写入文件就可以了 for { hdr, err := tr.Next() switch { case err == io.EOF: return nil case err != nil: return err case hdr == nil: continue } // 处理下保存路径，将要保存的目录加上 header 中的 Name // 这个变量保存的有可能是目录，有可能是文件，所以就叫 FileDir 了…… dstFileDir := filepath.Join(dst, hdr.Name) // 根据 header 的 Typeflag 字段，判断文件的类型 switch hdr.Typeflag { case tar.TypeDir: // 如果是目录时候，创建目录 // 判断下目录是否存在，不存在就创建 if b := ExistDir(dstFileDir); !b { // 使用 MkdirAll 不使用 Mkdir ，就类似 Linux 终端下的 mkdir -p， // 可以递归创建每一级目录 if err := os.MkdirAll(dstFileDir, 0775); err != nil { return err } } case tar.TypeReg: // 如果是文件就写入到磁盘 // 创建一个可以读写的文件，权限就使用 header 中记录的权限 // 因为操作系统的 FileMode 是 int32 类型的，hdr 中的是 int64，所以转换下 file, err := os.OpenFile(dstFileDir, os.O_CREATE|os.O_RDWR, os.FileMode(hdr.Mode)) if err != nil { return err } n, err := io.Copy(file, tr) if err != nil { return err } // 将解压结果输出显示 fmt.Printf(\u0026#34;成功解压： %s , 共处理了 %d 个字符\\n\u0026#34;, dstFileDir, n) // 不要忘记关闭打开的文件，因为它是在 for 循环中，不能使用 defer // 如果想使用 defer 就放在一个单独的函数中 file.Close() } } return nil } // 判断目录是否存在 func ExistDir(dirname string) bool { fi, err := os.Stat(dirname) return (err == nil || os.IsExist(err)) \u0026amp;\u0026amp; fi.IsDir() } 到这里解压就完成了，只是一个实验代码，还有很多不完善的地方，欢迎提出宝贵的意见。\n","date":"14 July 2023","permalink":"/python/python-blog/","section":"Pythons","summary":"This is title # 这个包比较简单，就是将文件进行打包和解包，要是熟悉 Linux 下的 tar 命令这个就很好理解了。 主要是通过 tar.","title":"Python 博客第一篇"},{"content":"","date":"14 July 2023","permalink":"/python/","section":"Pythons","summary":"","title":"Pythons"},{"content":" This is title # 这个包比较简单，就是将文件进行打包和解包，要是熟悉 Linux 下的 tar 命令这个就很好理解了。 主要是通过 tar.Reader 读取 tar 包，通过 tar.Writer 写入 tar 包，在写入的过程中再设置一下头，详细的过程以示例的方式进行展示，可以查看代码里面的注释。\n参考：\n标准库 tar 中文文档\n标准库 tar 官方文档\n单个文件操作 # 这个非常简单，就是读取一个文件，进行打包及解包操作即可。\n单个文件打包 # 从 /etc/passwd 下复制了一个 passwd 文件到当前目录下，用来做压缩测试。什么文件都是可以的，自己随意写一个也行。这里的示例主要为了说明 tar ，没有处理路径，所以过程全部假设是在当前目录下执行。\ncp /etc/passwd . 关于文件的打包直接查看示例代码，已经在示例代码中做了详细的注释。\n示例代码（ pack_single_file.go ）：\npackage main import ( \u0026#34;os\u0026#34; \u0026#34;log\u0026#34; \u0026#34;archive/tar\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; ) func main() { // 准备打包的源文件 var srcFile = \u0026#34;passwd\u0026#34; // 打包后的文件 var desFile = fmt.Sprintf(\u0026#34;%s.tar\u0026#34;,srcFile) // 需要注意文件的打开即关闭的顺序，因为 defer 是后入先出，所以关闭顺序很重要 // 第一次写这个示例的时候就没注意，导致写完的 tar 包不完整 // ###### 第 1 步，先准备好一个 tar.Writer 结构，然后再向里面写入内容。 ###### // 创建一个文件，用来保存打包后的 passwd.tar 文件 fw, err := os.Create(desFile) ErrPrintln(err) defer fw.Close() // 通过 fw 创建一个 tar.Writer tw := tar.NewWriter(fw) // 这里不要忘记关闭，如果不能成功关闭会造成 tar 包不完整 // 所以这里在关闭的同时进行判断，可以清楚的知道是否成功关闭 defer func() { if err := tw.Close(); err != nil { ErrPrintln(err) } }() // ###### 第 2 步，处理文件信息，也就是 tar.Header 相关的 ###### // tar 包共有两部分内容：文件信息和文件数据 // 通过 Stat 获取 FileInfo，然后通过 FileInfoHeader 得到 hdr tar.*Header fi, err := os.Stat(srcFile) ErrPrintln(err) hdr, err := tar.FileInfoHeader(fi, \u0026#34;\u0026#34;) // 将 tar 的文件信息 hdr 写入到 tw err = tw.WriteHeader(hdr) ErrPrintln(err) // 将文件数据写入 // 打开准备写入的文件 fr, err := os.Open(srcFile) ErrPrintln(err) defer fr.Close() written, err := io.Copy(tw, fr) ErrPrintln(err) log.Printf(\u0026#34;共写入了 %d 个字符的数据\\n\u0026#34;,written) } // 定义一个用来打印的函数，少写点代码，因为要处理很多次的 err // 后面其他示例还会继续使用这个函数，就不单独再写，望看到此函数了解 func ErrPrintln(err error) { if err != nil { log.Println(err) os.Exit(1) } } 单个文件解包 # 这个也很简单，基本上将上面过程反过来，只需要处理 tar.Reader 即可，详细的描述见示例。\n这里就用刚刚打包的 passwd.tar 文件做示例，如果怕结果看不出效果，可以将之前用的 passwd 源文件删除。\nrm passwd 示例代码（ unpack_single_file.go ）：\npackage main import ( \u0026#34;os\u0026#34; \u0026#34;archive/tar\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; ) func main() { var srcFile = \u0026#34;passwd.tar\u0026#34; // 将 tar 包打开 fr, err := os.Open(srcFile) ErrPrintln(err) defer fr.Close() // 通过 fr 创建一个 tar.*Reader 结构，然后将 tr 遍历，并将数据保存到磁盘中 tr := tar.NewReader(fr) for hdr, err := tr.Next(); err != io.EOF; hdr, err = tr.Next(){ // 处理 err ！= nil 的情况 ErrPrintln(err) // 获取文件信息 fi := hdr.FileInfo() // 创建一个空文件，用来写入解包后的数据 fw, err := os.Create(fi.Name()) ErrPrintln(err) // 将 tr 写入到 fw n, err := io.Copy(fw, tr) ErrPrintln(err) log.Printf(\u0026#34;解包： %s 到 %s ，共处理了 %d 个字符的数据。\u0026#34;, srcFile,fi.Name(),n) // 设置文件权限，这样可以保证和原始文件权限相同，如果不设置，会根据当前系统的 umask 来设置。 os.Chmod(fi.Name(),fi.Mode().Perm()) // 注意，因为是在循环中，所以就没有使用 defer 关闭文件 // 如果想使用 defer 的话，可以将文件写入的步骤单独封装在一个函数中即可 fw.Close() } } func ErrPrintln(err error){ if err != nil { log.Fatalln(err) os.Exit(1) } } 操作整个目录 # 我们实际中 tar 很少会去打包单个文件，一般都是打包整个目录，并且打包的时候通过 gzip 或者 bzip2 压缩。\n如果要打包整个目录，可以通过递归的方式来实现。这里只演示了 gzip 方式压缩，这个实现非常简单，只需要在 fw 和 tw 之前加上一层压缩即可，详情见示例代码。\n为了测试打包整个目录，复制了一个 log 目录到当前路径下。什么目录和文件都可以，只是因为这个里面内容比较多，就拿这个来做测试了。\n# 出现没有权限的错误不用管它，复制过来多少是多少吧 cp -r /var/log/ . 详细的操作会在注释中说明，不过在之前单文件中出现过的步骤不再注释。\n打包压缩 # 示例代码（ targz.go ）：\npackage main import ( \u0026#34;archive/tar\u0026#34; \u0026#34;compress/gzip\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; \u0026#34;strings\u0026#34; ) func main() { // 修改日志格式，显示出错代码的所在行，方便调试，实际项目中一般不记录这个。 var src = \u0026#34;apt\u0026#34; var dst = fmt.Sprintf(\u0026#34;%s.tar.gz\u0026#34;, src) // 将步骤写入了一个函数中，这样处理错误方便一些 if err := Tar(src, dst); err != nil { log.Fatalln(err) } } func Tar(src, dst string) (err error) { // 创建文件 fw, err := os.Create(dst) if err != nil { return } defer fw.Close() // 将 tar 包使用 gzip 压缩，其实添加压缩功能很简单， // 只需要在 fw 和 tw 之前加上一层压缩就行了，和 Linux 的管道的感觉类似 gw := gzip.NewWriter(fw) defer gw.Close() // 创建 Tar.Writer 结构 tw := tar.NewWriter(gw) // 如果需要启用 gzip 将上面代码注释，换成下面的 defer tw.Close() // 下面就该开始处理数据了，这里的思路就是递归处理目录及目录下的所有文件和目录 // 这里可以自己写个递归来处理，不过 Golang 提供了 filepath.Walk 函数，可以很方便的做这个事情 // 直接将这个函数的处理结果返回就行，需要传给它一个源文件或目录，它就可以自己去处理 // 我们就只需要去实现我们自己的 打包逻辑即可，不需要再去路径相关的事情 return filepath.Walk(src, func(fileName string, fi os.FileInfo, err error) error { // 因为这个闭包会返回个 error ，所以先要处理一下这个 if err != nil { return err } // 这里就不需要我们自己再 os.Stat 了，它已经做好了，我们直接使用 fi 即可 hdr, err := tar.FileInfoHeader(fi, \u0026#34;\u0026#34;) if err != nil { return err } // 这里需要处理下 hdr 中的 Name，因为默认文件的名字是不带路径的， // 打包之后所有文件就会堆在一起，这样就破坏了原本的目录结果 // 例如： 将原本 hdr.Name 的 syslog 替换程 log/syslog // 这个其实也很简单，回调函数的 fileName 字段给我们返回来的就是完整路径的 log/syslog // strings.TrimPrefix 将 fileName 的最左侧的 / 去掉， // 熟悉 Linux 的都知道为什么要去掉这个 hdr.Name = strings.TrimPrefix(fileName, string(filepath.Separator)) // 写入文件信息 if err := tw.WriteHeader(hdr); err != nil { return err } // 判断下文件是否是标准文件，如果不是就不处理了， // 如： 目录，这里就只记录了文件信息，不会执行下面的 copy if !fi.Mode().IsRegular() { return nil } // 打开文件 fr, err := os.Open(fileName) defer fr.Close() if err != nil { return err } // copy 文件数据到 tw n, err := io.Copy(tw, fr) if err != nil { return err } // 记录下过程，这个可以不记录，这个看需要，这样可以看到打包的过程 log.Printf(\u0026#34;成功打包 %s ，共写入了 %d 字节的数据\\n\u0026#34;, fileName, n) return nil }) } 打包及压缩就搞定了，不过这个代码现在我还发现有个问题，就是不能处理软链接\n解包解压 # 这个过程基本就是把压缩的过程返回来，多了些创建目录的操作\npackage main import ( \u0026#34;archive/tar\u0026#34; \u0026#34;compress/gzip\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; ) func main() { var dst = \u0026#34;\u0026#34; // 不写就是解压到当前目录 var src = \u0026#34;log.tar.gz\u0026#34; UnTar(dst, src) } func UnTar(dst, src string) (err error) { // 打开准备解压的 tar 包 fr, err := os.Open(src) if err != nil { return } defer fr.Close() // 将打开的文件先解压 gr, err := gzip.NewReader(fr) if err != nil { return } defer gr.Close() // 通过 gr 创建 tar.Reader tr := tar.NewReader(gr) // 现在已经获得了 tar.Reader 结构了，只需要循环里面的数据写入文件就可以了 for { hdr, err := tr.Next() switch { case err == io.EOF: return nil case err != nil: return err case hdr == nil: continue } // 处理下保存路径，将要保存的目录加上 header 中的 Name // 这个变量保存的有可能是目录，有可能是文件，所以就叫 FileDir 了…… dstFileDir := filepath.Join(dst, hdr.Name) // 根据 header 的 Typeflag 字段，判断文件的类型 switch hdr.Typeflag { case tar.TypeDir: // 如果是目录时候，创建目录 // 判断下目录是否存在，不存在就创建 if b := ExistDir(dstFileDir); !b { // 使用 MkdirAll 不使用 Mkdir ，就类似 Linux 终端下的 mkdir -p， // 可以递归创建每一级目录 if err := os.MkdirAll(dstFileDir, 0775); err != nil { return err } } case tar.TypeReg: // 如果是文件就写入到磁盘 // 创建一个可以读写的文件，权限就使用 header 中记录的权限 // 因为操作系统的 FileMode 是 int32 类型的，hdr 中的是 int64，所以转换下 file, err := os.OpenFile(dstFileDir, os.O_CREATE|os.O_RDWR, os.FileMode(hdr.Mode)) if err != nil { return err } n, err := io.Copy(file, tr) if err != nil { return err } // 将解压结果输出显示 fmt.Printf(\u0026#34;成功解压： %s , 共处理了 %d 个字符\\n\u0026#34;, dstFileDir, n) // 不要忘记关闭打开的文件，因为它是在 for 循环中，不能使用 defer // 如果想使用 defer 就放在一个单独的函数中 file.Close() } } return nil } // 判断目录是否存在 func ExistDir(dirname string) bool { fi, err := os.Stat(dirname) return (err == nil || os.IsExist(err)) \u0026amp;\u0026amp; fi.IsDir() } 到这里解压就完成了，只是一个实验代码，还有很多不完善的地方，欢迎提出宝贵的意见。\n","date":"12 July 2023","permalink":"/python/tests/","section":"Pythons","summary":"This is title # 这个包比较简单，就是将文件进行打包和解包，要是熟悉 Linux 下的 tar 命令这个就很好理解了。 主要是通过 tar.","title":"First Blog"},{"content":"Go 面经 相关记录\n基础语法 # = 和 := 的区别？ =是赋值变量，:=是定义变量 指针的作用 一个指针可以指向任意变量的地址，它所指向的地址在32位或64位机器上分别固定占4或8个字节。指针的作用有 获取变量的值 改变变量的值 用指针替代值传入函数，比如类的接收器就是这样的 Go 允许多个返回值吗？ 可以。通常函数除了一般返回值还会返回一个error Go 有异常类型吗？ 有。Go用error类型代替try\u0026hellip;catch语句，这样可以节省资源。同时增加代码可读性 也可以用errors.New()来定义自己的异常。errors.Error()会返回异常的字符串表示。只要实现error接口就可以定义自己的异常， 什么是协程（Goroutine） *协程是用户态轻量级线程，它是线程调度的基本单位。通常在函数前加上go关键字就能实现并发。一个Goroutine会以一个很小的栈启动2KB或4KB，当遇到栈空间不足时，栈会自动伸缩， 因此可以轻易实现成千上万个goroutine同时启动 如何高效地拼接字符串 拼接字符串的方式有：+ , fmt.Sprintf , strings.Builder, bytes.Buffer, strings.Join 使用+操作符进行拼接时，会对字符串进行遍历，计算并开辟一个新的空间来存储原来的两个字符串由于采用了接口参数，必须要用反射获取值，因此有性能损耗 用WriteString()进行拼接，内部实现是指针+切片，同时String()返回拼接后的字符串，它是直接把[]byte转换为string，从而避免变量拷贝 bytes.Buffer是一个一个缓冲byte类型的缓冲器，这个缓冲器里存放着都是byte,bytes.buffer底层也是一个[]byte切片 strings.join也是基于strings.builder来实现的,并且可以自定义分隔符，在join方法内调用了b.Grow(n)方法，这个是进行初步的容量分配，而前面计算的n的长度就是我们要拼接的slice的长度，因为我们传入切片长度固定，所以提前进行容量分配可以减少内存分配，很高效 性能比较：strings.Join ≈ strings.Builder \u0026gt; bytes.Buffer \u0026gt; \u0026ldquo;+\u0026rdquo; \u0026gt; fmt.Sprintf 什么是 rune 类型 如何判断 map 中是否包含某个 key ？ Go 支持默认参数或可选参数吗？ defer 的执行顺序 如何交换 2 个变量的值？ Go 语言 tag 的用处？ 如何判断 2 个字符串切片（slice) 是相等的？ 字符串打印时，%v 和 %+v 的区别 Go 语言中如何表示枚举值(enums)？ 空 struct{} 的用途 实现原理 # init() 函数是什么时候执行的 Go 语言的局部变量分配在栈上还是堆上？ 2 个 interface 可以比较吗 ？ 2 个 nil 可能不相等吗？ 简述 Go 语言GC(垃圾回收)的工作原理 函数返回局部变量的指针是否安全？ 非接口非接口的任意类型 T() 都能够调用 *T 的方法吗？反过来呢？ 并发编程 # 无缓冲的 channel 和有缓冲的 channel 的区别？ 什么是协程泄露(Goroutine Leak)？ Go 可以限制运行时操作系统线程的数量吗？ 代码输出 # 变量与常量 作用域 defer 延迟调用 ","date":"11 July 2023","permalink":"/golang/interview/problems/","section":"Golang","summary":"Go 面经 相关记录","title":"Go 面试题"},{"content":"","date":"23 July 2022","permalink":"/tags/template/","section":"Tags","summary":"","title":"Template"},{"content":"仅作为日志模板\n","date":"23 July 2022","permalink":"/diary/template/","section":"Diaries","summary":"仅作为日志模板","title":"模板日志"},{"content":"","date":"1 January 0001","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"1 January 0001","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"1 January 0001","permalink":"/series/","section":"Series","summary":"","title":"Series"}]